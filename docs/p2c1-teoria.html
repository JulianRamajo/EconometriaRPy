<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Econometría - 2&nbsp; El modelo de regresión lineal y sus hipótesis básicas</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./p2c2-app1.html" rel="next">
<link href="./p2-mrl-basico.html" rel="prev">
<link href="./EconMetricsRPy-logo.png" rel="icon" type="image/png">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./p2-mrl-basico.html">CAPÍTULO 2: EL MODELO DE REGRESIÓN LINEAL Y SUS HIPÓTESIS BÁSICAS</a></li><li class="breadcrumb-item"><a href="./p2c1-teoria.html"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">El modelo de regresión lineal y sus hipótesis básicas</span></a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Econometría</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/JulianRamajo/EconometriaRPy" rel="" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">PREFACIO</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./p0-aspectos-tecnicos.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">CAPÍTULO 0: ASPECTOS TÉCNICOS</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p0c1-setup.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Instalación y configuración de R y Python en RStudio</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p0c2-rpy-comp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">R versus Python: Comparación de lenguajes</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p0c3-reticulate.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Programación conjunta en R y Python con reticulate</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./p1-introduccion.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">CAPÍTULO 1: CONCEPTOS BÁSICOS</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p1c1-teoria.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Conceptos básicos de la econometría</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p1c2-app1a.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Aplicación 1.1.a (Gestión y representación gráfica de datos): Gramática básica del <em>tidyverse</em></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p1c2-app1b.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Aplicación 1.1.b (Gestión y representación gráfica de datos): Gestión de datos con R y Python</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p1c2-app2a.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Aplicación 1.2a (Gestión y representación gráfica de datos financieros): Propiedades estadísticas básicas de los activos bursátiles</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p1c2-app2b.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Aplicación 1.2b (Gestión y representación gráfica de datos financieros): Relaciones entre activos bursátiles</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p1c2-app3a.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Aplicación 1.3.a (Gestión y representación gráfica de datos espaciales): Emisiones de CO2 al nivel mundial</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p1c2-app3b.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Aplicación 1.3b (Gestión y representación gráfica de datos espaciales): Desarrollo humano al nivel mundial</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p1c2-app4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Aplicación 1.4 (Regresiones con datos de corte transversal): Demanda familiar de carne</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p1c2-app5.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Aplicación 1.5 (Regresiones con datos de series temporales): Consumo privado en Estados Unidos</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./p2-mrl-basico.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">CAPÍTULO 2: EL MODELO DE REGRESIÓN LINEAL Y SUS HIPÓTESIS BÁSICAS</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p2c1-teoria.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">El modelo de regresión lineal y sus hipótesis básicas</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p2c2-app1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Aplicación 2.1 (Estimación MCO y contrastes de hipótesis): Modelo CAPM para las acciones del Banco Santander</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p2c2-app2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Aplicación 2.2 (Predicción con el modelo de regresión lineal): Curva de Engel para el gasto en alimentos</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p2c2-app3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Aplicación 2.3 (Modelo de regresión lineal con forma funcional polinómica): Curva de costes en el sector textil</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p2c2-app4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Aplicación 2.4 (Modelo de regresión lineal con términos de interacción): Efectos diferenciados de la publicidad sobre las ventas</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p2c2-app5.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Aplicación 2.5 (Modelo de regresión lineal con variables cualitativas): Brecha salarial entre hombres y mujeres en Estados Unidos</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p2c2-app6.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Aplicación 2.6 (Series temporales de alta frecuencia y estacionalidad): Demanda de electricidad en el estado de Victoria, Australia</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./p3-mrl-extendido.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">CAPÍTULO 3: DIAGNOSIS, CORRECCIONES Y EXTENSIONES DEL MODELO DE REGRESIÓN LINEAL</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p3c1-teoria.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Diagnosis, correcciones y extensiones del modelo de regresión lineal</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p3c2-app1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Aplicación 3.1 (Evaluación, validación y especificación del MRL): El modelo APT de valoración de activos</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p3c2-app2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Aplicación 3.2 (Evaluación, validación y especificación del MRL): Ventas en una cadena de supermercados</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p3c2-app3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Aplicación 3.3 (Estabilidad de los parámetros estructurales): Diferenciación salarial por sexo. Exportaciones españolas.</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p3c2-app4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Aplicación 3.4 (No normalidad de los errores y observaciones atípicas): Muestra de datos con observaciones atípicas simuladas</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p3c2-app5.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Aplicación 3.5 (Regresiones heteroscedásticas): Función de demanda con varianza no constante</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p3c2-app6.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Aplicación 3.6 (Regresiones con volatilidad variable en el tiempo): Relación entre los precios de los carburantes y el precio del petroleo</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p3c2-app7.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Aplicación 3.7 (Autocorrelación y regresiones dinámicas): Modelos ARDL y VAR</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p3c2-app8a.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Aplicación 3.8a (Dependencia espacial - Geometría: polígonos): Estadísticas ‘morales’ en Francia en 1830</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p3c2-app8b.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Aplicación 3.8b (Dependencia espacial - Geometría: puntos): Precio de la vivienda en el condado de Lucas [Ohio, Estados Unidos]</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p3c2-app9.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Aplicación 3.9 (Información muestral - Falta de observaciones): Exclusión social en el comportamiento de las aseguradoras de Chicago</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p3c2-app10.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Aplicación 3.10 (Información muestral - Multicolinealidad): Análisis de la rentabilidad empresarial</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p3c2-app11.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Aplicación 3.11 (Regresores endógenos - El estimador de variables instrumentales: Determinantes de la inflación al nivel internacional</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p3c2-app12.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Aplicación 3.12 (Variable dependiente discreta o limitada): Modelos Logit/Probit, Tobit y Heckit</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p3c2-app13.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Aplicación 3.13 (Modelos econométricos para datos de panel): Productividad de la industria química en China</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">BIBLIOGRAFÍA</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introducción" id="toc-introducción" class="nav-link active" data-scroll-target="#introducción"><span class="header-section-number">2.1</span> Introducción</a></li>
  <li><a href="#el-modelo-de-regresión-lineal" id="toc-el-modelo-de-regresión-lineal" class="nav-link" data-scroll-target="#el-modelo-de-regresión-lineal"><span class="header-section-number">2.2</span> El modelo de regresión lineal</a>
  <ul>
  <li><a href="#especificación" id="toc-especificación" class="nav-link" data-scroll-target="#especificación"><span class="header-section-number">2.2.1</span> Especificación</a></li>
  <li><a href="#hipótesis-básicas-del-mrl-condiciones-de-regularidad" id="toc-hipótesis-básicas-del-mrl-condiciones-de-regularidad" class="nav-link" data-scroll-target="#hipótesis-básicas-del-mrl-condiciones-de-regularidad"><span class="header-section-number">2.2.2</span> Hipótesis básicas del MRL (condiciones de regularidad)</a></li>
  <li><a href="#algunas-propiedades-interpretaciones-o-representaciones-del-mrl" id="toc-algunas-propiedades-interpretaciones-o-representaciones-del-mrl" class="nav-link" data-scroll-target="#algunas-propiedades-interpretaciones-o-representaciones-del-mrl"><span class="header-section-number">2.2.3</span> Algunas propiedades, interpretaciones o representaciones del MRL</a></li>
  <li><a href="#estimación-de-los-parámetros-estructurales-el-método-de-mínimos-cuadrados-ordinarios-mco" id="toc-estimación-de-los-parámetros-estructurales-el-método-de-mínimos-cuadrados-ordinarios-mco" class="nav-link" data-scroll-target="#estimación-de-los-parámetros-estructurales-el-método-de-mínimos-cuadrados-ordinarios-mco"><span class="header-section-number">2.2.4</span> Estimación de los parámetros estructurales: el método de mínimos cuadrados ordinarios (MCO)</a></li>
  <li><a href="#propiedades-estadísticas-y-algebraicas-del-estimador-mco-widehatmathbfbeta" id="toc-propiedades-estadísticas-y-algebraicas-del-estimador-mco-widehatmathbfbeta" class="nav-link" data-scroll-target="#propiedades-estadísticas-y-algebraicas-del-estimador-mco-widehatmathbfbeta"><span class="header-section-number">2.2.5</span> Propiedades estadísticas y algebraicas del estimador MCO, <span class="math inline">\(\widehat{\mathbf{\beta}}\)</span></a></li>
  <li><a href="#propiedades-algebraicas-de-los-residuos-y-de-la-regresión-mco" id="toc-propiedades-algebraicas-de-los-residuos-y-de-la-regresión-mco" class="nav-link" data-scroll-target="#propiedades-algebraicas-de-los-residuos-y-de-la-regresión-mco"><span class="header-section-number">2.2.6</span> Propiedades algebraicas de los residuos y de la regresión MCO</a></li>
  <li><a href="#estimación-de-la-varianza-de-los-errores-mathbfsigmamathbf2" id="toc-estimación-de-la-varianza-de-los-errores-mathbfsigmamathbf2" class="nav-link" data-scroll-target="#estimación-de-la-varianza-de-los-errores-mathbfsigmamathbf2"><span class="header-section-number">2.2.7</span> Estimación de la varianza de los errores, <span class="math inline">\(\mathbf{\sigma}^{\mathbf{2}}\)</span></a></li>
  </ul></li>
  <li><a href="#grado-de-ajuste-de-una-regresión" id="toc-grado-de-ajuste-de-una-regresión" class="nav-link" data-scroll-target="#grado-de-ajuste-de-una-regresión"><span class="header-section-number">2.3</span> Grado de ajuste de una regresión</a>
  <ul>
  <li><a href="#coeficiente-de-determinación" id="toc-coeficiente-de-determinación" class="nav-link" data-scroll-target="#coeficiente-de-determinación"><span class="header-section-number">2.3.1</span> Coeficiente de determinación</a></li>
  <li><a href="#coeficiente-de-determinación-ajustado" id="toc-coeficiente-de-determinación-ajustado" class="nav-link" data-scroll-target="#coeficiente-de-determinación-ajustado"><span class="header-section-number">2.3.2</span> Coeficiente de determinación ajustado</a></li>
  </ul></li>
  <li><a href="#inferencia-estadística-i-intervalos-de-confianza-y-contrastes-de-hipótesis-para-los-parámetros-del-modelo" id="toc-inferencia-estadística-i-intervalos-de-confianza-y-contrastes-de-hipótesis-para-los-parámetros-del-modelo" class="nav-link" data-scroll-target="#inferencia-estadística-i-intervalos-de-confianza-y-contrastes-de-hipótesis-para-los-parámetros-del-modelo"><span class="header-section-number">2.4</span> Inferencia estadística (I): Intervalos de confianza y contrastes de hipótesis para los parámetros del modelo</a>
  <ul>
  <li><a href="#distribución-de-los-estimadores-de-los-parámetros-estructurales" id="toc-distribución-de-los-estimadores-de-los-parámetros-estructurales" class="nav-link" data-scroll-target="#distribución-de-los-estimadores-de-los-parámetros-estructurales"><span class="header-section-number">2.4.1</span> Distribución de los estimadores de los parámetros estructurales</a></li>
  <li><a href="#intervalos-de-confianza-para-los-parámetros-estructurales" id="toc-intervalos-de-confianza-para-los-parámetros-estructurales" class="nav-link" data-scroll-target="#intervalos-de-confianza-para-los-parámetros-estructurales"><span class="header-section-number">2.4.2</span> Intervalos de confianza para los parámetros estructurales</a></li>
  <li><a href="#contrastes-de-hipótesis-sobre-parámetros-estructurales" id="toc-contrastes-de-hipótesis-sobre-parámetros-estructurales" class="nav-link" data-scroll-target="#contrastes-de-hipótesis-sobre-parámetros-estructurales"><span class="header-section-number">2.4.3</span> Contrastes de hipótesis sobre parámetros estructurales</a></li>
  <li><a href="#intervalo-de-confianza-y-contrastes-de-hipótesis-para-la-varianza-residual" id="toc-intervalo-de-confianza-y-contrastes-de-hipótesis-para-la-varianza-residual" class="nav-link" data-scroll-target="#intervalo-de-confianza-y-contrastes-de-hipótesis-para-la-varianza-residual"><span class="header-section-number">2.4.4</span> Intervalo de confianza y contrastes de hipótesis para la varianza residual</a></li>
  </ul></li>
  <li><a href="#inferencia-estadística-ii-contrastes-conjuntos-de-restricciones-y-el-estimador-de-mínimos-cuadrados-restringidos" id="toc-inferencia-estadística-ii-contrastes-conjuntos-de-restricciones-y-el-estimador-de-mínimos-cuadrados-restringidos" class="nav-link" data-scroll-target="#inferencia-estadística-ii-contrastes-conjuntos-de-restricciones-y-el-estimador-de-mínimos-cuadrados-restringidos"><span class="header-section-number">2.5</span> Inferencia estadística (II): Contrastes conjuntos de restricciones y el estimador de mínimos cuadrados restringidos</a>
  <ul>
  <li><a href="#distribución-de-los-estimadores-de-combinaciones-lineales-de-parámetros-estructurales" id="toc-distribución-de-los-estimadores-de-combinaciones-lineales-de-parámetros-estructurales" class="nav-link" data-scroll-target="#distribución-de-los-estimadores-de-combinaciones-lineales-de-parámetros-estructurales"><span class="header-section-number">2.5.1</span> Distribución de los estimadores de combinaciones lineales de parámetros estructurales</a></li>
  <li><a href="#contrastes-conjuntos-de-restricciones-lineales-sobre-el-vector-mathbfbeta" id="toc-contrastes-conjuntos-de-restricciones-lineales-sobre-el-vector-mathbfbeta" class="nav-link" data-scroll-target="#contrastes-conjuntos-de-restricciones-lineales-sobre-el-vector-mathbfbeta"><span class="header-section-number">2.5.2</span> Contrastes conjuntos de restricciones lineales sobre el vector <span class="math inline">\(\mathbf{\beta}\)</span></a></li>
  <li><a href="#contraste-de-validez-general-del-modelo-de-regresión" id="toc-contraste-de-validez-general-del-modelo-de-regresión" class="nav-link" data-scroll-target="#contraste-de-validez-general-del-modelo-de-regresión"><span class="header-section-number">2.5.3</span> Contraste de validez general del modelo de regresión</a></li>
  <li><a href="#el-estimador-de-mínimos-cuadrados-restringidos" id="toc-el-estimador-de-mínimos-cuadrados-restringidos" class="nav-link" data-scroll-target="#el-estimador-de-mínimos-cuadrados-restringidos"><span class="header-section-number">2.5.4</span> El estimador de mínimos cuadrados restringidos</a></li>
  </ul></li>
  <li><a href="#predicciones" id="toc-predicciones" class="nav-link" data-scroll-target="#predicciones"><span class="header-section-number">2.6</span> Predicciones</a></li>
  <li><a href="#forma-funcional" id="toc-forma-funcional" class="nav-link" data-scroll-target="#forma-funcional"><span class="header-section-number">2.7</span> Forma funcional</a>
  <ul>
  <li><a href="#regresiones-cuadráticas-o-polinómicas" id="toc-regresiones-cuadráticas-o-polinómicas" class="nav-link" data-scroll-target="#regresiones-cuadráticas-o-polinómicas"><span class="header-section-number">2.7.1</span> Regresiones cuadráticas o polinómicas</a></li>
  <li><a href="#regresiones-logarítmicas" id="toc-regresiones-logarítmicas" class="nav-link" data-scroll-target="#regresiones-logarítmicas"><span class="header-section-number">2.7.2</span> Regresiones logarítmicas</a></li>
  <li><a href="#regresiones-semi-logarítmicas" id="toc-regresiones-semi-logarítmicas" class="nav-link" data-scroll-target="#regresiones-semi-logarítmicas"><span class="header-section-number">2.7.3</span> Regresiones semi-logarítmicas</a></li>
  <li><a href="#regresión-recíproca" id="toc-regresión-recíproca" class="nav-link" data-scroll-target="#regresión-recíproca"><span class="header-section-number">2.7.4</span> Regresión recíproca</a></li>
  <li><a href="#regresiones-con-términos-de-interacción" id="toc-regresiones-con-términos-de-interacción" class="nav-link" data-scroll-target="#regresiones-con-términos-de-interacción"><span class="header-section-number">2.7.5</span> Regresiones con términos de interacción</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">El modelo de regresión lineal y sus hipótesis básicas</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<section id="introducción" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="introducción"><span class="header-section-number">2.1</span> Introducción</h2>
<p>A través del contenido recogido en este tema pretendemos sentar las bases que permitirán llegar a realizar un análisis de regresión con variables económicas, siendo capaz de juzgar la validez del modelo y de interpretar sus resultados desde los puntos de vista estadístico y económico.</p>
<p>Así, analizaremos relaciones econométricas del tipo</p>
<p><span class="math display">\[y = \beta_{1} + \beta_{2}x_{2} + \ldots + \beta_{K}x_{K} + e\]</span></p>
<p>es decir, regresiones con una variable dependiente, <span class="math inline">\(y\)</span>, y varias variables explicativas, <span class="math inline">\(x_{j}\)</span>, las cuales ayudan a explicar parcialmente el comportamiento de la variable <span class="math inline">\(y\)</span>.</p>
</section>
<section id="el-modelo-de-regresión-lineal" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="el-modelo-de-regresión-lineal"><span class="header-section-number">2.2</span> El modelo de regresión lineal</h2>
<section id="especificación" class="level3" data-number="2.2.1">
<h3 data-number="2.2.1" class="anchored" data-anchor-id="especificación"><span class="header-section-number">2.2.1</span> Especificación</h3>
<p>Consideremos que, de acuerdo con la teoría económica, una variable <span class="math inline">\(y\)</span>, que describe el comportamiento de un agente económico o un determinado aspecto de un sistema económico bajo estudio, está relacionada con un conjunto de variables explicativas<span class="math inline">\(\ x_{2},x_{3},\ldots,x_{K}\)</span>, según la relación lineal <span class="math inline">\(\ y = \beta_{1} + \beta_{2}x_{2} + \ldots + \beta_{K}x_{K} + e\)</span> . En esta ecuación supondremos que <span class="math inline">\(\beta_{1},\beta_{2},\ldots,\beta_{K}\)</span> son parámetros fijos y que <span class="math inline">\(e\)</span> es una variable aleatoria, es decir, sus valores se determinan por un mecanismo probabilista.</p>
<p>Por tanto, al expresar la variable <em>y</em> mediante la relación anterior, estamos suponiendo que tiene dos componentes: una <em>parte explicada o sistemática</em>, que vendría dada por <span class="math inline">\(\beta_{1} + \beta_{2}x_{2} + \ldots + \beta_{K}x_{K}\)</span> y una <em>parte no explicada</em>, <span class="math inline">\(e\)</span>, conocida como perturbación aleatoria o error del modelo.</p>
<p>La perturbación aleatoria del modelo (<span class="math inline">\(e\)</span>) recoge factores tales como:</p>
<ul>
<li><p>El comportamiento aleatorio propio de los agentes económicos.</p></li>
<li><p>Los errores de medida en la variable <em>y</em> o en las variables &gt; explicativas <em>x</em>’s.</p></li>
<li><p>El efecto conjunto de otras variables no incluidas en el modelo.</p></li>
</ul>
<p>El objetivo básico de la econometría inicialmente se centrará en la estimación del conjunto de parámetros estructurales, <span class="math inline">\(\mathbf{\beta} = \left( \beta_{1},\beta_{2},\ldots,\beta_{K} \right)'\)</span>, siendo el principal interés el obtener dichas estimaciones y realizar inferencias sobre los parámetros poblacionales y, por tanto, sobre la estructura del sistema económico analizado.</p>
<p>La finalidad última consistirá en realizar predicciones sobre valores futuros de <em>y</em>, evaluar políticas concretas (cómo varía <em>y</em> al cambiar los valores de alguna de las <em>x</em>’s) o realizar análisis de control (qué valor debe tomar alguna o algunas de las variables explicativas para alcanzar un determinado valor de <em>y</em>).</p>
<p>Como es habitual en estadística, intentaremos construir un <em>estimador</em>, es decir, una función que a cada muestra le haga corresponder un valor de los parámetros, que posea buenas propiedades. Para ello consideraremos que disponemos de una <em>muestra</em> de <em>n</em> (o <em>T</em>) observaciones de las variables <em>x</em>’s e <em>y</em>.</p>
<p>Se llama <em>modelo de regresión lineal (MRL)</em> al modelo probabilístico</p>
<p><span class="math display">\[y = \beta_{1} + \beta_{2}x_{2} + \ldots + \beta_{K}x_{K} + e\]</span></p>
<p>donde, <span class="math inline">\(y\)</span>, la <em>variable dependiente</em> o explicada, es una variable observable aleatoria; <span class="math inline">\(x_{2},x_{3},\ldots,x_{K}\)</span> las <em>variables explicativas</em>, son variables observables, aleatorias o que toman valores fijos en muestras repetidas; <span class="math inline">\(\mathbf{\beta} = \left( \beta_{1},\beta_{2},\ldots,\beta_{K} \right)'\)</span>, los <em>parámetros estructurales</em> del modelo, son un conjunto de constantes fijas y desconocidas; y <span class="math inline">\(e\)</span>, el <em>término de error</em>, es una variable aleatoria.</p>
<p>Para la estimación del modelo se dispone de una muestra de <em>n</em> observaciones <span class="math inline">\(\left( y_{i},x_{2i},\ldots,x_{Ki} \right)\)</span> independientes e idénticamente distribuidas (<em>IID</em>) extraídas de la distribución conjunta asociada al MRL, de modo que para cada <span class="math inline">\(i = 1,2,\ldots,n\)</span> se tiene que</p>
<p><span class="math display">\[y_{i} = \beta_{1} + \beta_{2}x_{2i} + \ldots + \beta_{K}x_{Ki} + e_{i}\]</span></p>
<p>Este conjunto de ecuaciones puede expresarse matricialmente por:</p>
<p><span class="math display">\[\begin{bmatrix}
y_{1} \\
\vdots \\
y_{n} \\
\end{bmatrix} = \begin{bmatrix}
1 &amp; x_{21} &amp; \ldots &amp; x_{K1} \\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots \\
1 &amp; x_{2n} &amp; \ldots &amp; x_{Kn} \\
\end{bmatrix}\begin{bmatrix}
\beta_{1} \\
\vdots \\
\beta_{K} \\
\end{bmatrix} + \begin{bmatrix}
e_{1} \\
\vdots \\
e_{n} \\
\end{bmatrix}\]</span></p>
<p>o también, de forma más compacta, como</p>
<p><span class="math display">\[\mathbf{y} = \mathbf{X\beta} + \mathbf{e}\]</span></p>
<p>donde: <span class="math inline">\(\mathbf{y} = \left( y_{1},y_{2},\ldots,y_{n} \right)'\)</span> es el <span class="math inline">\(n \times 1\)</span> vector columna de las observaciones de la variable endógena; <span class="math inline">\(\mathbf{X} = \left(\mathbf{x}_{1},\mathbf{x}_{2},\ldots,\mathbf{x}_{K} \right)\)</span> es la matriz, de orden <span class="math inline">\(n \times K\)</span> que en cada columna consigna las observaciones de las variables exógenas <span class="math inline">\(\mathbf{x}_{j}\)</span> (<span class="math inline">\(j = 1,2,\ldots,K\)</span>), considerando <span class="math inline">\(\mathbf{x}_{j} = \left( x_{j1},x_{j2},\ldots,x_{jn} \right)'\)</span>, siendo <span class="math inline">\(x_{1i} = 1\ \forall\ i = 1,2,\ldots,n\)</span>; <span class="math inline">\(\mathbf{\beta} = \left( \beta_{1},\beta_{2},\ldots,\beta_{K} \right)'\)</span> es el <span class="math inline">\(K \times 1\)</span> vector que contiene los coeficientes desconocidos (parámetros estructurales) del modelo; y <span class="math inline">\(\mathbf{e} = \left( e_{1},e_{2},\ldots,e_{n} \right)'\)</span> es el <span class="math inline">\(n \times 1\)</span> vector de los errores o perturbaciones aleatorias del modelo.</p>
</section>
<section id="hipótesis-básicas-del-mrl-condiciones-de-regularidad" class="level3" data-number="2.2.2">
<h3 data-number="2.2.2" class="anchored" data-anchor-id="hipótesis-básicas-del-mrl-condiciones-de-regularidad"><span class="header-section-number">2.2.2</span> Hipótesis básicas del MRL (condiciones de regularidad)</h3>
<p>Para que el <em>estimador de mínimos cuadrados ordinarios (MCO)</em>, que se expondrá más adelante, tenga las propiedades estadísticas adecuadas, deben cumplirse las siguientes hipótesis:</p>
<p><strong>H1</strong>: Respecto a la <em>especificación del modelo</em>:</p>
<ul>
<li><p><strong>H1.1</strong>: Aparecen todas las variables relevantes: el modelo no excluye ninguna variable explicativa esencial (<em>completitud</em>), ni incluye variables irrelevantes (<em>exactitud</em>).</p></li>
<li><p><strong>H1.2</strong>: El término de error, <span class="math inline">\(e\)</span>, actúa de forma aditiva (<em>aditividad</em>).</p></li>
<li><p><strong>H1.3</strong>: La forma funcional es correcta y, además, es lineal en los parámetros (<em>linealidad</em>).</p></li>
<li><p><strong>H1.4</strong>: Los parámetros estructurales <span class="math inline">\(\beta_{j}\)</span> son constantes (<em>estabilidad estructural</em>).</p></li>
</ul>
<p><strong>H2</strong>: Respecto a los <em>regresores del modelo</em>:</p>
<ul>
<li><p><strong>H2.1:</strong> O bien las variables explicativas son no estocásticas y toman valores fijos en muestras repetidas (y además los valores no son todos iguales entre si, es decir, la varianza muestral debe ser distinta de cero) o, en el caso más común en economía, donde los datos son de tipo no experimental, si las variables explicativas son aleatorias, deben estar incorrelacionadas con los factores contenidos en el término de error, <span class="math inline">\(E(\mathbf{e}|\mathbf{X}) = \mathbf{0}\)</span> (<em>exogeneidad estricta</em>).</p></li>
<li><p><strong>H2.2:</strong> Debe cumplirse que <span class="math inline">\(n &gt; \ K\)</span> y <span class="math inline">\(\ rango\left( \mathbf{X} \right) = K\)</span>, es decir, (1) debe haber un número mayor de observaciones que de parámetros a estimar, y (2) puesto que el <em>rango</em> de la matriz <em>X</em> debe ser <em>completo</em> (<span class="math inline">\(K\)</span>), ninguna variable explicativa puede ser combinación lineal exacta de las restantes, es decir, deben ser linealmente independientes (<em>ausencia de colinealidad perfecta</em>).</p></li>
</ul>
<p><strong>H3</strong>: Respecto a los <em>errores del modelo</em>:</p>
<ul>
<li><p><strong>H3.1</strong>: <span class="math inline">\(Var\left( e_{i}|\mathbf{X} \right) = \sigma^{2}\ \ \forall\ i = 1,2,\ldots,n\)</span> (<em>homoscedasticidad</em>). El parámetro de escala <span class="math inline">\(\sigma\)</span> se conoce como desviación estándar de la regresión (o del modelo), midiendo “el tamaño medio de los errores del modelo”.</p></li>
<li><p><strong>H3.2</strong>: <span class="math inline">\(Cov\left( e_{i},e_{j}|\mathbf{X} \right) = 0\ \ \ \forall\ i \neq j\)</span> (<em>ausencia de correlación</em>).</p></li>
<li><p><strong>H3.3</strong>: Los errores siguen una distribución normal, <span class="math inline">\(e_{i}|\mathbf{X}\sim N\ \ \forall\ i = 1,2,\ldots,n\)</span> (<em>normalidad</em>).</p></li>
</ul>
</section>
<section id="algunas-propiedades-interpretaciones-o-representaciones-del-mrl" class="level3" data-number="2.2.3">
<h3 data-number="2.2.3" class="anchored" data-anchor-id="algunas-propiedades-interpretaciones-o-representaciones-del-mrl"><span class="header-section-number">2.2.3</span> Algunas propiedades, interpretaciones o representaciones del MRL</h3>
<ul>
<li><p>La hipótesis de exogeneidad estricta implica que <span class="math inline">\(E\left( e_{i} \right) = 0\ \ \forall\ i\)</span>, <span class="math inline">\(E(\mathbf{Xe}) = \mathbf{0}\)</span> y <span class="math inline">\(Cov(\mathbf{X},\mathbf{e}) = \mathbf{0}\)</span>. En este sentido, en lugar de la propiedad estricta, suele utilizarse la hipótesis derivada <span class="math inline">\(Cov\left( \mathbf{x}_{j},\mathbf{e} \right) = 0\)</span> para cada variable explicativa <span class="math inline">\(x_{j}\)</span>, <span class="math inline">\(j = 1,2,\ldots,K\)</span> (<em>exogeneidad débil</em>), lo que significa que no debe existir ninguna correlación lineal entre las variables explicativas y el término de error. Si no se verifica esta hipótesis, es decir, cuando <span class="math inline">\(Cov\left( \mathbf{x}_{k},\mathbf{e} \right) \neq 0\)</span> para alguna variable explicativa <span class="math inline">\(\mathbf{x}_{k}\)</span>, se dice que ese <em>regresor</em> es <em>endógeno</em>, y en este caso el estimador de mínimos cuadrados ordinarios que propondremos más adelante deja de tener buenas propiedades estadísticas (en concreto, será sesgado e inconsistente).</p></li>
<li><p>La ausencia de multicolinealidad estricta implica que <span class="math inline">\(\det(\mathbf{X}^{\mathbf{'}}\mathbf{X}) \neq \mathbf{0}\)</span>, lo que garantiza la existencia de la inversa de la matriz <span class="math inline">\(\left( \mathbf{X}\mathbf{'}\mathbf{X} \right)\)</span> que aparece en el cálculo de las estimacones mínimo-cuadráticas.</p></li>
<li><p>Las sub-hipótesis H3.2 y H3.3 pueden agruparse en una única expresión matricial: <span class="math display">\[\mathbf{\Omega} = Cov\left( \mathbf{e} \right) = \begin{bmatrix}
Var\left( e_{1} \right) &amp; \cdots &amp; Cov\ \left( e_{1},e_{n} \right) \\
\vdots &amp; \ddots &amp; \vdots \\Cov\ \left( e_{1},e_{n} \right) &amp; \cdots &amp; Var\left( e_{n} \right) \\
\end{bmatrix} = \begin{bmatrix}
\sigma^{2} &amp; \cdots &amp; 0 \\
\vdots &amp; \ddots &amp; \vdots \\
0 &amp; \cdots &amp; \sigma^{2} \\
\end{bmatrix} = \sigma^{2}\mathbf{I}\]</span></p></li>
</ul>
<p>Las perturbaciones que cumplen estas dos propiedades (homoscedasticidad y ausencia de correlación) son conocidas como <em>perturbaciones esféricas</em>.</p>
<ul>
<li>Puesto que se cumple que <span class="math display">\[\mathbf{e}\mathbf{|}\mathbf{X}\sim N_{n}(\mathbf{0}_{n},\sigma^{2}\mathbf{I}_{n}\mathbf{)}\]</span> en términos de la variable dependiente la expresión anterior es equivalente a decir que el vector de respuestas <span class="math inline">\(\mathbf{y}\)</span> sigue tiene una distribución normal multivariante</li>
</ul>
<p><span class="math display">\[\mathbf{y}\mathbf{|}\mathbf{X}\sim N_{n}(\mathbf{X\beta},\sigma^{2}\mathbf{I}_{n}\mathbf{)}\]</span></p>
<p>Se tiene entonces que, al igual que para el vector de errores <strong><em>e</em></strong>, la matriz de covarianzas de <span class="math inline">\(\mathbf{y}\)</span> viene dada por <span class="math inline">\(Cov\left( \mathbf{y}\mathbf{|}\mathbf{X} \right) = \sigma^{2}\mathbf{I}_{n}\)</span>.</p>
<p>De igual manera, la <em>función de regresión poblacional</em> es una función lineal dada por <span class="math display">\[E(y_{i}\mathbf{|}\mathbf{X}\mathbf{) =}\mathbf{X}_{i}\mathbf{\beta}\]</span> siendo <span class="math inline">\(\mathbf{X}_{i} = \left( 1,x_{2i},\ldots,x_{Ki} \right)\)</span> la <em>i</em>-ésima fila de la matriz <span class="math inline">\(\mathbf{X}\)</span>, que también puede escribirse en forma no matricial como</p>
<p><span class="math display">\[E\left( y_{i}|x_{2i},\ldots,x_{Ki} \right) = \beta_{1} + \beta_{2}x_{2i} + \ldots + \beta_{K}x_{Ki}\]</span></p>
<p>verificándose, por tanto, que</p>
<p><span class="math display">\[\beta_{j} = \frac{\partial E\left( y|\mathbf{X} \right)}{\partial x_{j}}\]</span></p>
<p>Entonces, cada parámetro <span class="math inline">\(\beta_{j}\)</span> mide aproximadamente el <em>efecto parcial</em> (tasa de cambio marginal) sobre la media (condicional) de la variable dependiente <span class="math inline">\(y\)</span> de un cambio unitario en la variable explicativa <span class="math inline">\(x_{j}\)</span>, bajo la hipótesis de que el resto de las variables explicativas permanece constante (supuesto ‘<em>ceteris paribus</em>’ que, en la práctica, dado el carácter en general ‘observacional’ de los datos económicos, debe ser asumido con cautela).</p>
<p>Una vez estimado el modelo, al substituir los parámetros desconocidos por sus estimaciones, se tiene entonces que:</p>
<p><span class="math display">\[{\widehat{\beta}}_{j} \cong \mathrm{\Delta}\widehat{E}\left( y|x_{2},\ldots,x_{K} \right)\ \ cuando\ \ \mathrm{\Delta}x_{j} = 1\ \ y\ \ \mathrm{\Delta}x_{i} = 0\ \ para\ \ i \neq j\]</span></p>
<p>Se puede visualizar el efecto de una variable <span class="math inline">\(x_{j}\)</span> sobre la media condicional estimada fijando el valor del resto de regresores <span class="math inline">\(x_{i}\ (i \neq j)\)</span> en los valores medios muestrales <span class="math inline">\(x_{i} = {\overline{x}}_{i}\)</span>, y representando la variable <span class="math inline">\(E(y|{\overline{x}}_{2},\ldots,x_{j},\ldots,{\overline{x}}_{K})\)</span> como función de <span class="math inline">\(x_{j}\)</span>. Estas figuras se conocen como <em>gráficas de efectos (’effect plots</em>’), y suelen representarse junto con el intervalo de confianza para los valores estimados. Contienen, por tanto, una información similar a las parejas parámetro-desviación típica estimados, <span class="math inline">\(\left( {\widehat{\beta}}_{j},se\left( {\widehat{\beta}}_{j} \right) \right)\)</span>, que se verán más adelante, y que miden el efecto parcial, y su variabilidad, de cada variable explicativa.</p>
<p>Por otra parte, en la discusión anterior se está asumiendo que cuando un regresor cambia el resto de las variables explicativas permanece constante, lo que implica que no existe correlación entre ellas. Si quiere medirse el efecto de una variable explicativa cuando ya existen otros regresores (correlacionados o no con la primera), suelen utilizarse las llamadas <em>gráficas de variables añadidas (’added-variable plots</em>’), que miden el efecto de una variable sobre la media condicional descontando el efecto del resto de regresores. Se construyen realizando regresiones auxiliares de la variable dependiente y la variable explicativa de interés sobre el resto de los regresores, y representando posteriormente la parte no explicada de ambas regresiones (los residuos) una frente a la otra. La pendiente de dicha regresión coincide con el parámetro <span class="math inline">\(\beta\)</span> correspondiente a la regresión que incluye todas las variables del modelo, lo que sugiere que las estimaciones de un modelo de regresión lineal múltiple llevan descontado el efecto del resto de regresores en la función media.</p>
<p>Como resumen de este compendio de anotaciones sobre el MRL, el modelo de regresión lineal <span class="math inline">\(\mathbf{y} = \mathbf{X\beta} + \mathbf{e}\)</span> debe cumplir las siguientes hipótesis:</p>
<ul>
<li><p>(H1-<em>Linealidad</em>): <span class="math inline">\(E\left( y_{i} \middle| \mathbf{X} \right) = \mathbf{X}_{i}\mathbf{\beta}\)</span></p></li>
<li><p>(H2-<em>Homoscedasticidad</em>): <span class="math inline">\(Var\left( y_{i} \middle| \mathbf{X} \right) = \sigma^{2}\ ,\ \ i = 1,2,\ldots,n\)</span></p></li>
<li><p>(H3-<em>Incorrelación</em>): <span class="math inline">\(Cov\left( y_{i},y_{j} \middle| \mathbf{X} \right) = 0&nbsp;&nbsp;,&nbsp;&nbsp;i \neq j\)</span></p></li>
<li><p>(H4-<em>Normalidad</em>): <span class="math inline">\(y_{i}\mathbf{|}\mathbf{X}\ \sim N\left( \mathbf{X}_{i}\mathbf{\beta},\sigma^{2} \right)\ \ ,\ \ i = 1,2,\ldots,n\)</span></p></li>
<li><p>(H5-<em>Exogeneidad</em>): <span class="math inline">\(Cov\left( \mathbf{x}_{k},\mathbf{e} \right) = 0&nbsp;,&nbsp;&nbsp;k = 1,2,\ldots,K\)</span></p></li>
</ul>
</section>
<section id="estimación-de-los-parámetros-estructurales-el-método-de-mínimos-cuadrados-ordinarios-mco" class="level3" data-number="2.2.4">
<h3 data-number="2.2.4" class="anchored" data-anchor-id="estimación-de-los-parámetros-estructurales-el-método-de-mínimos-cuadrados-ordinarios-mco"><span class="header-section-number">2.2.4</span> Estimación de los parámetros estructurales: el método de mínimos cuadrados ordinarios (MCO)</h3>
<p>Sea <span class="math inline">\(E(y_{i}|x_{2i},\ldots,x_{Ki}) = \beta_{1} + \beta_{2}x_{2i} + \ldots + \beta_{K}x_{Ki}\)</span> la función de regresión poblacional que se deriva de un cierto modelo económico, es decir, <span class="math inline">\(E(y_{i})\)</span> representa el valor esperado (condicional) de la variable <em>y</em> dado un cierto valor de las variables <em>x</em>’s. Dada la naturaleza aleatoria de la variable dependiente, los valores que se observan de <em>y</em> diferirán generalmente de <span class="math inline">\(E(y_{i})\)</span> por múltiples causas, por lo que se tendrá la relación <span class="math inline">\(y_{i} = E\left( y_{i} \right) + e_{i}\)</span> donde el término de error <span class="math inline">\(e_{i}\)</span> representa el efecto combinado de esos factores que ‘perturban’ la relación teórica.</p>
<p>En el caso del modelo con una sola variable explicativa, la representación gráfica de la relación <span class="math inline">\(y_{i} = E\left( y_{i} \right) + e_{i}\)</span> sería la siguiente:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/MLR-1.png" class="img-fluid figure-img" width="400"></p>
<figcaption class="figure-caption">Figura 1</figcaption>
</figure>
</div>
<p>Supongamos que se dispone de una <em>muestra de observaciones independientes</em> para un conjunto de individuos (<span class="math inline">\(i = 1,\ldots,n\)</span>) o de períodos de tiempo (<span class="math inline">\(t = 1,\ldots,T\)</span>). El método de <em>mínimos cuadrados ordinarios</em> (<em>MCO</em>) toma como estimación de la regresión poblacional la función más próxima al conjunto de puntos muestrales, para lo cual encuentra <em>estimaciones</em> <span class="math inline">\(b_{i}\)</span> para cada parámetro <span class="math inline">\(\beta_{i}\)</span> de la regresión poblacional.</p>
<p>Formalmente, si se define la <em>función de regresión muestral</em> como</p>
<p><span class="math display">\[\widehat{y} = b_{1} + b_{2}x_{2} + \ldots + b_{K}x_{K}\]</span></p>
<p>entonces para cada observación de las variables exógenas <span class="math inline">\(\mathbf{X}_{i} = \left( 1,x_{2i},\ldots,x_{Ki} \right)\)</span> el correspondiente punto sobre dicha función viene dado por <span class="math inline">\({\widehat{y}}_{i} = b_{1} + b_{2}x_{2i} + \ldots + b_{K}x_{Ki} = \mathbf{X}_{i}\mathbf{b}\)</span>. Si se definen los errores estimados (<em>residuos</em>) del modelo, para cada observación, como las distancias verticales</p>
<p><span class="math display">\[{\widehat{e}}_{i} = y_{i} - {\widehat{y}}_{i}\]</span></p>
<p>el método de mínimos cuadrados intenta minimizar la suma de los cuadrados de los residuos:</p>
<p><span class="math display">\[S\left( b_{1},b_{2},\ldots,b_{K} \right) = \sum_{i = 1}^{n}{{\widehat{e}}_{i}^{2} = \sum_{i = 1}^{n}\left( y_{i} - {\widehat{y}}_{i} \right)^{2}}\]</span></p>
<p>Gráficamente, para el modelo de dos variables, la recta MCO estimada sería como la mostrada a continuación:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/MLR-3.png" class="img-fluid figure-img" width="400"></p>
<figcaption class="figure-caption">Figura 2</figcaption>
</figure>
</div>
<p>Técnicamente, la función a minimizar se puede expresar matricialmente de la forma siguiente:</p>
<p><span class="math display">\[S\left( \mathbf{b} \right) = \sum_{i = 1}^{n}{{\widehat{e}}_{i}^{2} = {\widehat{\mathbf{e}}}^{\mathbf{'}}\mathbf{e}\mathbf{= (}\mathbf{y}\mathbf{-}\mathbf{Xb}\mathbf{)'(}\mathbf{y}\mathbf{-}\mathbf{Xb}\mathbf{)}}\]</span></p>
<p>Para obtener los valores de las estimaciones que minimizan dicha función derivamos respecto de <span class="math inline">\(\mathbf{b}\)</span>, e igualamos a 0 (<em>condición de primer orden</em>), obteniéndose el siguiente conjunto de ecuaciones:</p>
<p><span class="math display">\[\frac{\partial S\left( \mathbf{b} \right)}{\partial\mathbf{b}} = \mathbf{0}\ \  \Longleftrightarrow \ \ \mathbf{X}^{\mathbf{'}}\mathbf{Xb}\mathbf{=}\mathbf{X}^{\mathbf{'}}\mathbf{y}\]</span></p>
<p>que es conocido como <em>sistema de ecuaciones normales</em>.</p>
<p>Como <span class="math inline">\(\left( \mathbf{X}\mathbf{'}\mathbf{X} \right)\)</span> es una matriz cuadrada de orden <em>K</em>, y rango también igual a <em>K</em>, tiene matriz inversa, dada por <span class="math inline">\(\left( \mathbf{X}\mathbf{'}\mathbf{X} \right)^{- 1}\)</span>, lo cual nos permite pre-multiplicar las ecuaciones normales por dicha matriz inversa, obteniéndose las <em>estimaciones MCO</em> del vector <em>β</em>, dadas por</p>
<p><span class="math display">\[\mathbf{b}\mathbf{=}\left( \mathbf{X}\mathbf{'}\mathbf{X} \right)^{\mathbf{-}\mathbf{1}}\mathbf{X}^{\mathbf{'}}\mathbf{y}\]</span></p>
</section>
<section id="propiedades-estadísticas-y-algebraicas-del-estimador-mco-widehatmathbfbeta" class="level3" data-number="2.2.5">
<h3 data-number="2.2.5" class="anchored" data-anchor-id="propiedades-estadísticas-y-algebraicas-del-estimador-mco-widehatmathbfbeta"><span class="header-section-number">2.2.5</span> Propiedades estadísticas y algebraicas del estimador MCO, <span class="math inline">\(\widehat{\mathbf{\beta}}\)</span></h3>
<p>Cuando la expresión <span class="math inline">\(\mathbf{b} = \left( \mathbf{X}\mathbf{'}\mathbf{X} \right)^{- 1}\mathbf{X}\mathbf{'}\mathbf{y}\)</span> se considera función de los valores de la variable <strong><em>y</em></strong> para distintas muestras, hablaremos del <em>estimador MCO</em>,</p>
<p><span class="math display">\[\widehat{\mathbf{\beta}}\mathbf{=}\left( \mathbf{X}\mathbf{'}\mathbf{X} \right)^{\mathbf{-}\mathbf{1}}\mathbf{X}^{\mathbf{'}}\mathbf{y}\]</span></p>
<p>del cual cabe preguntarse, al tratarse de una variable aleatoria, qué propiedades caracterizan su distribución. Pues bien, si se verifican todas las hipótesis hechas para el MRL, las propiedades del estimador MCO son las siguientes:</p>
<p>1. El estimador <span class="math inline">\(\widehat{\mathbf{\beta}} = \left( \mathbf{X}\mathbf{'}\mathbf{X} \right)^{- 1}\mathbf{X}\mathbf{'}\mathbf{y}\)</span> es una función <em>lineal</em> de <strong><em>y</em></strong>.</p>
<p>2. <span class="math inline">\(E\left( \widehat{\mathbf{\beta}}\mathbf{|}\mathbf{X} \right) = \mathbf{\beta}\)</span>, es decir, <span class="math inline">\(\widehat{\mathbf{\beta}}\)</span> es un estimador <em>insesgado</em>.</p>
<p>3. La matriz de varianzas-covarianzas viene dada por <span class="math inline">\(Cov\left( \widehat{\mathbf{\beta}}\mathbf{|}\mathbf{X} \right) = \sigma^{2}\left( \mathbf{X}\mathbf{'}\mathbf{X} \right)^{\mathbf{-}\mathbf{1}}\)</span>.</p>
<p>4. <em>Teorema de Gauss-Markov</em>: el estimador <span class="math inline">\(\widehat{\mathbf{\beta}}\)</span> es el <em>mejor estimador lineal insesgado (MELI)</em> de los parámetros estructurales del modelo de regresión, proporcionando varianzas mínimas dentro de esa clase de estimadores.</p>
<p>5. Si se cumple la hipótesis de normalidad, se puede demostrar que el estimador MCO es también <em>eficiente</em>, es decir, el mejor de todos los estimadores insesgados, sean lineales o no. Esta propiedad se deriva del hecho de que, asumiendo la normalidad de los errores, el estimador MCO del vector de parámetros <span class="math inline">\(\mathbf{\beta}\)</span> coincide con el <em>estimador de máxima verosimilitud (MV)</em>. Puede demostrarse fácilmente este resultado teniendo en cuenta que el logaritmo de la función de verosimilitud del modelo de regresión lineal múltiple normal viene dado por la expresión <span class="math inline">\(\log{\left\lbrack L(\mathbf{\beta},\sigma^{2}|\mathbf{y} \right\rbrack = - {\frac{n}{2}\log(2\pi)} - {\frac{n}{2}\log{\left( \sigma^{2} \right) - {\frac{1}{2\sigma^{2}}\sum_{i = 1}^{n}\left( y_{i} - \mathbf{X}_{i}\mathbf{\beta} \right)^{2}}}}}\)</span> y que el vector <span class="math inline">\(\mathbf{\beta}\)</span> sólo aparece en el último término, lo que implica que maximizar <span class="math inline">\({log}L\)</span> respecto a <span class="math inline">\(\mathbf{\beta}\)</span> equivale a minimizar <span class="math inline">\(\sum_{i = 1}^{n}\left( y_{i} - \mathbf{X}_{i}\mathbf{\beta} \right)^{2}\)</span> respecto a dicho vector. Por otra parte, diferenciando <span class="math inline">\(\log L\)</span> respecto a <span class="math inline">\(\sigma^{2}\)</span> e igualando a cero el resultado, se obtiene la estimación MV de la varianza residual, <span class="math inline">\({\widetilde{\sigma}}^{2} = \frac{\sum_{i = 1}^{n}\left( y_{i} - \mathbf{X}_{i}\widehat{\mathbf{\beta}} \right)^{2}}{n}\)</span>, similar a la estimación MCO de dicho parámetro que se verá más adelante, salvo por el hecho de que aquí en el cociente aparece <em>n</em> en lugar de <em>n</em>-<em>K</em>.</p>
<p>6. Bajo la hipótesis de normalidad, y teniendo en cuenta 2 y 3, se tiene que <span class="math inline">\(\widehat{\mathbf{\beta}}\sim N\left( \mathbf{\beta},\sigma^{2}\left( \mathbf{X}\mathbf{'}\mathbf{X} \right)^{\mathbf{-}\mathbf{1}} \right)\)</span>, es decir, el estimador <span class="math inline">\(\widehat{\mathbf{\beta}}\)</span> sigue una distribución normal multivariante.</p>
<p>7. El estimador MCO, <span class="math inline">\(\widehat{\mathbf{\beta}}\)</span>, es <em>consistente</em>, <span class="math inline">\(plim\left( \widehat{\mathbf{\beta}} \right) = \mathbf{\beta}\)</span>, lo que significa que conforme aumenta el tamaño de la muestra crece la probabilidad de que las estimaciones se encuentren cerca de los valores poblacionales verdaderos que se intentan estimar: <span class="math display">\[\ \lim_{n \rightarrow \infty}{P\left( \left| {\widehat{\beta}}_{j} - \beta_{j} \right| &gt; \varepsilon \right)} = 0\ \ \ \ \ \forall\varepsilon &gt; 0\]</span> Una condición necesaria para que se cumpla la propiedad de consistencia es que se verifique que <span class="math inline">\(\lim_{n \rightarrow \infty}{E\left( {\widehat{\beta}}_{j} \right) = \beta_{j}}\)</span> y <span class="math inline">\(\lim_{n \rightarrow \infty}{Var\left( {\widehat{\beta}}_{j} \right) = 0}\)</span>].</p>
</section>
<section id="propiedades-algebraicas-de-los-residuos-y-de-la-regresión-mco" class="level3" data-number="2.2.6">
<h3 data-number="2.2.6" class="anchored" data-anchor-id="propiedades-algebraicas-de-los-residuos-y-de-la-regresión-mco"><span class="header-section-number">2.2.6</span> Propiedades algebraicas de los residuos y de la regresión MCO</h3>
<p>En una aplicación concreta, una vez que se han calculado las estimaciones MCO, se verifica que:</p>
<p>1. Los residuos del modelo, <span class="math inline">\({\widehat{e}}_{i} = y_{i} - {\widehat{y}}_{i}\)</span>, cumplen <span class="math inline">\(\sum_{i = 1}^{n}{x_{ji}\widehat{e}}_{i} = 0,\ \ j = 2,\ldots,\ K\)</span> y, por tanto, el vector de residuos es ortogonal a la matriz de variables explicativas: <span class="math inline">\(\mathbf{X}^{'}\widehat{\mathbf{e}} = 0\)</span></p>
<p>2. La suma de los residuos es cero, <span class="math inline">\(\sum_{i = 1}^{n}{\widehat{e}}_{i} = 0\)</span> y, por tanto, la media muestral de los mismos es nula, <span class="math inline">\(\overline{\widehat{\mathbf{e}}} = 0\)</span>.</p>
<p>3. Puesto que la media de los errores estimados es cero, debe cumplirse entonces que <span class="math inline">\(\overline{y} = b_{1} + b_{2}{\overline{x}}_{2} + \ldots + b_{K}{\overline{x}}_{K}\)</span> y, por tanto, la función de regresión muestral siempre pasa por la media muestral de los datos.</p>
</section>
<section id="estimación-de-la-varianza-de-los-errores-mathbfsigmamathbf2" class="level3" data-number="2.2.7">
<h3 data-number="2.2.7" class="anchored" data-anchor-id="estimación-de-la-varianza-de-los-errores-mathbfsigmamathbf2"><span class="header-section-number">2.2.7</span> Estimación de la varianza de los errores, <span class="math inline">\(\mathbf{\sigma}^{\mathbf{2}}\)</span></h3>
<p>Dado que <span class="math inline">\(Cov\left( \widehat{\mathbf{\beta}}|\mathbf{X} \right) = \sigma^{2}{(\mathbf{X}^{'}\mathbf{X})}^{- 1}\)</span>, para conocer la precisión con la que se estiman los parámetros se necesita estimar la varianza de las perturbaciones aleatorias, <span class="math inline">\(\sigma^{2}\)</span>. Teniendo en cuenta que <span class="math inline">\(\sigma^{2} = Var\left( e_{i} \right) = E\left( e_{i}^{2} \right)\)</span>, como estimación de la varianza poblacional <span class="math inline">\(\mathbf{\sigma}^{\mathbf{2}}\)</span> se toma la <em>varianza residual muestral</em> <span class="math inline">\(\mathbf{s}^{\mathbf{2}}\)</span>, dada por la expresión</p>
<p><span class="math display">\[s^{2} = \frac{\sum_{i = 1}^{n}{\widehat{e}}_{\mathbf{i}}^{\mathbf{2}}}{n - K}\]</span></p>
<p>siendo su raíz cuadrada, <span class="math inline">\(s\)</span>, el <em>error estándar estimado</em>, también llamado desviación típica de la regresión.</p>
<p>Cuando la expresión de <span class="math inline">\(s^{\mathbf{2}}\)</span> se considera función de los valores de la variable <strong><em>y</em></strong> para distintas muestras, hablaremos del *estimador de la varianza residual,</p>
<p><span class="math display">\[{\widehat{\sigma}}^{2} = \frac{\mathbf{y}^{'}\left( \mathbf{I} - {\mathbf{X}\left( \mathbf{X}\mathbf{'}\mathbf{X} \right)}^{\mathbf{-}\mathbf{1}}\mathbf{X}^{\mathbf{'}} \right)\mathbf{y}}{n - K} = \frac{{\widehat{\mathbf{e}}}^{\mathbf{'}}\widehat{\mathbf{e}}}{n - K}\]</span></p>
<p>Al igual que en el caso del estimador MCO del vector de parámetros estructurales <strong><em>β</em></strong>, se puede demostrar que la varianza residual es un estimador insesgado de <em>σ</em><sup>2</sup>, cumpliéndose entonces que <span class="math inline">\(\ E({\widehat{\sigma}}^{2}) = \sigma^{2}\)</span>.</p>
<p>En el caso del estimador MV de la varianza residual, se puede demostrar que <span class="math inline">\(E\left( {\widetilde{\sigma}}^{2} \right) = \frac{n - K}{n}\sigma^{2}\)</span>. Por tanto, aunque el estimador es sesgado, dicho sesgo desaparece asintóticamente al cumplirse que <span class="math inline">\(\lim_{n \rightarrow \infty}{\left( \frac{n - K}{n} \right) = 1}\)</span>.</p>
</section>
</section>
<section id="grado-de-ajuste-de-una-regresión" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="grado-de-ajuste-de-una-regresión"><span class="header-section-number">2.3</span> Grado de ajuste de una regresión</h2>
<p>Se entiende por grado (o bondad) de(l) ajuste de una regresión como “el nivel de proximidad de la función de regresión muestral, <span class="math inline">\({\widehat{y}}_{i} = b_{1} + b_{2}x_{2i} + \ldots + b_{K}x_{Ki}\)</span>, al conjunto de datos que sirve de soporte para la estimación”.</p>
<p>Como se ha mencionado anteriormente, la regresión estimada por MCO es la que proporciona la menor suma de los cuadrados de los residuos, <span class="math inline">\(SRC = \sum_{i = 1}^{n}{\widehat{e}}_{i}^{2} = \sum_{i = 1}^{n}\left( y_{i} - {\widehat{y}}_{i} \right)^{2}\)</span>, que llamaremos <em>suma residual de cuadrados</em>. Esta medida podría ser una primera aproximación válida del ajuste de una regresión, pero se ve afectada por las unidades de medida de cada una de las variables, de modo que es posible variar su magnitud solo con el cambio de escala de alguna de los regresores. Este hecho hace imprescindible la introducción de otras medidas de ajuste que sean homogéneas y que, en caso necesario, permitan comparar la bondad del ajuste entre modelos diferentes.</p>
<section id="coeficiente-de-determinación" class="level3" data-number="2.3.1">
<h3 data-number="2.3.1" class="anchored" data-anchor-id="coeficiente-de-determinación"><span class="header-section-number">2.3.1</span> Coeficiente de determinación</h3>
<p>Tal y como se muestra en la gráfica posterior, en el modelo de regresión lineal general, una vez estimada la función de regresión muestral, se cumple la igualdad:</p>
<p><span class="math inline">\(y_{i} - \overline{y} = \left( {\widehat{y}}_{i} - \overline{y} \right) + \left( y_{i} - {\widehat{y}}_{i} \right) = \left( {\widehat{y}}_{i} - \overline{y} \right) + {\widehat{e}}_{i}\)</span>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/MLR-4.png" class="img-fluid figure-img" width="400"></p>
<figcaption class="figure-caption">Figura 3</figcaption>
</figure>
</div>
<p>Por tanto, puede descomponerse la variabilidad total de los valores de la variable endógena alrededor de su media,</p>
<p><span class="math display">\[STC = \Sigma(y_{i} - \overline{y})^{2}\]</span></p>
<p>en dos partes, la suma de cuadrados explicada por la regresión (<em>SEC</em>) y la suma residual de cuadrados (<em>SRC</em>)</p>
<p><span class="math display">\[STC = SEC + SRC\]</span></p>
<p>es decir,</p>
<p><span class="math display">\[\Sigma(y_{i} - \overline{y})^{2} = \Sigma({\widehat{y}}_{i} - \overline{y})^{2} + \Sigma{ê_{i}}^{2}\]</span></p>
<p>A partir de esta descomposición de la variación total de la variable <em>y</em>, se define el <em>coeficiente de determinación</em> como la proporción de la variación total de <em>y</em> explicada por la regresión, es decir,</p>
<p><span class="math display">\[R^{2} = \frac{SEC}{STC} = 1 - \frac{SRC}{STC}\]</span></p>
<p>Este coeficiente cumple las siguientes propiedades:</p>
<ul>
<li><p><span class="math inline">\(0 \leq R^{2} \leq 1\)</span>.</p></li>
<li><p><span class="math inline">\(R^{2} = \left( r_{y,\widehat{y}} \right)^{2}\)</span>, es decir, el &gt; coeficiente de determinación coincide con el cuadrado del &gt; coeficiente de correlación lineal entre los valores observados de &gt; <span class="math inline">\(y\)</span> y los valores estimados <span class="math inline">\(\widehat{y}\)</span>.</p></li>
<li><p><span class="math inline">\(R^{2} = 1\)</span> cuando <span class="math inline">\(STC = SEC\)</span> o, equivalentemente, <span class="math inline">\(SRC = 0\)</span>; es &gt; decir, cuando todos los puntos muestrales están sobre la función &gt; de regresión (<span class="math inline">\(\sum_{i = 1}^{n}{\widehat{e}}_{i}^{2} = 0\)</span>) y, por &gt; tanto, el ajuste del modelo de regresión es perfecto.</p></li>
<li><p><span class="math inline">\(R^{2} = 0\)</span> cuando <span class="math inline">\(SEC = 0\)</span>, es decir, <span class="math inline">\(STC = SRC\)</span> y, por tanto, &gt; todos los <span class="math inline">\(b_{i}\)</span>, para <span class="math inline">\(i &gt; 1\)</span>, son nulos y el modelo se reduce a &gt; <span class="math inline">\(y_{i} = \overline{y} + e_{i}\)</span>. Consecuentemente, las variables &gt; exógenas no tendrán ninguna influencia en la variación de los &gt; valores de variable <em>y</em>, es decir, no tendrán ningún valor &gt; explicativo y el ajuste del modelo de regresión es nulo.</p></li>
</ul>
</section>
<section id="coeficiente-de-determinación-ajustado" class="level3" data-number="2.3.2">
<h3 data-number="2.3.2" class="anchored" data-anchor-id="coeficiente-de-determinación-ajustado"><span class="header-section-number">2.3.2</span> Coeficiente de determinación ajustado</h3>
<p>Se puede demostrar que, si se incrementa el número de variables explicativas del modelo, aunque no tengan relación alguna con la variable dependiente, nunca disminuye el <span class="math inline">\(R^{2}\)</span> de la regresión. Esto hace que comparar dos regresiones en base al <span class="math inline">\(R^{2}\)</span> obtenido, con la misma variable dependiente y diferente número de regresores, no sea fiable. De hecho, la inclusión discrecional en el modelo de variables explicativas ajenas al sistema económico estudiado puede elevar artificialmente su valor.</p>
<p>Por tal razón, y con el fin de descontar el efecto que el número de variables explicativas (y la consiguiente pérdida de grados de libertad) introduce en el <span class="math inline">\(R^{2}\)</span>, se define el <em>coeficiente de determinación ajustado</em> (ajusta el <span class="math inline">\(R^{2}\)</span> original por los grados de libertad de cada suma de cuadrados):</p>
<p><span class="math display">\[{\overline{R}}^{2} = 1 - \frac{\frac{SRC}{(n - K)}}{\frac{STC}{(n - 1)}}\]</span></p>
<p>De su definición, es inmediata la relación</p>
<p><span class="math display">\[{\overline{R}}^{2} = 1 - (1 - R^{2})\frac{n - 1}{n - K}\]</span></p>
<p>y, por tanto, el aumento del valor del <span class="math inline">\({\overline{R}}^{2}\)</span> al incluir una nueva variable explicativa dependerá de su contribución ‘real’ al modelo y, a diferencia de lo que ocurre con el <span class="math inline">\(R^{2}\)</span> estándar, puede disminuir cuando se incorporan nuevas variables si no compensa la pérdida de grados de libertad.</p>
<p>Puede observarse, de la propia definición del estadístico, que se cumple la igualdad</p>
<p><span class="math display">\[{\overline{R}}^{2} = 1 - \frac{{\widehat{\sigma}}^{2}}{\frac{STC}{(n - 1)}}\]</span></p>
<p>lo que implica que cuanto mayor sea la dispersión de los residuos, es decir, cuando más elevada sea la variación de las desviaciones de los valores observados respecto a la función de regresión muestral estimada, menor será el ajuste proporcionado por el modelo. Esta expresión nos muestra también que conseguir un buen ajuste del modelo, maximizando el estadístico <span class="math inline">\({\overline{R}}^{2}\)</span>, es equivalente a minimizar la varianza residual de la regresión, <span class="math inline">\({\widehat{\sigma}}^{2}\)</span>.</p>
</section>
</section>
<section id="inferencia-estadística-i-intervalos-de-confianza-y-contrastes-de-hipótesis-para-los-parámetros-del-modelo" class="level2" data-number="2.4">
<h2 data-number="2.4" class="anchored" data-anchor-id="inferencia-estadística-i-intervalos-de-confianza-y-contrastes-de-hipótesis-para-los-parámetros-del-modelo"><span class="header-section-number">2.4</span> Inferencia estadística (I): Intervalos de confianza y contrastes de hipótesis para los parámetros del modelo</h2>
<section id="distribución-de-los-estimadores-de-los-parámetros-estructurales" class="level3" data-number="2.4.1">
<h3 data-number="2.4.1" class="anchored" data-anchor-id="distribución-de-los-estimadores-de-los-parámetros-estructurales"><span class="header-section-number">2.4.1</span> Distribución de los estimadores de los parámetros estructurales</h3>
<p>De las hipótesis hechas para el modelo de regresión lineal se tiene que <span class="math inline">\(\widehat{\mathbf{\beta}}\sim N(\mathbf{\beta},\sigma^{2}\left( \mathbf{X}^{\mathbf{'}}\mathbf{X} \right)^{- 1})\)</span>. Por tanto, las distribuciones marginales vienen dadas por</p>
<p><span class="math display">\[{\widehat{\beta}}_{k}\sim N(\beta_{k},\sigma^{2}a_{kk})\]</span></p>
<p>donde <span class="math inline">\(a_{kk}\)</span> es el elemento <em>k</em>-ésimo de la diagonal de la matriz <span class="math inline">\({(\mathbf{X}^{'}\mathbf{X})}^{- 1}\)</span> y, en consecuencia, cada elemento <span class="math inline">\(\sigma^{2}a_{kk}\)</span> representa la varianza del estimador <span class="math inline">\({\widehat{\beta}}_{k}\)</span>, y su raíz cuadrada la desviación estándar del mismo.</p>
<p>Si se estima la desviación típica de cada <span class="math inline">\({\widehat{\beta}}_{k}\)</span> por <span class="math inline">\(se({\widehat{\beta}}_{k}) = \widehat{\sigma}\sqrt{a_{kk}}\)</span>, se puede demostrar entonces que el estadístico <span class="math inline">\(\frac{({\widehat{\beta}}_{k} - \beta_{k})}{se({\widehat{\beta}}_{k})}\)</span> sigue una distribución <em>t</em> de Student con <em>n</em>-<em>K</em> grados de libertad, es decir,</p>
<p><span class="math display">\[\frac{{\widehat{\beta}}_{k} - \beta_{k}}{se({\widehat{\beta}}_{k})}\sim t_{n - K}\]</span></p>
<p>Este resultado permite la obtención de expresiones sencillas de los intervalos de confianza para cada uno de los parámetros estructurales, <em>β<sub>k</sub></em>, y la realización de contrastes estadísticos acerca de sus posibles valores.</p>
</section>
<section id="intervalos-de-confianza-para-los-parámetros-estructurales" class="level3" data-number="2.4.2">
<h3 data-number="2.4.2" class="anchored" data-anchor-id="intervalos-de-confianza-para-los-parámetros-estructurales"><span class="header-section-number">2.4.2</span> Intervalos de confianza para los parámetros estructurales</h3>
<p>Puesto que <span class="math inline">\(\frac{{\widehat{\beta}}_{k} - \beta_{k}}{se\left( {\widehat{\beta}}_{k} \right)}\sim t_{n - K}\)</span> se sigue que <span class="math inline">\(P\left( - t_{n - K,\frac{\alpha}{2}} &lt; \frac{{\widehat{\beta}}_{k} - \beta_{k}}{se({\widehat{\beta}}_{k})} &lt; t_{n - K,\frac{\alpha}{2}} \right) = 1 - \alpha\)</span> y, realizando algunas operaciones aritméticas elementales, se llega a la siguiente expresión:</p>
<p><span class="math display">\[P\left( {\widehat{\beta}}_{k} - t_{n - K,\frac{\alpha}{2}}se({\widehat{\beta}}_{k}) &lt; \beta_{k} &lt; {\widehat{\beta}}_{k} + t_{n - K,\frac{\alpha}{2}}se({\widehat{\beta}}_{k}) \right) = 1 - \alpha\]</span></p>
<p>Por tanto, el intervalo de confianza al 100(1-<em>α</em>)% para el parámetro <em>β<sub>k</sub></em> viene dado por:</p>
<p><span class="math display">\[I_{100(1 - \alpha)\%}(\beta_{k}) = \left( {\widehat{\beta}}_{k} - t_{n - K,\frac{\alpha}{2}}se({\widehat{\beta}}_{k}),{\widehat{\beta}}_{k} + t_{n - K,\frac{\alpha}{2}}se({\widehat{\beta}}_{k}) \right)\]</span></p>
<p>Debemos tener presente que, dada una muestra concreta, la probabilidad de que el intervalo obtenido contenga al verdadero valor de <em>β<sub>k</sub></em> es 1, si contiene al verdadero valor, ó 0 si no lo contiene. Sin embargo, nuestra confianza de que el intervalo contenga al verdadero valor de <em>β<sub>k</sub></em> es del <span class="math inline">\(100(1 - \alpha)\)</span> por ciento porque, en un contexto de muestras repetidas, este sería el porcentaje de intervalos de este tipo que contendrían al verdadero valor <em>β<sub>k</sub></em>.</p>
</section>
<section id="contrastes-de-hipótesis-sobre-parámetros-estructurales" class="level3" data-number="2.4.3">
<h3 data-number="2.4.3" class="anchored" data-anchor-id="contrastes-de-hipótesis-sobre-parámetros-estructurales"><span class="header-section-number">2.4.3</span> Contrastes de hipótesis sobre parámetros estructurales</h3>
<p>Para <em>contrastes de hipótesis bilaterales</em> del tipo</p>
<p><span class="math display">\[{H_{0}:\left\{ \beta_{k} = c \right\}\text{\ \ frente\ a\ \ }H}_{1}:\left\{ \beta_{k} \neq c \right\}\]</span></p>
<p>se utiliza como estadístico de prueba el valor</p>
<p><span class="math display">\[t_{0} = \frac{{\widehat{\beta}}_{k} - c}{se({\widehat{\beta}}_{k})}\]</span></p>
<p>que, bajo <span class="math inline">\(H_{0}\)</span>, sigue una distribución <em>t</em> de Student con <em>n-K</em> grados de libertad, <span class="math inline">\(t_{0}\sim t_{n - K}\)</span>. Por tanto, la región crítica (de rechazo de <span class="math inline">\(H_{0}\)</span>) al nivel de significación <em>α</em> viene dada por <span class="math inline">\(\left| t_{0} \right| &gt; t_{n - K,\alpha/2}\)</span>, tal y como se muestra en el gráfico siguiente:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/MLR-6.png" class="img-fluid figure-img" width="400"></p>
<figcaption class="figure-caption">Figura 4</figcaption>
</figure>
</div>
<p>Resultan de especial interés los contrastes de las hipótesis individuales:</p>
<p><span class="math display">\[{H_{0}:\left\{ \beta_{k} = 0 \right\}\text{\ \ frente\ a\ \ }H}_{1}:\left\{ \beta_{k} \neq 0 \right\}\]</span></p>
<p>que equivalen a contrastar la significación estadística de cada una de las variables explicativas del modelo (<em>contrastes de significación individual de los parámetros del modelo</em>). En este caso, los estadísticos <span class="math inline">\(t_{0}\)</span> toman la expresión <span class="math inline">\(t_{\beta_{k}} = \frac{{\widehat{\beta}}_{k}}{se({\widehat{\beta}}_{k})}\)</span>, valores que se conocen como estadísticos <em>t</em> o <em>t</em>-<em>ratios</em> de cada parámetro. Se dirá que un parámetro <span class="math inline">\(\beta_{k}\)</span> es estadísticamente significativo cuando se cumpla que <span class="math inline">\(\left| t_{\beta_{k}} \right| &gt; t_{n - K,\alpha/2}\)</span> (también puede utilizarse el método del <em>P</em>-valor, que se explicará más adelante, como criterio de significación estadística).</p>
<p>También son habituales los <em>contrastes unilaterales</em> del tipo</p>
<p><span class="math display">\[{H_{0}:\left\{ \beta_{k} \leq c \right\}\text{\ \ frente\ a\ \ }H}_{1}:\left\{ \beta_{k} &gt; c \right\}\]</span></p>
<p>o el complementario</p>
<p><span class="math display">\[{H_{0}:\left\{ \beta_{k} \geq c \right\}\text{\ \ frente\ a\ \ }H}_{1}:\left\{ \beta_{k} &lt; c \right\}\]</span></p>
<p>Para realizar estos contrastes propuestos también se utiliza la prueba <em>t</em>: basándonos en que <span class="math inline">\(\frac{{\widehat{\beta}}_{k} - \beta_{k}}{se\left( {\widehat{\beta}}_{k} \right)}\sim t_{n - K}\)</span>, si se verifica la hipótesis nula, entonces ha de cumplirse que <span class="math inline">\(t_{0} = \frac{{\widehat{\beta}}_{k} - c}{se({\widehat{\beta}}_{k})}\)</span> sigue una distribución <em>t</em> de Student con <em>n-K</em> grados de libertad. A partir de este resultado, para un nivel de significación <em>α</em>, la región de rechazo para el estadístico de contraste <em>t</em><sub>0</sub> vendrá dada por <span class="math inline">\(t_{0} \geq t_{n - K,\alpha}\)</span>para el contraste unilateral por la derecha , y por <span class="math inline">\(t_{0} \leq - t_{n - K,\alpha}\)</span> para el contraste unilateral por la izquierda. Ambas regiones se muestran en las gráficas siguientes:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/MLR-7.png" class="img-fluid figure-img" width="400"></p>
<figcaption class="figure-caption">Figura 5</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/MLR-8.png" class="img-fluid figure-img" width="400"></p>
<figcaption class="figure-caption">Figura 6</figcaption>
</figure>
</div>
</section>
<section id="intervalo-de-confianza-y-contrastes-de-hipótesis-para-la-varianza-residual" class="level3" data-number="2.4.4">
<h3 data-number="2.4.4" class="anchored" data-anchor-id="intervalo-de-confianza-y-contrastes-de-hipótesis-para-la-varianza-residual"><span class="header-section-number">2.4.4</span> Intervalo de confianza y contrastes de hipótesis para la varianza residual</h3>
<p>Dado que se verifica que</p>
<p><span class="math display">\[\frac{(n - K){\widehat{\sigma}}^{2}}{\sigma^{2}}\sim\chi_{n - K}^{2}\]</span></p>
<p>y teniendo en cuenta que la distribución Chi-cuadrado no es simétrica, obtenemos que <span class="math inline">\(P\left( \chi_{n - K,1 - \frac{\alpha}{2}}^{2} &lt; \frac{(n - K)\widehat{\sigma}^{2}}{\sigma^{2}} &lt; \chi_{n - K,\frac{\alpha}{2}}^{2} \right) = 1 - \alpha\)</span>.</p>
<p>Entonces, de nuevo mediante operaciones aritméticas simples para despejar la varianza residual, se llega a que</p>
<p><span class="math display">\[P\left( \frac{(n - K)\widehat{\sigma}^{2}}{\chi_{n - K,\frac{\alpha}{2}}^{2}} &lt; \sigma^{2} &lt; \frac{(n - K)\widehat{\sigma}^{2}}{\chi_{n - K,1 - \frac{\alpha}{2}}^{2}} \right) = 1 - \alpha\]</span></p>
<p>Por tanto, el intervalo al 100(1-<em>α</em>)% de confianza para el parámetro <em>σ</em><sup>2</sup> viene dado por</p>
<p><span class="math display">\[I_{100(1 - \alpha)\%}(\sigma^{2}) = \left( \frac{(n - K)\widehat{\sigma}^{2}}{\chi_{n - K,\frac{\alpha}{2}}^{2}},\frac{(n - K)\widehat{\sigma}^{2}}{\chi_{n - K,1 - \frac{\alpha}{2}}^{2}} \right)\]</span></p>
<p>Para realizar un contraste donde la hipótesis nula es <span class="math inline">\(H_{0}:\left\{ \sigma^{2} = c \right\}\)</span> y la alternativa viene dada por <span class="math inline">\(H_{1}:\left\{ \sigma^{2} \neq c \right\}\)</span>, con un nivel de significación <em>α</em>, el estadístico utilizado es <span class="math inline">\(\chi_{0}^{2} = \frac{(n - K)\widehat{\sigma}^{2}}{c}\)</span>, y la región crítica viene dada en este caso por las zonas (de rechazo) <span class="math inline">\(\chi_{0}^{2} &lt; \chi_{n - K,1 - \frac{\alpha}{2}}^{2}\)</span> y <span class="math inline">\(\chi_{0}^{2} &gt; \chi_{n - K,\frac{\alpha}{2}}^{2}\)</span>.</p>
</section>
</section>
<section id="inferencia-estadística-ii-contrastes-conjuntos-de-restricciones-y-el-estimador-de-mínimos-cuadrados-restringidos" class="level2" data-number="2.5">
<h2 data-number="2.5" class="anchored" data-anchor-id="inferencia-estadística-ii-contrastes-conjuntos-de-restricciones-y-el-estimador-de-mínimos-cuadrados-restringidos"><span class="header-section-number">2.5</span> Inferencia estadística (II): Contrastes conjuntos de restricciones y el estimador de mínimos cuadrados restringidos</h2>
<section id="distribución-de-los-estimadores-de-combinaciones-lineales-de-parámetros-estructurales" class="level3" data-number="2.5.1">
<h3 data-number="2.5.1" class="anchored" data-anchor-id="distribución-de-los-estimadores-de-combinaciones-lineales-de-parámetros-estructurales"><span class="header-section-number">2.5.1</span> Distribución de los estimadores de combinaciones lineales de parámetros estructurales</h3>
<p>Para llevar a cabo contrastes conjuntos en el MRL, debemos utilizar un resultado estadístico general sobre distribuciones de variables aleatorias multivariantes. Concretamente, dada una matriz <span class="math inline">\(\mathbf{R}\)</span> de orden <span class="math inline">\(q \times K\)</span>, del hecho de que <span class="math inline">\(\widehat{\mathbf{\beta}}\sim N(\mathbf{\beta},\sigma^{2}\left( \mathbf{X}^{\mathbf{'}}\mathbf{X} \right)^{- 1})\)</span> se tiene que <span class="math inline">\(\mathbf{R}\widehat{\mathbf{\beta}}\sim N(\mathbf{R\beta},\sigma^{2}\mathbf{R}\left( \mathbf{X}^{\mathbf{'}}\mathbf{X} \right)^{- 1}\mathbf{R}')\)</span> y, entonces,</p>
<p><span class="math display">\[\frac{\frac{\left( \mathbf{R}\widehat{\mathbf{\beta}} - \mathbf{R\beta} \right)^{'}\left\lbrack {\mathbf{R}\left( \mathbf{X}\mathbf{'}\mathbf{X} \right)^{\mathbf{-}\mathbf{1}}\mathbf{R}}^{\mathbf{'}} \right\rbrack^{- 1}\left( \mathbf{R}\widehat{\mathbf{\beta}} - \mathbf{R\beta} \right)}{q}}{{\widehat{\sigma}}^{2}}\sim F_{q,n - K}\]</span></p>
<p>resultado que permite la realización de cualquier contraste de restricciones lineales múltiples.</p>
</section>
<section id="contrastes-conjuntos-de-restricciones-lineales-sobre-el-vector-mathbfbeta" class="level3" data-number="2.5.2">
<h3 data-number="2.5.2" class="anchored" data-anchor-id="contrastes-conjuntos-de-restricciones-lineales-sobre-el-vector-mathbfbeta"><span class="header-section-number">2.5.2</span> Contrastes conjuntos de restricciones lineales sobre el vector <span class="math inline">\(\mathbf{\beta}\)</span></h3>
<p>Sea <span class="math inline">\(\mathbf{R}\)</span> una matriz de restricciones lineales sobre <span class="math inline">\(\mathbf{\beta}\)</span>, siendo <span class="math inline">\(rango\left( \mathbf{R} \right) = q\)</span>, donde <em>q</em> representa el número de restricciones. Por ejemplo, el conjunto de <span class="math inline">\(q = 4\)</span> restricciones</p>
<p><span class="math display">\[\beta_{2} = 0,\ \beta_{3} = \beta_{4},\ \beta_{1} + 2\beta_{5} + 3\beta_{6} = 2,\ \beta_{1} + \beta_{4} + \beta_{5} + \beta_{6} = 0\]</span></p>
<p>puede expresarse matricialmente como:</p>
<p><span class="math display">\[\begin{bmatrix}
0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 1 &amp; - 1 &amp; 0 &amp; 0 \\
1 &amp; 0 &amp; 0 &amp; 0 &amp; 2 &amp; 3 \\
1 &amp; 0 &amp; 0 &amp; 1 &amp; 1 &amp; 1 \\
\end{bmatrix}\begin{bmatrix}
\beta_{1} \\
\beta_{2} \\
\beta_{3} \\
\beta_{4} \\
\beta_{5} \\
\beta_{6} \\
\end{bmatrix} = \begin{bmatrix}
0 \\
0 \\
2 \\
0 \\
\end{bmatrix}\]</span></p>
<p>En general, cualquier conjunto de restricciones lineales puede escribirse como</p>
<p><span class="math display">\[\left\{ \ \begin{matrix}
r_{11}\beta_{1} + r_{12}\beta_{2} + \cdots + r_{1K}\beta_{K} = r_{1} \\
\vdots \\
r_{q1}\beta_{1} + r_{q2}\beta_{2} + \cdots + r_{qK}\beta_{K} = r_{q} \\
\end{matrix} \right.\ \]</span></p>
<p>sistema de ecuaciones que en forma matricial se expresa como <span class="math inline">\(\mathbf{R\beta} = \mathbf{r}\)</span>.</p>
<p>Teniendo en cuenta el resultado estadístico general, para el contraste de restricciones lineales donde la hipótesis nula <span class="math inline">\(H_{0}\mathbf{:}\left\{ \mathbf{R\beta} = \mathbf{r} \right\}\)</span> se confronta a la alternativa <span class="math inline">\(H_{1}\mathbf{:}\left\{ \mathbf{R\beta} \neq \mathbf{r} \right\}\)</span>, se puede utilizar el estadístico <em>F</em> de Fisher-Snedecor, el cual bajo <span class="math inline">\(H_{0}\)</span> se distribuye como</p>
<p><span class="math display">\[F_{0} = \frac{\frac{\left( \mathbf{R}\widehat{\mathbf{\beta}} - \mathbf{r} \right)^{'}\left\lbrack {\mathbf{R}\left( \mathbf{X}\mathbf{'}\mathbf{X} \right)^{\mathbf{-}\mathbf{1}}\mathbf{R}}^{\mathbf{'}} \right\rbrack^{- 1}\left( \mathbf{R}\widehat{\mathbf{\beta}} - \mathbf{r} \right)}{q}}{{\widehat{\sigma}}^{2}}\sim F_{q,n - K}\]</span></p>
<p>En consecuencia, se rechazará la hipótesis nula <span class="math inline">\(H_{0}\)</span> cuando el valor del estadístico muestral <span class="math inline">\(F_{0}\)</span> supere el valor crítico teórico, <span class="math inline">\(F_{0} &gt; F_{q,n - K,\alpha}\)</span>, mientras que si se cumple que <span class="math inline">\(F_{0} \leq F_{q,n - K,\alpha}\)</span> entonces no se rechazará <span class="math inline">\(H_{0}:\left\{ \mathbf{R\beta} = \mathbf{r} \right\}\)</span>.</p>
<p>También puede utilizarse para el contraste de restricciones lineales el estadístico de Wald</p>
<p><span class="math display">\[W_{0} = \frac{\left( \mathbf{R}\widehat{\mathbf{\beta}} - \mathbf{r} \right)^{'}\left\lbrack {\mathbf{R}\left( \mathbf{X}\mathbf{'}\mathbf{X} \right)^{\mathbf{-}\mathbf{1}}\mathbf{R}}^{\mathbf{'}} \right\rbrack^{- 1}\left( \mathbf{R}\widehat{\mathbf{\beta}} - \mathbf{r} \right)}{{\widehat{\sigma}}^{2}}\overset{as}{\sim}F_{q,n - K}\]</span></p>
<p>cumpliéndose que <span class="math inline">\(W_{0} = qF_{0}\)</span>.</p>
<p>Lógicamente, este contraste no ofrece ninguna ventaja sobre el estadístico <em>F</em><sub>0</sub> para el caso lineal (de hecho, al tratarse de un contraste asintótico, sólo es fiable si el tamaño de la muestra es suficientemente grande); en cambio, puede utilizarse para situaciones mucho más generales que el primero, en particular, para contrastar restricciones no lineales para las que el estadístico <em>F</em><sub>0</sub> resulta inaplicable.</p>
<p>Así, si el contraste que se quiere realizar tiene la forma general <span class="math inline">\(H_{0}:\left\{ \mathbf{g}\left( \mathbf{\beta} \right) = \mathbf{r} \right\}\)</span> frente a <span class="math inline">\(H_{1}:\left\{ \mathbf{g}\left( \mathbf{\beta} \right) \neq \mathbf{r} \right\}\)</span>para algún conjunto de funciones no lineales <span class="math inline">\(\mathbf{g} = (g_{1},g_{2},\ldots,g_{q})\)</span>, debe utilizarse necesariamente la prueba de Wald, que ahora toma la expresión</p>
<p><span class="math display">\[W_{0} = \frac{\left( \mathbf{g}\left( \widehat{\mathbf{\beta}} \right) - \mathbf{r} \right)^{'}\left\lbrack {\mathbf{G}\left( \widehat{\mathbf{\beta}} \right)\left( \mathbf{X}\mathbf{'}\mathbf{X} \right)^{\mathbf{-}\mathbf{1}}\mathbf{G}\left( \widehat{\mathbf{\beta}} \right)}^{\mathbf{'}} \right\rbrack^{- 1}\left( \mathbf{g}\left( \widehat{\mathbf{\beta}} \right) - \mathbf{r} \right)}{{\widehat{\sigma}}^{2}}\]</span></p>
<p>donde <span class="math inline">\(\mathbf{G} = \left( \frac{\partial g_{1}}{\partial\mathbf{\beta}'},\frac{\partial g_{2}}{\partial\mathbf{\beta}'},\ldots,\frac{\partial g_{q}}{\partial\mathbf{\beta}'} \right)\)</span>. Si la hipótesis nula es cierta, la distribución asintótica del estadístico <em>W</em><sub>0</sub> es una variable <span class="math inline">\(\chi^{2}\)</span>, <span class="math inline">\(W_{0}\overset{as}{\sim}\chi_{q}^{2}\)</span> .</p>
</section>
<section id="contraste-de-validez-general-del-modelo-de-regresión" class="level3" data-number="2.5.3">
<h3 data-number="2.5.3" class="anchored" data-anchor-id="contraste-de-validez-general-del-modelo-de-regresión"><span class="header-section-number">2.5.3</span> Contraste de validez general del modelo de regresión</h3>
<p>El contraste de significación conjunta (global) de los parámetros del modelo de regresión lineal viene dado por</p>
<p><span class="math display">\[H_{0}:\left\{ \beta_{2} = 0,\beta_{3} = 0,\ldots,\beta_{K} = 0 \right\}\ \ frente\ a\ \ H_{1}:\left\{ \exists j &gt; 1/\beta_{j} \neq 0 \right\}\]</span></p>
<p>lo que equivale a contrastar</p>
<p><span class="math display">\[H_{0}:\left\{ y_{i} = \beta_{1} + e_{i} \right\}\ \ frente\ a\ \ H_{1}:\left\{ y_{i} = \beta_{1} + \beta_{2}x_{2i} + \ldots + \beta_{K}x_{Ki} + e_{i} \right\}\]</span></p>
<p>es decir, la hipótesis nula de que ninguna de las variables explicativas ejerce una influencia estadísticamente significativa sobre la variable dependiente, frente a la alternativa de que al menos una de ellas resulta estadísticamente significativa.</p>
<p>En este caso particular, puede calcularse el valor del estadístico de prueba <em>F</em><sub>0</sub> a partir de la expresión</p>
<p><span class="math display">\[F_{0} = \frac{\frac{SEC}{(K - 1)}}{\frac{SRC}{(n - K)}}\]</span></p>
<p>lo cual permite realizar el siguiente cuadro de <em>análisis de la varianza</em> para el modelo de regresión lineal:</p>
<table class="table">
<colgroup>
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
</colgroup>
<tbody>
<tr class="odd">
<td style="text-align: left;"><em>Fuente de variación</em></td>
<td style="text-align: left;">Suma de cuadrados</td>
<td style="text-align: left;">Grados de libertad</td>
<td style="text-align: left;">‘Media’ de cuadrados</td>
<td style="text-align: left;"><em>F</em><sub>0</sub></td>
</tr>
<tr class="even">
<td style="text-align: left;">Regresión</td>
<td style="text-align: left;"><span class="math display">\[SEC = {\widehat{\mathbf{\beta}}}^{'}\mathbf{X}^{'}\mathbf{y} - n{\overline{y}}^{2}\]</span></td>
<td style="text-align: left;"><span class="math display">\[K - 1\]</span></td>
<td style="text-align: left;"><span class="math display">\[\frac{SEC}{(K - 1)}\]</span></td>
<td style="text-align: left;"><span class="math display">\[\frac{\frac{SEC}{(K - 1)}}{\frac{SRC}{(n - K)}}\]</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;">Residual</td>
<td style="text-align: left;"><span class="math display">\[SRC = \mathbf{y}^{'}\mathbf{y} - \widehat{\mathbf{\beta}}\mathbf{X}^{'}\mathbf{y}\]</span></td>
<td style="text-align: left;"><span class="math display">\[n - K\]</span></td>
<td style="text-align: left;"><span class="math display">\[\frac{SRC}{(n - K)}\]</span></td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;">Total</td>
<td style="text-align: left;"><span class="math display">\[STC = \mathbf{y}^{'}\mathbf{y} - n{\overline{y}}^{2}\]</span></td>
<td style="text-align: left;"><span class="math display">\[n - 1\]</span></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<p>También puede demostrarse que, para este contraste de validez general del modelo de regresión, se tiene que</p>
<p><span class="math display">\[F_{0} = \frac{\frac{R^{2}}{(K - 1)}}{\frac{(1 - R^{2})}{(n - K)}}\]</span></p>
<p>lo que permite interpretar el estadístico <em>F</em><sub>0</sub> como la “versión estadística del coeficiente <em>R</em><sup>2</sup>”: si el <em>R</em><sup>2</sup> de una regresión es suficientemente alto, el estadístico <em>F</em><sub>0</sub> rechazará fácilmente la hipótesis nula <span class="math inline">\(H_{0}:\left\{ \beta_{2} = 0,\beta_{3} = 0,\ldots,\beta_{K} = 0 \right\}\)</span>, mientras que si el valor <em>R<sup>2</sup></em> es muy bajo no se rechazará dicha hipótesis, lo que implica que ninguna de las variables del modelo de regresión planteado resulta estadísticamente significativa.</p>
</section>
<section id="el-estimador-de-mínimos-cuadrados-restringidos" class="level3" data-number="2.5.4">
<h3 data-number="2.5.4" class="anchored" data-anchor-id="el-estimador-de-mínimos-cuadrados-restringidos"><span class="header-section-number">2.5.4</span> El estimador de mínimos cuadrados restringidos</h3>
<p>En ocasiones, las restricciones lineales sobre los parámetros del modelo vienen ‘impuestas’ por la teoría económica subyacente, es decir, se trata de restricciones poblacionales en forma de información <em>a priori</em>. Otras veces, sin embargo, dichas restricciones son de tipo muestral, esto es, se alude a restricciones <em>a posteriori</em>. En cualquier caso, puede plantearse la idea de incorporar tal información extra al modelo de regresión al objeto de mejorar los resultados. Estaríamos en este caso ante el método conocido como mínimos cuadrados restringidos (MCR).</p>
<p>Las estimaciones de mínimos cuadrados restringidos se obtienen minimizando la función objetivo estándar de MCO, <span class="math inline">\(S(\mathbf{\beta}) = (\mathbf{y} - \mathbf{X\beta})'(\mathbf{y} - \mathbf{X\beta})\)</span>, sujeta a un cierto conjunto de restricciones, <span class="math inline">\(\mathbf{R\beta} = \mathbf{r}\)</span>. Matemáticamente, esto equivale a minimizar la función</p>
<p><span class="math display">\[L\left( \mathbf{\beta} \right) = \left( \mathbf{y} - \mathbf{X\beta} \right)^{'}\left( \mathbf{y} - \mathbf{X\beta} \right) - \mathbf{\lambda}'\left( \mathbf{R\beta} - \mathbf{r} \right)\]</span></p>
<p>donde <span class="math inline">\(\mathbf{\lambda} = \left( \lambda_{1},\ldots,\lambda_{q} \right)^{'}\)</span> es el llamado vector de <em>multiplicadores de Lagrange</em>. La solución matemática a este problema da lugar a las estimaciones MCR, que vienen dadas por la expresión:</p>
<p><span class="math display">\[\mathbf{b}_{MCR} = \mathbf{b} + {(\mathbf{X}^{\mathbf{'}}\mathbf{X})}^{- 1}\mathbf{R}'\left\lbrack \mathbf{R}\left( \mathbf{X}^{'}\mathbf{X} \right)^{- 1}\mathbf{R}^{'} \right\rbrack^{- 1}(\mathbf{r} - \mathbf{Rb})\]</span></p>
<p>Puede demostrarse que las estimaciones obtenidas son equivalentes a las que se llegaría re-parametrizando el modelo original (no restringido) mediante la incorporación al mismo de las restricciones<span class="math inline">\(\mathbf{\ }\mathbf{R\beta} = \mathbf{r}\)</span>, y estimando este último (el modelo restringido) por MCO. De ahí el nombre del estimador.</p>
<p>En cuanto a las propiedades estadísticas y algebraicas del estimador MCR, <span class="math inline">\({\widehat{\mathbf{\beta}}}_{MCR}\)</span>, se cumple lo siguiente:</p>
<ul>
<li><p><span class="math inline">\(E\left( {\widehat{\mathbf{\beta}}}_{MCR} \right) = \mathbf{\beta}\)</span> &gt; si y sólo si <span class="math inline">\(\mathbf{R\beta}\mathbf{=}\mathbf{r}\)</span>, es decir, &gt; el estimador MCR sólo es insesgado si las restricciones son &gt; ciertas.</p></li>
<li><p><span class="math inline">\(Cov\left( {\widehat{\mathbf{\beta}}}_{MCR} \right) \leq Cov\left( {\widehat{\mathbf{\beta}}}_{MCO} \right)\)</span> &gt; aún cuando las restricciones sean falsas, es decir, el estimador &gt; MCR es siempre más eficiente que el estimador MCO.</p></li>
</ul>
<p>Por tanto, si las restricciones <span class="math inline">\(\mathbf{R\beta} = \mathbf{r}\)</span> son válidas, <span class="math inline">\({\widehat{\mathbf{\beta}}}_{MCR}\)</span> es mejor que <span class="math inline">\({\widehat{\mathbf{\beta}}}_{MCO}\)</span> por ser insesgado y de menor varianza. En otro caso, el estimador <span class="math inline">\({\widehat{\mathbf{\beta}}}_{MCR}\)</span> es sesgado aún cuando pueda tener menor varianza que el estimador <span class="math inline">\({\widehat{\mathbf{\beta}}}_{MCO}\)</span> y, por tanto, tenga un error cuadrático medio menor.</p>
<p>Para contrastar la validez de las restricciones, <span class="math inline">\(H_{0}:\left\{ \mathbf{R\beta} = \mathbf{r} \right\}\)</span>, puede utilizarse el estadístico <span class="math inline">\(F_{0}\)</span>, pudiendo demostrarse en este caso que dicho estadístico viene dado por la expresión</p>
<p><span class="math display">\[F_{0} = \frac{(SRC_{R} - SRC_{NR})/q}{SRC_{NR}/(n - K)}\]</span></p>
<p>donde <em>SRC<sub>NR</sub></em> representa la suma de residuos al cuadrado del modelo no restringido y <em>SRC<sub>R</sub></em> la suma de cuadrados residual correspondiente al modelo restringido, siendo <em>q</em> el número de restricciones incluidas en<span class="math inline">\(\mathbf{\ }\mathbf{R\beta} = \mathbf{r}\)</span>.</p>
</section>
</section>
<section id="predicciones" class="level2" data-number="2.6">
<h2 data-number="2.6" class="anchored" data-anchor-id="predicciones"><span class="header-section-number">2.6</span> Predicciones</h2>
<p>Supongamos que se parte del modelo de regresión lineal <span class="math inline">\(\mathbf{y} = \mathbf{X\beta} + \mathbf{e}\)</span>, el cual produce valores ajustados del tipo <span class="math inline">\(\widehat{\mathbf{y}} = \mathbf{Xb}\)</span> al ser estimado mediante MCO. Dado un nuevo conjunto de valores de las variables exógenas, <span class="math inline">\(\mathbf{X}_{0} = \left( 1,x_{20},\ldots,x_{K0} \right)\)</span>, el objetivo de la predicción consiste en estimar el valor desconocido <span class="math inline">\(y_{0}\)</span> dados los distintos valores (conocidos) de las variables explicativas contenidos en el vector <span class="math inline">\(\mathbf{X}_{0}\)</span>. De forma lógica, la estimación propuesta para dicho valor viene dada por la expresión <span class="math inline">\({\widehat{y}}_{0} = b_{1} + b_{2}x_{20} + \ldots + b_{K}x_{K0} = \mathbf{X}_{0}\mathbf{b}\)</span>.</p>
<p>El estimador propuesto, denominado <em>predictor</em>, tiene la expresión</p>
<p><span class="math display">\[{\widehat{y}}_{0} = \mathbf{X}_{0}\widehat{\mathbf{\beta}}\]</span></p>
<p>y debemos tener presente que, según el modelo de regresión, <span class="math inline">\(y_{0} = \mathbf{X}_{0}\mathbf{\beta}\mathbf{+}e_{0}\)</span>, por lo que si la predicción viene dada por <span class="math inline">\({\widehat{y}}_{0} = \mathbf{X}_{0}\widehat{\mathbf{\beta}}\)</span>, el error de predicción que se cometerá será</p>
<p><span class="math display">\[{e_{0}^{P} = y_{0} - \widehat{y}}_{0}\]</span></p>
<p>Teniendo en cuenta que si se verifican las hipótesis del MRL se cumple que <span class="math inline">\(\widehat{\mathbf{\beta}}\sim N(\mathbf{\beta},\sigma^{2}\left( \mathbf{X}^{\mathbf{'}}\mathbf{X} \right)^{- 1})\)</span>, entonces la distribución de <span class="math inline">\({\widehat{y}}_{0}\)</span> viene dada por</p>
<p><span class="math display">\[{\widehat{y}}_{0} = \mathbf{X}_{0}\widehat{\mathbf{\beta}}\mathbf{\sim}N(\mathbf{X}_{0}\mathbf{\beta}\mathbf{,}\mathbf{X}_{0}Cov\left( \widehat{\mathbf{\beta}} \right)\mathbf{X}_{0}^{\mathbf{'}})\]</span></p>
<p>por lo que es obvio que se cumple que <span class="math inline">\({E\left( e_{0}^{P} \right)}_{\ }\)</span>=0 y, por tanto, <span class="math inline">\({\widehat{y}}_{0}\)</span> es un estimador insesgado del valor <span class="math inline">\(y_{0}\)</span>.</p>
<p>Por otra parte, teniendo en cuenta las propiedades encontradas para el estimador MCO, se cumple que <span class="math inline">\({\widehat{y}}_{0} = \mathbf{X}_{0}\widehat{\mathbf{\beta}}\)</span> es el mejor predictor lineal insesgado para el valor <em>y</em><sub>0</sub>.</p>
<p>También se puede demostrar que la varianza del error de predicción puntual toma la expresión</p>
<p><span class="math display">\[{Var\left( e_{0}^{P} \right) = \sigma^{2}\left( 1 + \mathbf{X}_{0}\left( \mathbf{X}^{'}\mathbf{X} \right)^{\mathbf{-}\mathbf{1}}\mathbf{X}_{0}^{\mathbf{'}} \right)\ }_{\ }\]</span></p>
<p>Substituyendo el valor de <span class="math inline">\(\sigma^{2}\)</span> por su estimación MCO, <span class="math inline">\({\widehat{\sigma}}^{2}\)</span>, y tras la normalización correspondiente, se puede demostrar que</p>
<p><span class="math display">\[\frac{e_{0}^{P}}{\sqrt{\widehat{Var\left( e_{0}^{P} \right)}}} = \frac{y_{0}\mathbf{-}\mathbf{X}_{0}\widehat{\mathbf{\beta}}\mathbf{\ }}{\widehat{\sigma}\sqrt{\ 1 + \mathbf{X}_{0}\left( \mathbf{X}^{'}\mathbf{X} \right)^{- 1}\mathbf{X}_{0}^{\mathbf{'}}}\ }\sim t_{n - K}\]</span></p>
<p>Este resultado nos permite realizar contrastes sobre los valores de las predicciones, así como construir un intervalo de confianza para la predicción realizada, el cual está dado por la expresión:</p>
<p><span class="math display">\[I_{100(1 - \alpha)\%}(y_{0}) = \left( {\widehat{y}}_{0} - t_{n - K,\frac{\alpha}{2}}se\left( {\widehat{e}}_{0}^{P} \right)\ ,\ {\widehat{y}}_{0} + t_{n - K,\frac{\alpha}{2}}se({\widehat{e}}_{0}^{P}) \right)\]</span></p>
<p>donde <span class="math inline">\(se\left( {\widehat{e}}_{0}^{P} \right)\)</span> representa la desviación típica estimada del error de predicción, <span class="math inline">\(se\left( {\widehat{e}}_{0}^{P} \right) = \sqrt{\widehat{Var\left( e_{0}^{P} \right)}}\)</span>.</p>
<p>La expresión anterior para el intervalo de confianza nos muestra que, cuanto más alejado esté el valor <span class="math inline">\(\mathbf{X}_{0} = \left( 1,x_{20},\ldots,x_{K0} \right)\)</span> de la media de los datos muestrales, <span class="math inline">\(\overline{\mathbf{X}} = \left( 1,{\overline{x}}_{2},\ldots,{\overline{x}}_{K} \right)\)</span>, mayor será la amplitud del intervalo de predicción y, por tanto, menos fiables serán las predicciones. En la gráfica siguiente se muestra este comportamiento de los intervalos de predicción:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/MLR-24.png" class="img-fluid figure-img" width="500"></p>
<figcaption class="figure-caption">Figura 7</figcaption>
</figure>
</div>
</section>
<section id="forma-funcional" class="level2" data-number="2.7">
<h2 data-number="2.7" class="anchored" data-anchor-id="forma-funcional"><span class="header-section-number">2.7</span> Forma funcional</h2>
<p>El modelo de regresión estándar, <span class="math inline">\(y = \beta_{1} + \beta_{2}x_{2} + \ldots + \beta_{K}x_{K} + e\)</span>, se caracteriza por ser lineal no sólo en los parámetros, sino también en las variables. No obstante, existen otros modelos que, si bien no son lineales en las variables, son lineales en los parámetros. Entre estos se encuentran una amplia gama de especificaciones que son aplicadas con frecuencia en la práctica econométrica.</p>
<p>Haremos referencia a continuación a las formas funcionales más habituales, utilizando como soporte para la exposición el modelo de regresión lineal con una sola variable explicativa <span class="math inline">\(x\)</span>, <span class="math inline">\(y_{i} = \beta_{1} + \beta_{2}x_{i} + e_{i}\)</span>. En la gráfica siguiente se muestran algunas de las funciones a las que nos referiremos en los próximos párrafos:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/MLR-10.png" class="img-fluid figure-img" width="400"></p>
<figcaption class="figure-caption">Figura 8</figcaption>
</figure>
</div>
<section id="regresiones-cuadráticas-o-polinómicas" class="level3" data-number="2.7.1">
<h3 data-number="2.7.1" class="anchored" data-anchor-id="regresiones-cuadráticas-o-polinómicas"><span class="header-section-number">2.7.1</span> Regresiones cuadráticas o polinómicas</h3>
<p>El <em>modelo</em> de regresión <em>cuadrático</em> se expresa matemáticamente como</p>
<p><span class="math display">\[y_{i} = \beta_{1} + \beta_{2}x_{i} + \beta_{3}x_{i}^{2} + e_{i}\]</span></p>
<p>En este modelo, a diferencia del caso lineal básico, la pendiente de la curva de regresión no es constante, sino que depende de los valores de la variable <em>x</em>. Concretamente, se tiene que <span class="math inline">\(\frac{\partial y_{i}}{\partial x_{i}} = \beta_{2} + 2\beta_{3}x_{i}\)</span> y, por tanto, según que el parámetro <span class="math inline">\(\beta_{3}\)</span> sea positivo o negativo, la tasa de cambio de la variable <em>y</em> frente a la variable <em>x</em> aumentará o disminuirá conforme lo haga la variable <em>x</em>.</p>
<p>El modelo cuadrático puede generalizarse añadiendo potencias de orden superior en la variable <span class="math inline">\(x\)</span>, por ejemplo, <span class="math inline">\(x^{3}\)</span> o <span class="math inline">\(x^{4}\)</span>, con lo que se consigue mayor flexibilidad en el modelo de respuestas <span class="math inline">\(\frac{\partial y_{i}}{\partial x_{i}}\)</span> posible:</p>
<p><span class="math display">\[y_{i} = \beta_{1} + \beta_{2}x_{i} + \beta_{3}x_{i}^{2} + \beta_{4}x_{i}^{3} + \ldots + e_{i}\]</span></p>
<p>Ejemplos de regresiones polinómicas son las funciones de costes, totales o medios, para las que pueden llegar a observarse modelos cúbicos o cuadráticos, respectivamente.</p>
</section>
<section id="regresiones-logarítmicas" class="level3" data-number="2.7.2">
<h3 data-number="2.7.2" class="anchored" data-anchor-id="regresiones-logarítmicas"><span class="header-section-number">2.7.2</span> Regresiones logarítmicas</h3>
<p>El <em>modelo</em> dado por la función potencial, <span class="math inline">\(y_{i} = \alpha_{0}x_{i}^{\alpha_{1}}e^{e_{i}}\)</span>, es conocido como <em>logarítmico</em> (<em>log-log</em>). Para poder aplicar los resultados obtenidos para el modelo lineal se transforma la expresión original tomando logaritmos naturales, obteniéndose</p>
<p><span class="math display">\[\log y_{i} = \beta_{1} + \beta_{2}\log x_{i} + e_{i}\]</span></p>
<p>donde <span class="math inline">\(\beta_{1} = \log\alpha_{0}\)</span> y <span class="math inline">\(\beta_{2} = \alpha_{1}\)</span>.</p>
<p>El modelo transformado es lineal en los parámetros y puede estimarse por mínimos cuadrados ordinarios. Para ello, se definen <span class="math inline">\(y_{i}^{*} = \log y_{i}\)</span> y <span class="math inline">\(x_{i}^{*} = \log x_{i}\)</span>, y se reescribe el modelo transformado como <span class="math inline">\(y_{i}^{*} = \beta_{1} + \beta_{2}x_{i}^{*} + e_{i}\)</span>.</p>
<p>Este modelo también se conoce como <em>modelo de elasticidad constante</em>, pues el coeficiente <em>β</em><sub>2</sub> es la elasticidad de la variable <em>y</em> respecto de <em>x</em> ya que <span class="math inline">\(\beta_{2} = \frac{\partial y_{i}^{*}}{\partial x_{i}^{*}} = \frac{\partial logy}{\partial logx} = \frac{\partial y/y}{\partial x/x} = e_{y|x}\)</span>. Entonces, un 1% de cambio en la variable <em>x</em> llevará asociado, en promedio, un cambio en la variable <em>y</em> del <em>β</em><sub>2</sub>%.</p>
<p>Un ejemplo clásico de modelo logarítmico es la especificación Cobb-Douglas para una función de producción, <span class="math inline">\(Y_{i} = AL_{i}^{\alpha}K_{i}^{\beta}e^{e_{i}}\)</span>, que tras tomar logaritmos se convierte en <span class="math inline">\(\log Y_{i} = \beta_{1} + \beta_{2}\log L_{i} + \beta_{3}\log K_{i} + e_{i}\)</span>. En este caso, los parámetros <span class="math inline">\(\beta_{1}\)</span> y <span class="math inline">\(\beta_{2}\)</span> representan, respectivamente, las elasticidades de la producción (<em>Y</em>) respecto al trabajo (<em>L</em>) y el capital (<em>K</em>).</p>
</section>
<section id="regresiones-semi-logarítmicas" class="level3" data-number="2.7.3">
<h3 data-number="2.7.3" class="anchored" data-anchor-id="regresiones-semi-logarítmicas"><span class="header-section-number">2.7.3</span> Regresiones semi-logarítmicas</h3>
<p>Si nos interesa medir el cambio porcentual en <em>y</em>, ante un cambio unitario absoluto en <em>x</em>, el <em>modelo</em> a utilizar deberá ser del tipo <em>logarítmico-lineal (log-lin)</em></p>
<p><span class="math display">\[\log y_{i} = \beta_{1} + \beta_{2}x_{i} + e_{i}\]</span></p>
<p>el cual se obtiene tras tomar logaritmos en una función exponencial del tipo <span class="math inline">\(y_{i} = \alpha_{0}e^{\beta_{\, 2}x_{i}}e^{e_{i}}\)</span> (y denotando <span class="math inline">\(\beta_{1} = \log\alpha_{0}\)</span>). Para este modelo, el cambio relativo en <em>y</em> producido por un cambio absoluto de una unidad en <em>x</em> viene dado por 100<em>β</em><sub>2</sub>%.</p>
<p>Esta forma funcional aparece frecuentemente cuando se analizan funciones de demanda de dinero, por ejemplo, de la forma <span class="math inline">\(\log M_{t} = \beta_{1} + \beta_{2}r_{t} + e_{t}\)</span>, donde <em>M</em> es la cantidad de dinero en circulación y <em>r</em> el tipo de interés. En este caso, 100<em>β</em><sub>2</sub> es la <em>semi-elasticidad</em> de la demanda de dinero respecto al tipo de interés: un cambio de un punto en los tipos de interés llevará asociado, en promedio, un cambio del 100<em>β</em><sub>2</sub>% en la demanda agregada de dinero.</p>
<p>Cuando interesa medir el cambio absoluto en <em>y</em> ante cambios relativos en <em>x</em>, el <em>modelo</em> adecuado será del tipo <em>lineal-logarítmico (lin-log)</em></p>
<p><span class="math display">\[y_{i} = \beta_{1} + \beta_{2}\log x_{i} + e_{i}\]</span></p>
<p>y, en este caso, el cambio absoluto en la variable <em>y</em> producido por un cambio relativo del 1% en la variable <em>x</em> será de <em>β</em><sub>2</sub>/100 unidades.</p>
</section>
<section id="regresión-recíproca" class="level3" data-number="2.7.4">
<h3 data-number="2.7.4" class="anchored" data-anchor-id="regresión-recíproca"><span class="header-section-number">2.7.4</span> Regresión recíproca</h3>
<p>Se trata de un modelo del tipo</p>
<p><span class="math display">\[y_{i} = \beta_{1} + \beta_{2}\frac{1}{x_{i}} + e_{i}\]</span></p>
<p>existiendo, por tanto, una <em>relación inversa entre las variables</em> <em>x</em> e <em>y</em> (aparte del término de error). Estos modelos se caracterizan porque, al crecer <em>x</em> indefinidamente, <em>y</em> tiende hacia un valor asintótico <span class="math inline">\(\beta_{1}\)</span>, es decir, <span class="math inline">\(\lim_{x \rightarrow \infty}y = \beta_{1}\)</span>.</p>
</section>
<section id="regresiones-con-términos-de-interacción" class="level3" data-number="2.7.5">
<h3 data-number="2.7.5" class="anchored" data-anchor-id="regresiones-con-términos-de-interacción"><span class="header-section-number">2.7.5</span> Regresiones con términos de interacción</h3>
<p>El <em>modelo</em> de regresión <em>con interacciones</em> se expresa matemáticamente como</p>
<p><span class="math display">\[y_{i} = \beta_{1} + \beta_{2}x_{i} + \beta_{3}x_{i}z_{i} + e_{i}\ \]</span></p>
<p>En este modelo, igual que en el caso cuadrático, la pendiente de la curva de regresión no es constante, sino que depende ahora de los valores de la variable <em>z</em>. Concretamente, se cumple que <span class="math inline">\(\frac{\partial y_{i}}{\partial x_{i}} = \beta_{2} + \beta_{3}z_{i}\)</span> y, por tanto, según que el parámetro <span class="math inline">\(\beta_{3}\)</span> sea positivo o negativo, la tasa de cambio de la variable <em>y</em> frente a la variable <em>x</em> aumentará o disminuirá conforme lo haga la variable <em>z</em>.</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./p2-mrl-basico.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">CAPÍTULO 2: EL MODELO DE REGRESIÓN LINEAL Y SUS HIPÓTESIS BÁSICAS</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./p2c2-app1.html" class="pagination-link">
        <span class="nav-page-text">Aplicación 2.1 (Estimación MCO y contrastes de hipótesis): Modelo CAPM para las acciones del Banco Santander</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>