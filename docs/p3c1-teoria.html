<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Econometría - 3&nbsp; Diagnosis, correcciones y extensiones del modelo de regresión lineal</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./p3c2-app1.html" rel="next">
<link href="./p3-mrl-extendido.html" rel="prev">
<link href="./EconMetricsRPy-logo.png" rel="icon" type="image/png">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./p3-mrl-extendido.html">CAPÍTULO 3: DIAGNOSIS, CORRECCIONES Y EXTENSIONES DEL MODELO DE REGRESIÓN LINEAL</a></li><li class="breadcrumb-item"><a href="./p3c1-teoria.html"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Diagnosis, correcciones y extensiones del modelo de regresión lineal</span></a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Econometría</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/JulianRamajo/EconometriaRPy" rel="" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">PREFACIO</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./p0-aspectos-tecnicos.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">CAPÍTULO 0: ASPECTOS TÉCNICOS</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p0c1-setup.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Instalación y configuración de R y Python en RStudio</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p0c2-rpy-comp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">R versus Python: Comparación de lenguajes</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p0c3-reticulate.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Programación conjunta en R y Python con reticulate</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./p1-introduccion.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">CAPÍTULO 1: CONCEPTOS BÁSICOS</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p1c1-teoria.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Conceptos básicos de la econometría</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p1c2-app1a.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Aplicación 1.1.a (Gestión y representación gráfica de datos): Gramática básica del <em>tidyverse</em></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p1c2-app1b.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Aplicación 1.1.b (Gestión y representación gráfica de datos): Gestión de datos con R y Python</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p1c2-app2a.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Aplicación 1.2a (Gestión y representación gráfica de datos financieros): Propiedades estadísticas básicas de los activos bursátiles</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p1c2-app2b.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Aplicación 1.2b (Gestión y representación gráfica de datos financieros): Relaciones entre activos bursátiles</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p1c2-app3a.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Aplicación 1.3.a (Gestión y representación gráfica de datos espaciales): Emisiones de CO2 al nivel mundial</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p1c2-app3b.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Aplicación 1.3b (Gestión y representación gráfica de datos espaciales): Desarrollo humano al nivel mundial</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p1c2-app4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Aplicación 1.4 (Regresiones con datos de corte transversal): Demanda familiar de carne</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p1c2-app5.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Aplicación 1.5 (Regresiones con datos de series temporales): Consumo privado en Estados Unidos</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./p2-mrl-basico.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">CAPÍTULO 2: EL MODELO DE REGRESIÓN LINEAL Y SUS HIPÓTESIS BÁSICAS</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p2c1-teoria.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">El modelo de regresión lineal y sus hipótesis básicas</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p2c2-app1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Aplicación 2.1 (Estimación MCO y contrastes de hipótesis): Modelo CAPM para las acciones del Banco Santander</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p2c2-app2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Aplicación 2.2 (Predicción con el modelo de regresión lineal): Curva de Engel para el gasto en alimentos</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p2c2-app3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Aplicación 2.3 (Modelo de regresión lineal con forma funcional polinómica): Curva de costes en el sector textil</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p2c2-app4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Aplicación 2.4 (Modelo de regresión lineal con términos de interacción): Efectos diferenciados de la publicidad sobre las ventas</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p2c2-app5.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Aplicación 2.5 (Modelo de regresión lineal con variables cualitativas): Brecha salarial entre hombres y mujeres en Estados Unidos</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p2c2-app6.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Aplicación 2.6 (Series temporales de alta frecuencia y estacionalidad): Demanda de electricidad en el estado de Victoria, Australia</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./p3-mrl-extendido.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">CAPÍTULO 3: DIAGNOSIS, CORRECCIONES Y EXTENSIONES DEL MODELO DE REGRESIÓN LINEAL</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p3c1-teoria.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Diagnosis, correcciones y extensiones del modelo de regresión lineal</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p3c2-app1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Aplicación 3.1 (Evaluación, validación y especificación del MRL): El modelo APT de valoración de activos</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p3c2-app2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Aplicación 3.2 (Evaluación, validación y especificación del MRL): Ventas en una cadena de supermercados</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p3c2-app3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Aplicación 3.3 (Estabilidad de los parámetros estructurales): Diferenciación salarial por sexo. Exportaciones españolas.</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p3c2-app4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Aplicación 3.4 (No normalidad de los errores y observaciones atípicas): Muestra de datos con observaciones atípicas simuladas</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p3c2-app5.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Aplicación 3.5 (Regresiones heteroscedásticas): Función de demanda con varianza no constante</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p3c2-app6.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Aplicación 3.6 (Regresiones con volatilidad variable en el tiempo): Relación entre los precios de los carburantes y el precio del petroleo</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p3c2-app7.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Aplicación 3.7 (Autocorrelación y regresiones dinámicas): Modelos ARDL y VAR</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p3c2-app8a.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Aplicación 3.8a (Dependencia espacial - Geometría: polígonos): Estadísticas ‘morales’ en Francia en 1830</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p3c2-app8b.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Aplicación 3.8b (Dependencia espacial - Geometría: puntos): Precio de la vivienda en el condado de Lucas [Ohio, Estados Unidos]</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p3c2-app9.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Aplicación 3.9 (Información muestral - Falta de observaciones): Exclusión social en el comportamiento de las aseguradoras de Chicago</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p3c2-app10.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Aplicación 3.10 (Información muestral - Multicolinealidad): Análisis de la rentabilidad empresarial</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p3c2-app11.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Aplicación 3.11 (Regresores endógenos - El estimador de variables instrumentales: Determinantes de la inflación al nivel internacional</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p3c2-app12.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Aplicación 3.12 (Variable dependiente discreta o limitada): Modelos Logit/Probit, Tobit y Heckit</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p3c2-app13.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Aplicación 3.13 (Modelos econométricos para datos de panel): Productividad de la industria química en China</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">BIBLIOGRAFÍA</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#criterios-generales-de-evaluación-y-validación-de-un-modelo-econométrico" id="toc-criterios-generales-de-evaluación-y-validación-de-un-modelo-econométrico" class="nav-link active" data-scroll-target="#criterios-generales-de-evaluación-y-validación-de-un-modelo-econométrico"><span class="header-section-number">3.1</span> Criterios generales de evaluación y validación de un modelo econométrico</a>
  <ul>
  <li><a href="#coherencia-del-modelo-estimado-con-los-datos" id="toc-coherencia-del-modelo-estimado-con-los-datos" class="nav-link" data-scroll-target="#coherencia-del-modelo-estimado-con-los-datos"><span class="header-section-number">3.1.1</span> Coherencia del modelo estimado con los datos</a></li>
  <li><a href="#estadísticos-de-precisión-e-información" id="toc-estadísticos-de-precisión-e-información" class="nav-link" data-scroll-target="#estadísticos-de-precisión-e-información"><span class="header-section-number">3.1.2</span> Estadísticos de precisión e información</a></li>
  </ul></li>
  <li><a href="#especificación-del-modelo" id="toc-especificación-del-modelo" class="nav-link" data-scroll-target="#especificación-del-modelo"><span class="header-section-number">3.2</span> Especificación del modelo</a>
  <ul>
  <li><a href="#determinación-de-los-regresores-y-de-la-forma-funcional" id="toc-determinación-de-los-regresores-y-de-la-forma-funcional" class="nav-link" data-scroll-target="#determinación-de-los-regresores-y-de-la-forma-funcional"><span class="header-section-number">3.2.1</span> Determinación de los regresores y de la forma funcional</a></li>
  <li><a href="#omisión-de-variables-relevantes" id="toc-omisión-de-variables-relevantes" class="nav-link" data-scroll-target="#omisión-de-variables-relevantes"><span class="header-section-number">3.2.2</span> Omisión de variables relevantes</a></li>
  <li><a href="#inclusión-de-variables-irrelevantes" id="toc-inclusión-de-variables-irrelevantes" class="nav-link" data-scroll-target="#inclusión-de-variables-irrelevantes"><span class="header-section-number">3.2.3</span> Inclusión de variables irrelevantes</a></li>
  <li><a href="#el-contraste-reset-de-especificación-correcta-de-la-forma-funcional" id="toc-el-contraste-reset-de-especificación-correcta-de-la-forma-funcional" class="nav-link" data-scroll-target="#el-contraste-reset-de-especificación-correcta-de-la-forma-funcional"><span class="header-section-number">3.2.4</span> El contraste RESET de especificación correcta de la forma funcional</a></li>
  </ul></li>
  <li><a href="#estabilidad-de-los-parámetros-estructurales" id="toc-estabilidad-de-los-parámetros-estructurales" class="nav-link" data-scroll-target="#estabilidad-de-los-parámetros-estructurales"><span class="header-section-number">3.3</span> Estabilidad de los parámetros estructurales</a>
  <ul>
  <li><a href="#los-contrastes-de-chow-y-quandt-de-cambio-estructural" id="toc-los-contrastes-de-chow-y-quandt-de-cambio-estructural" class="nav-link" data-scroll-target="#los-contrastes-de-chow-y-quandt-de-cambio-estructural"><span class="header-section-number">3.3.1</span> Los contrastes de Chow y Quandt de cambio estructural</a></li>
  <li><a href="#consecuencias-de-la-presencia-de-cambio-estructural" id="toc-consecuencias-de-la-presencia-de-cambio-estructural" class="nav-link" data-scroll-target="#consecuencias-de-la-presencia-de-cambio-estructural"><span class="header-section-number">3.3.2</span> Consecuencias de la presencia de cambio estructural</a></li>
  <li><a href="#regresiones-con-parámetros-variables" id="toc-regresiones-con-parámetros-variables" class="nav-link" data-scroll-target="#regresiones-con-parámetros-variables"><span class="header-section-number">3.3.3</span> Regresiones con parámetros variables</a>
  <ul>
  <li><a href="#modelos-con-variables-ficticias" id="toc-modelos-con-variables-ficticias" class="nav-link" data-scroll-target="#modelos-con-variables-ficticias"><span class="header-section-number">3.3.3.1</span> Modelos con variables ficticias</a></li>
  <li><a href="#el-caso-general-donde-los-parámetros-son-función-de-una-variable-conocida" id="toc-el-caso-general-donde-los-parámetros-son-función-de-una-variable-conocida" class="nav-link" data-scroll-target="#el-caso-general-donde-los-parámetros-son-función-de-una-variable-conocida"><span class="header-section-number">3.3.3.2</span> El caso general donde los parámetros son función de una variable conocida</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#distribución-de-los-errores-del-modelo" id="toc-distribución-de-los-errores-del-modelo" class="nav-link" data-scroll-target="#distribución-de-los-errores-del-modelo"><span class="header-section-number">3.4</span> Distribución de los errores del modelo</a>
  <ul>
  <li><a href="#no-normalidad-de-las-perturbaciones" id="toc-no-normalidad-de-las-perturbaciones" class="nav-link" data-scroll-target="#no-normalidad-de-las-perturbaciones"><span class="header-section-number">3.4.1</span> No normalidad de las perturbaciones</a></li>
  <li><a href="#consecuencias-de-la-no-normalidad" id="toc-consecuencias-de-la-no-normalidad" class="nav-link" data-scroll-target="#consecuencias-de-la-no-normalidad"><span class="header-section-number">3.4.2</span> Consecuencias de la no normalidad</a></li>
  <li><a href="#el-test-de-jarque-bera-de-normalidad" id="toc-el-test-de-jarque-bera-de-normalidad" class="nav-link" data-scroll-target="#el-test-de-jarque-bera-de-normalidad"><span class="header-section-number">3.4.3</span> El test de Jarque-Bera de normalidad</a></li>
  <li><a href="#estimación-robusta-del-mrl" id="toc-estimación-robusta-del-mrl" class="nav-link" data-scroll-target="#estimación-robusta-del-mrl"><span class="header-section-number">3.4.4</span> Estimación robusta del MRL</a></li>
  </ul></li>
  <li><a href="#heteroscedasticidad" id="toc-heteroscedasticidad" class="nav-link" data-scroll-target="#heteroscedasticidad"><span class="header-section-number">3.5</span> Heteroscedasticidad</a>
  <ul>
  <li><a href="#naturaleza-de-la-heteroscedasticidad" id="toc-naturaleza-de-la-heteroscedasticidad" class="nav-link" data-scroll-target="#naturaleza-de-la-heteroscedasticidad"><span class="header-section-number">3.5.1</span> Naturaleza de la heteroscedasticidad</a></li>
  <li><a href="#consecuencias-de-la-heteroscedasticidad" id="toc-consecuencias-de-la-heteroscedasticidad" class="nav-link" data-scroll-target="#consecuencias-de-la-heteroscedasticidad"><span class="header-section-number">3.5.2</span> Consecuencias de la heteroscedasticidad</a></li>
  <li><a href="#detección-de-la-heteroscedasticidad" id="toc-detección-de-la-heteroscedasticidad" class="nav-link" data-scroll-target="#detección-de-la-heteroscedasticidad"><span class="header-section-number">3.5.3</span> Detección de la heteroscedasticidad</a>
  <ul>
  <li><a href="#examen-gráfico-de-los-residuos-mco" id="toc-examen-gráfico-de-los-residuos-mco" class="nav-link" data-scroll-target="#examen-gráfico-de-los-residuos-mco"><span class="header-section-number">3.5.3.1</span> Examen gráfico de los residuos MCO</a></li>
  <li><a href="#contraste-general-de-white" id="toc-contraste-general-de-white" class="nav-link" data-scroll-target="#contraste-general-de-white"><span class="header-section-number">3.5.3.2</span> Contraste general de White</a></li>
  <li><a href="#contrastes-de-breusch-pagan-harvey-y-glejser" id="toc-contrastes-de-breusch-pagan-harvey-y-glejser" class="nav-link" data-scroll-target="#contrastes-de-breusch-pagan-harvey-y-glejser"><span class="header-section-number">3.5.3.3</span> Contrastes de Breusch-Pagan, Harvey y Glejser</a></li>
  <li><a href="#otras-formas-de-heteroscedasticidad-modelos-de-volatilidad-variable-en-el-tiempo" id="toc-otras-formas-de-heteroscedasticidad-modelos-de-volatilidad-variable-en-el-tiempo" class="nav-link" data-scroll-target="#otras-formas-de-heteroscedasticidad-modelos-de-volatilidad-variable-en-el-tiempo"><span class="header-section-number">3.5.3.4</span> Otras formas de heteroscedasticidad: modelos de volatilidad variable en el tiempo</a></li>
  </ul></li>
  <li><a href="#regresiones-heteroscedásticas" id="toc-regresiones-heteroscedásticas" class="nav-link" data-scroll-target="#regresiones-heteroscedásticas"><span class="header-section-number">3.5.4</span> Regresiones heteroscedásticas</a>
  <ul>
  <li><a href="#el-estimador-de-mínimos-cuadrados-corregidos-método-de-white" id="toc-el-estimador-de-mínimos-cuadrados-corregidos-método-de-white" class="nav-link" data-scroll-target="#el-estimador-de-mínimos-cuadrados-corregidos-método-de-white"><span class="header-section-number">3.5.4.1</span> El estimador de mínimos cuadrados corregidos (método de White)</a></li>
  <li><a href="#el-estimador-de-mínimos-cuadrados-ponderados" id="toc-el-estimador-de-mínimos-cuadrados-ponderados" class="nav-link" data-scroll-target="#el-estimador-de-mínimos-cuadrados-ponderados"><span class="header-section-number">3.5.4.2</span> El estimador de mínimos cuadrados ponderados</a></li>
  <li><a href="#el-estimador-de-máxima-verosimilitud-para-modelos-de-regresión-con-volatilidad-variable-en-el-tiempo" id="toc-el-estimador-de-máxima-verosimilitud-para-modelos-de-regresión-con-volatilidad-variable-en-el-tiempo" class="nav-link" data-scroll-target="#el-estimador-de-máxima-verosimilitud-para-modelos-de-regresión-con-volatilidad-variable-en-el-tiempo"><span class="header-section-number">3.5.4.3</span> El estimador de máxima verosimilitud para modelos de regresión con volatilidad variable en el tiempo</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#autocorrelación" id="toc-autocorrelación" class="nav-link" data-scroll-target="#autocorrelación"><span class="header-section-number">3.6</span> Autocorrelación</a>
  <ul>
  <li><a href="#naturaleza-y-consecuencias-de-la-autocorrelación" id="toc-naturaleza-y-consecuencias-de-la-autocorrelación" class="nav-link" data-scroll-target="#naturaleza-y-consecuencias-de-la-autocorrelación"><span class="header-section-number">3.6.1</span> Naturaleza y consecuencias de la autocorrelación</a></li>
  <li><a href="#detección-de-la-autocorrelación" id="toc-detección-de-la-autocorrelación" class="nav-link" data-scroll-target="#detección-de-la-autocorrelación"><span class="header-section-number">3.6.2</span> Detección de la autocorrelación</a>
  <ul>
  <li><a href="#examen-gráfico-de-los-residuos-mco-1" id="toc-examen-gráfico-de-los-residuos-mco-1" class="nav-link" data-scroll-target="#examen-gráfico-de-los-residuos-mco-1"><span class="header-section-number">3.6.2.1</span> Examen gráfico de los residuos MCO</a></li>
  <li><a href="#contraste-de-durbin-watson" id="toc-contraste-de-durbin-watson" class="nav-link" data-scroll-target="#contraste-de-durbin-watson"><span class="header-section-number">3.6.2.2</span> Contraste de Durbin-Watson</a></li>
  <li><a href="#contraste-general-de-breusch-godfrey" id="toc-contraste-general-de-breusch-godfrey" class="nav-link" data-scroll-target="#contraste-general-de-breusch-godfrey"><span class="header-section-number">3.6.2.3</span> Contraste general de Breusch-Godfrey</a></li>
  <li><a href="#otras-formas-de-autocorrelación-modelos-autorregresivos-con-dependencia-espacial" id="toc-otras-formas-de-autocorrelación-modelos-autorregresivos-con-dependencia-espacial" class="nav-link" data-scroll-target="#otras-formas-de-autocorrelación-modelos-autorregresivos-con-dependencia-espacial"><span class="header-section-number">3.6.2.4</span> Otras formas de autocorrelación: modelos autorregresivos con dependencia espacial</a></li>
  </ul></li>
  <li><a href="#regresiones-con-autocorrelación-en-los-errores-y-modelos-econométricos-dinámicos" id="toc-regresiones-con-autocorrelación-en-los-errores-y-modelos-econométricos-dinámicos" class="nav-link" data-scroll-target="#regresiones-con-autocorrelación-en-los-errores-y-modelos-econométricos-dinámicos"><span class="header-section-number">3.6.3</span> Regresiones con autocorrelación en los errores y modelos econométricos dinámicos</a>
  <ul>
  <li><a href="#el-estimador-de-mínimos-cuadrados-corregidos-método-de-newey-west" id="toc-el-estimador-de-mínimos-cuadrados-corregidos-método-de-newey-west" class="nav-link" data-scroll-target="#el-estimador-de-mínimos-cuadrados-corregidos-método-de-newey-west"><span class="header-section-number">3.6.3.1</span> El estimador de mínimos cuadrados corregidos (método de Newey-West)</a></li>
  <li><a href="#el-estimador-de-mínimos-cuadrados-generalizados" id="toc-el-estimador-de-mínimos-cuadrados-generalizados" class="nav-link" data-scroll-target="#el-estimador-de-mínimos-cuadrados-generalizados"><span class="header-section-number">3.6.3.2</span> El estimador de mínimos cuadrados generalizados</a></li>
  <li><a href="#modelos-con-correlación-espacial-en-los-errores" id="toc-modelos-con-correlación-espacial-en-los-errores" class="nav-link" data-scroll-target="#modelos-con-correlación-espacial-en-los-errores"><span class="header-section-number">3.6.3.3</span> Modelos con correlación espacial en los errores</a></li>
  <li><a href="#modelos-con-dinámica-en-el-tiempo" id="toc-modelos-con-dinámica-en-el-tiempo" class="nav-link" data-scroll-target="#modelos-con-dinámica-en-el-tiempo"><span class="header-section-number">3.6.3.4</span> Modelos con dinámica en el tiempo</a></li>
  <li><a href="#modelos-con-retardos-en-las-variables-explicativas" id="toc-modelos-con-retardos-en-las-variables-explicativas" class="nav-link" data-scroll-target="#modelos-con-retardos-en-las-variables-explicativas"><span class="header-section-number">3.6.3.5</span> Modelos con retardos en las variables explicativas</a></li>
  <li><a href="#modelos-con-retardos-en-la-variable-endógena" id="toc-modelos-con-retardos-en-la-variable-endógena" class="nav-link" data-scroll-target="#modelos-con-retardos-en-la-variable-endógena"><span class="header-section-number">3.6.3.6</span> Modelos con retardos en la variable endógena</a></li>
  <li><a href="#modelos-autorregresivos-con-retardos-distribuidos" id="toc-modelos-autorregresivos-con-retardos-distribuidos" class="nav-link" data-scroll-target="#modelos-autorregresivos-con-retardos-distribuidos"><span class="header-section-number">3.6.3.7</span> Modelos autorregresivos con retardos distribuidos</a></li>
  <li><a href="#modelos-vectoriales-autorregresivos" id="toc-modelos-vectoriales-autorregresivos" class="nav-link" data-scroll-target="#modelos-vectoriales-autorregresivos"><span class="header-section-number">3.6.3.8</span> Modelos vectoriales autorregresivos</a></li>
  <li><a href="#e.-modelos-con-corrección-del-error" id="toc-e.-modelos-con-corrección-del-error" class="nav-link" data-scroll-target="#e.-modelos-con-corrección-del-error"><span class="header-section-number">3.6.3.9</span> Modelos con corrección del error</a></li>
  <li><a href="#modelos-con-dinámica-en-el-espacio" id="toc-modelos-con-dinámica-en-el-espacio" class="nav-link" data-scroll-target="#modelos-con-dinámica-en-el-espacio"><span class="header-section-number">3.6.3.10</span> Modelos con dinámica en el espacio</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#información-muestral" id="toc-información-muestral" class="nav-link" data-scroll-target="#información-muestral"><span class="header-section-number">3.7</span> Información muestral</a>
  <ul>
  <li><a href="#falta-de-observaciones" id="toc-falta-de-observaciones" class="nav-link" data-scroll-target="#falta-de-observaciones"><span class="header-section-number">3.7.1</span> Falta de observaciones</a></li>
  <li><a href="#agregación-de-datos" id="toc-agregación-de-datos" class="nav-link" data-scroll-target="#agregación-de-datos"><span class="header-section-number">3.7.2</span> Agregación de datos</a></li>
  <li><a href="#multicolinealidad" id="toc-multicolinealidad" class="nav-link" data-scroll-target="#multicolinealidad"><span class="header-section-number">3.7.3</span> Multicolinealidad</a>
  <ul>
  <li><a href="#naturaleza-y-causas-de-la-multicolinealidad" id="toc-naturaleza-y-causas-de-la-multicolinealidad" class="nav-link" data-scroll-target="#naturaleza-y-causas-de-la-multicolinealidad"><span class="header-section-number">3.7.3.1</span> Naturaleza y causas de la multicolinealidad</a></li>
  <li><a href="#consecuencias-de-la-multicolinealidad" id="toc-consecuencias-de-la-multicolinealidad" class="nav-link" data-scroll-target="#consecuencias-de-la-multicolinealidad"><span class="header-section-number">3.7.3.2</span> Consecuencias de la multicolinealidad</a></li>
  <li><a href="#detección-de-la-multicolinealidad" id="toc-detección-de-la-multicolinealidad" class="nav-link" data-scroll-target="#detección-de-la-multicolinealidad"><span class="header-section-number">3.7.3.3</span> Detección de la multicolinealidad</a></li>
  <li><a href="#corrección-de-multicolinealidad" id="toc-corrección-de-multicolinealidad" class="nav-link" data-scroll-target="#corrección-de-multicolinealidad"><span class="header-section-number">3.7.3.4</span> Corrección de multicolinealidad</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#regresores-endógenos-errores-de-medida-yo-simultaneidad-en-las-variables-del-modelo" id="toc-regresores-endógenos-errores-de-medida-yo-simultaneidad-en-las-variables-del-modelo" class="nav-link" data-scroll-target="#regresores-endógenos-errores-de-medida-yo-simultaneidad-en-las-variables-del-modelo"><span class="header-section-number">3.8</span> Regresores endógenos: errores de medida y/o simultaneidad en las variables del modelo</a>
  <ul>
  <li><a href="#errores-de-medida-en-la-variable-dependiente" id="toc-errores-de-medida-en-la-variable-dependiente" class="nav-link" data-scroll-target="#errores-de-medida-en-la-variable-dependiente"><span class="header-section-number">3.8.1</span> Errores de medida en la variable dependiente</a></li>
  <li><a href="#errores-de-medida-en-las-variables-explicativas" id="toc-errores-de-medida-en-las-variables-explicativas" class="nav-link" data-scroll-target="#errores-de-medida-en-las-variables-explicativas"><span class="header-section-number">3.8.2</span> Errores de medida en las variables explicativas</a></li>
  <li><a href="#modelos-de-múltiples-ecuaciones-el-problema-de-la-simultaneidad" id="toc-modelos-de-múltiples-ecuaciones-el-problema-de-la-simultaneidad" class="nav-link" data-scroll-target="#modelos-de-múltiples-ecuaciones-el-problema-de-la-simultaneidad"><span class="header-section-number">3.8.3</span> Modelos de múltiples ecuaciones: el problema de la simultaneidad</a></li>
  <li><a href="#el-caso-general-de-variables-explicativas-estocásticas-correlacionadas-con-la-perturbación" id="toc-el-caso-general-de-variables-explicativas-estocásticas-correlacionadas-con-la-perturbación" class="nav-link" data-scroll-target="#el-caso-general-de-variables-explicativas-estocásticas-correlacionadas-con-la-perturbación"><span class="header-section-number">3.8.4</span> El caso general de variables explicativas estocásticas correlacionadas con la perturbación</a></li>
  <li><a href="#las-regresiones-con-variables-instrumentales" id="toc-las-regresiones-con-variables-instrumentales" class="nav-link" data-scroll-target="#las-regresiones-con-variables-instrumentales"><span class="header-section-number">3.8.5</span> Las regresiones con variables instrumentales</a>
  <ul>
  <li><a href="#el-test-de-hausman-de-endogeneidad" id="toc-el-test-de-hausman-de-endogeneidad" class="nav-link" data-scroll-target="#el-test-de-hausman-de-endogeneidad"><span class="header-section-number">3.8.5.1</span> El test de Hausman de endogeneidad</a></li>
  <li><a href="#el-estimador-vi-de-variables-instrumentales-mínimos-cuadrados-en-dos-etapas-mc2e" id="toc-el-estimador-vi-de-variables-instrumentales-mínimos-cuadrados-en-dos-etapas-mc2e" class="nav-link" data-scroll-target="#el-estimador-vi-de-variables-instrumentales-mínimos-cuadrados-en-dos-etapas-mc2e"><span class="header-section-number">3.8.5.2</span> El estimador VI de variables instrumentales (mínimos cuadrados en dos etapas, MC2E)</a></li>
  <li><a href="#el-test-de-sargan-de-validez-de-los-instrumentos" id="toc-el-test-de-sargan-de-validez-de-los-instrumentos" class="nav-link" data-scroll-target="#el-test-de-sargan-de-validez-de-los-instrumentos"><span class="header-section-number">3.8.5.3</span> El test de Sargan de validez de los instrumentos</a></li>
  <li><a href="#el-contraste-de-cragg-donald-de-debilidad-de-los-instrumentos" id="toc-el-contraste-de-cragg-donald-de-debilidad-de-los-instrumentos" class="nav-link" data-scroll-target="#el-contraste-de-cragg-donald-de-debilidad-de-los-instrumentos"><span class="header-section-number">3.8.5.4</span> El contraste de Cragg-Donald de debilidad de los instrumentos</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#observaciones-atípicas" id="toc-observaciones-atípicas" class="nav-link" data-scroll-target="#observaciones-atípicas"><span class="header-section-number">3.9</span> Observaciones atípicas</a>
  <ul>
  <li><a href="#identificación-y-análisis-de-la-influencia-de-las-observaciones-atípicas" id="toc-identificación-y-análisis-de-la-influencia-de-las-observaciones-atípicas" class="nav-link" data-scroll-target="#identificación-y-análisis-de-la-influencia-de-las-observaciones-atípicas"><span class="header-section-number">3.9.1</span> Identificación y análisis de la influencia de las observaciones atípicas</a>
  <ul>
  <li><a href="#detección-de-observaciones-atípicas-en-las-variables-explicativas" id="toc-detección-de-observaciones-atípicas-en-las-variables-explicativas" class="nav-link" data-scroll-target="#detección-de-observaciones-atípicas-en-las-variables-explicativas"><span class="header-section-number">3.9.1.1</span> Detección de observaciones atípicas en las variables explicativas</a></li>
  <li><a href="#detección-de-observaciones-atípicas-en-la-variable-dependiente" id="toc-detección-de-observaciones-atípicas-en-la-variable-dependiente" class="nav-link" data-scroll-target="#detección-de-observaciones-atípicas-en-la-variable-dependiente"><span class="header-section-number">3.9.1.2</span> Detección de observaciones atípicas en la variable dependiente</a></li>
  <li><a href="#influencia-real-de-las-observaciones-atípicas" id="toc-influencia-real-de-las-observaciones-atípicas" class="nav-link" data-scroll-target="#influencia-real-de-las-observaciones-atípicas"><span class="header-section-number">3.9.1.3</span> Influencia real de las observaciones atípicas</a></li>
  </ul></li>
  <li><a href="#cuestiones-que-plantea-la-presencia-de-observaciones-atípicas" id="toc-cuestiones-que-plantea-la-presencia-de-observaciones-atípicas" class="nav-link" data-scroll-target="#cuestiones-que-plantea-la-presencia-de-observaciones-atípicas"><span class="header-section-number">3.9.2</span> Cuestiones que plantea la presencia de observaciones atípicas</a></li>
  </ul></li>
  <li><a href="#variable-dependiente-discreta-o-limitada" id="toc-variable-dependiente-discreta-o-limitada" class="nav-link" data-scroll-target="#variable-dependiente-discreta-o-limitada"><span class="header-section-number">3.10</span> Variable dependiente discreta o limitada</a>
  <ul>
  <li><a href="#variable-dependiente-discreta" id="toc-variable-dependiente-discreta" class="nav-link" data-scroll-target="#variable-dependiente-discreta"><span class="header-section-number">3.10.1</span> Variable dependiente discreta</a>
  <ul>
  <li><a href="#el-modelo-de-probabilidad-lineal" id="toc-el-modelo-de-probabilidad-lineal" class="nav-link" data-scroll-target="#el-modelo-de-probabilidad-lineal"><span class="header-section-number">3.10.1.1</span> El modelo de probabilidad lineal</a></li>
  <li><a href="#modelos-logit-y-probit" id="toc-modelos-logit-y-probit" class="nav-link" data-scroll-target="#modelos-logit-y-probit"><span class="header-section-number">3.10.1.2</span> Modelos Logit y Probit</a></li>
  <li><a href="#el-modelo-logit" id="toc-el-modelo-logit" class="nav-link" data-scroll-target="#el-modelo-logit"><span class="header-section-number">3.10.1.3</span> El modelo Logit</a></li>
  <li><a href="#el-modelo-probit" id="toc-el-modelo-probit" class="nav-link" data-scroll-target="#el-modelo-probit"><span class="header-section-number">3.10.1.4</span> El modelo Probit</a></li>
  <li><a href="#consideraciones-especiales-en-los-modelos-logit-y-probit" id="toc-consideraciones-especiales-en-los-modelos-logit-y-probit" class="nav-link" data-scroll-target="#consideraciones-especiales-en-los-modelos-logit-y-probit"><span class="header-section-number">3.10.1.5</span> Consideraciones especiales en los modelos Logit y Probit</a>
  <ul class="collapse">
  <li><a href="#estimación" id="toc-estimación" class="nav-link" data-scroll-target="#estimación"><span class="header-section-number">3.10.1.5.1</span> Estimación</a></li>
  <li><a href="#predicción" id="toc-predicción" class="nav-link" data-scroll-target="#predicción"><span class="header-section-number">3.10.1.5.2</span> Predicción</a></li>
  <li><a href="#grado-de-ajuste-del-modelo" id="toc-grado-de-ajuste-del-modelo" class="nav-link" data-scroll-target="#grado-de-ajuste-del-modelo"><span class="header-section-number">3.10.1.5.3</span> Grado de ajuste del modelo</a></li>
  <li><a href="#contraste-de-hipótesis" id="toc-contraste-de-hipótesis" class="nav-link" data-scroll-target="#contraste-de-hipótesis"><span class="header-section-number">3.10.1.5.4</span> Contraste de hipótesis</a></li>
  <li><a href="#efectos-marginales" id="toc-efectos-marginales" class="nav-link" data-scroll-target="#efectos-marginales"><span class="header-section-number">3.10.1.5.5</span> Efectos marginales</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#variable-dependiente-limitada" id="toc-variable-dependiente-limitada" class="nav-link" data-scroll-target="#variable-dependiente-limitada"><span class="header-section-number">3.10.2</span> Variable dependiente limitada</a>
  <ul>
  <li><a href="#valores-censurados-el-modelo-tobit" id="toc-valores-censurados-el-modelo-tobit" class="nav-link" data-scroll-target="#valores-censurados-el-modelo-tobit"><span class="header-section-number">3.10.2.1</span> Valores censurados: el modelo Tobit</a></li>
  <li><a href="#valores-con-truncamiento-selectivo-el-modelo-heckit" id="toc-valores-con-truncamiento-selectivo-el-modelo-heckit" class="nav-link" data-scroll-target="#valores-con-truncamiento-selectivo-el-modelo-heckit"><span class="header-section-number">3.10.2.2</span> Valores con truncamiento selectivo: el modelo Heckit</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#paneles-de-datos" id="toc-paneles-de-datos" class="nav-link" data-scroll-target="#paneles-de-datos"><span class="header-section-number">3.11</span> Paneles de datos</a>
  <ul>
  <li><a href="#regresiones-para-datos-de-panel-y-efectos-individuales-inobservables" id="toc-regresiones-para-datos-de-panel-y-efectos-individuales-inobservables" class="nav-link" data-scroll-target="#regresiones-para-datos-de-panel-y-efectos-individuales-inobservables"><span class="header-section-number">3.11.1</span> Regresiones para datos de panel y efectos individuales inobservables</a></li>
  <li><a href="#ventajas-y-limitaciones-derivadas-del-uso-de-paneles-de-datos" id="toc-ventajas-y-limitaciones-derivadas-del-uso-de-paneles-de-datos" class="nav-link" data-scroll-target="#ventajas-y-limitaciones-derivadas-del-uso-de-paneles-de-datos"><span class="header-section-number">3.11.2</span> Ventajas y limitaciones derivadas del uso de paneles de datos</a></li>
  <li><a href="#modelo-de-efectos-fijos" id="toc-modelo-de-efectos-fijos" class="nav-link" data-scroll-target="#modelo-de-efectos-fijos"><span class="header-section-number">3.11.3</span> Modelo de efectos fijos</a>
  <ul>
  <li><a href="#modelo-con-efectos-individuales-y-efectos-temporales-fijos" id="toc-modelo-con-efectos-individuales-y-efectos-temporales-fijos" class="nav-link" data-scroll-target="#modelo-con-efectos-individuales-y-efectos-temporales-fijos"><span class="header-section-number">3.11.3.1</span> Modelo con efectos individuales y efectos temporales fijos</a></li>
  <li><a href="#anotaciones-respecto-al-modelo-de-efectos-fijos" id="toc-anotaciones-respecto-al-modelo-de-efectos-fijos" class="nav-link" data-scroll-target="#anotaciones-respecto-al-modelo-de-efectos-fijos"><span class="header-section-number">3.11.3.2</span> Anotaciones respecto al modelo de efectos fijos</a></li>
  </ul></li>
  <li><a href="#modelo-de-efectos-aleatorios" id="toc-modelo-de-efectos-aleatorios" class="nav-link" data-scroll-target="#modelo-de-efectos-aleatorios"><span class="header-section-number">3.11.4</span> Modelo de efectos aleatorios</a>
  <ul>
  <li><a href="#anotaciones-respecto-al-modelo-de-efectos-aleatorios" id="toc-anotaciones-respecto-al-modelo-de-efectos-aleatorios" class="nav-link" data-scroll-target="#anotaciones-respecto-al-modelo-de-efectos-aleatorios"><span class="header-section-number">3.11.4.1</span> Anotaciones respecto al modelo de efectos aleatorios</a></li>
  </ul></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Diagnosis, correcciones y extensiones del modelo de regresión lineal</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<section id="criterios-generales-de-evaluación-y-validación-de-un-modelo-econométrico" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="criterios-generales-de-evaluación-y-validación-de-un-modelo-econométrico"><span class="header-section-number">3.1</span> Criterios generales de evaluación y validación de un modelo econométrico</h2>
<p>Una vez que se ha estimado un modelo de regresión por MCO, se hace necesario calibrar hasta qué punto los resultados obtenidos pueden considerarse como aceptables, antes de proceder al uso empírico del modelo. Más aún, cuando se dispone de distintas especificaciones alternativas, de varias variables exógenas potenciales o de varias formas funcionales posibles, es preciso disponer de criterios que nos permitan seleccionar el modelo más idóneo.</p>
<p>En ambos casos, entre los criterios más importantes que pueden utilizarse para evaluar o validar uno o varios de los modelos estimados, se pueden destacar los siguientes:</p>
<section id="coherencia-del-modelo-estimado-con-los-datos" class="level3" data-number="3.1.1">
<h3 data-number="3.1.1" class="anchored" data-anchor-id="coherencia-del-modelo-estimado-con-los-datos"><span class="header-section-number">3.1.1</span> Coherencia del modelo estimado con los datos</h3>
<p>En términos generales, al menos ha de exigirse que los modelos estimados ‘sean capaces de explicar los datos existentes’. Ello implica satisfacer distintas propiedades:</p>
<ul>
<li><p>En primer lugar, se contrastará la significación conjunta del modelo (coeficientes <span class="math inline">\(R^{2}\)</span> o <span class="math inline">\({\overline{R}}^{2}\)</span> y estadístico <span class="math inline">\(F\)</span>) y la de los parámetros individuales (<span class="math inline">\(t\)</span>-ratios), así como su adecuación a la teoría económica subyacente (signos y magnitudes de los coeficientes estimados correctos, propiedades de largo y corto plazo, etc.).</p></li>
<li><p>También debe examinarse la coherencia <em>a posteriori</em> con las hipótesis establecidas <em>a priori</em> para el MRL. Aunque en los epígrafes 2 a 4 de este tema analizaremos de forma independiente cada una de ellas, existen en la literatura contrastes que permiten hacer una valoración general del cumplimiento de dichas hipótesis. En concreto, Peña y Slate (2006) han propuesto una <em>prueba de validez global</em> de las hipótesis del MRL que permite contrastar simultáneamente las propiedades P1-P4 que debe cumplir el modelo. El estadístico global se define por la expresión</p>
<p><span class="math display">\[{\widehat{G}}_{4}^{2} = {\widehat{S}}_{1}^{2} + {\widehat{S}}_{2}^{2} + {\widehat{S}}_{3}^{2} + {\widehat{S}}_{4}^{2}\]</span></p>
<p>y cada componente de este, <span class="math inline">\({\widehat{S}}_{j}^{2}\)</span>, es un estadístico direccional relacionado con el cumplimiento de las propiedades individuales. Como este estadístico tiene una distribución asintótica <span class="math inline">\(\chi^{2}\)</span>, en términos prácticos puede asumirse que, siempre que se cumpla que <span class="math inline">\(n - K \geq 30\)</span>, cuando se verifique <span class="math inline">\({\widehat{G}}_{4}^{2} &gt; \chi_{4,\alpha}^{2}\)</span> al menos una de las propiedades P1-P4 no es válida.</p></li>
<li><p>Aparte de estas medidas de coherencia global, los modelos elegidos deben ser ‘parsimoniosos’, esto es, no contener un excesivo número de parámetros, y a ser posible ‘anidar’ a todos los posibles modelos competidores. Esta última propiedad implica que el modelo empírico seleccionado no sólo debería ser coherente con los datos, sino también explicar por qué otros modelos competidores son capaces o no de hacer eso mismo.</p></li>
</ul>
</section>
<section id="estadísticos-de-precisión-e-información" class="level3" data-number="3.1.2">
<h3 data-number="3.1.2" class="anchored" data-anchor-id="estadísticos-de-precisión-e-información"><span class="header-section-number">3.1.2</span> Estadísticos de precisión e información</h3>
<p>Estas medidas están basadas en la comparación entre los valores reales observados y los ajustados (predichos) por el modelo, e intentan medir el grado de acierto <em>ex post</em> en las predicciones del modelo. Existen diferentes formas de medir esta precisión de las predicciones.</p>
<p>Una primera aproximación es representar, o bien los valores de la variable <em>y</em> frente a sus valores ajustados (estimados) <span class="math inline">\(\widehat{y}\)</span> , o los residuos de la estimación. Ambas representaciones pueden facilitar una importante información sobre la validez del modelo y el cumplimiento de las hipótesis básicas.</p>
<p>En segundo lugar, como medidas no gráficas de aproximación de las predicciones, o de la capacidad predictiva del modelo, se utilizan habitualmente los siguientes <em>estadísticos de precisión</em>:</p>
<ul>
<li><p>El <em>error cuadrático medio</em>: &gt; <span class="math inline">\(ECM = \frac{\sum_{i = 1}^{n}\left( y_{i} - {\widehat{y}}_{i} \right)^{2}}{n}\)</span>, &gt; o su raíz cuadrada, <span class="math inline">\(RECM = \sqrt{ECM}\)</span></p></li>
<li><p>El <em>error absoluto medio</em>: &gt; <span class="math inline">\(EAM = \frac{\sum_{i = 1}^{n}\left| y_{i} - {\widehat{y}}_{i} \right|}{n}\)</span></p></li>
</ul>
<p>Finalmente, también es habitual el uso de diferentes *estadísticos de información** para seleccionar entre los distintos modelos disponibles. Estos estadísticos están basados en el valor (del logaritmo) de la función de verosimilitud del MRL</p>
<p><span class="math display">\[\log{L\left( \widehat{\beta},{\widehat{\sigma}}^{2} \right)} = - \frac{n}{2}\left\lbrack 1 + \log{2\pi} + \log(ECM) \right\rbrack\]</span></p>
<p>el cual mide la ‘distancia’ del modelo estimado respecto del modelo “real” y, por tanto, es una medida del ajuste estadístico proporcionado por las estimaciones. Entre los más importantes podemos mencionar los dos siguientes:</p>
<ul>
<li><p><em>Criterio de información de Akaike</em>: <span class="math inline">\(AIC = nlog(ECM) + 2K\)</span></p></li>
<li><p><em>Criterio Bayesiano de Schwarz</em>: <span class="math inline">\(BIC = nlog(ECM) + Klog(n)\)</span></p></li>
</ul>
<p>Para cada uno de estos criterios se considera que el mejor modelo corresponde al menor valor del estadístico. Entre los dos criterios de selección propuestos, el <span class="math inline">\(BIC\)</span> seleccionará en general el modelo más ‘parsimonioso’, es decir, el modelo con un menor número de parámetros libres a estimar, mientras que el <span class="math inline">\(AIC\)</span> elegirá el más parametrizado.</p>
</section>
</section>
<section id="especificación-del-modelo" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="especificación-del-modelo"><span class="header-section-number">3.2</span> Especificación del modelo</h2>
<section id="determinación-de-los-regresores-y-de-la-forma-funcional" class="level3" data-number="3.2.1">
<h3 data-number="3.2.1" class="anchored" data-anchor-id="determinación-de-los-regresores-y-de-la-forma-funcional"><span class="header-section-number">3.2.1</span> Determinación de los regresores y de la forma funcional</h3>
<p>Hasta el momento se ha tomado como punto de partida la hipótesis de que el modelo de regresión lineal <span class="math inline">\(\mathbf{y} = \mathbf{X\beta} + \mathbf{e}\)</span> está bien especificado. Este apartado se centrará en analizar las implicaciones prácticas que tiene una mala especificación del modelo de regresión.</p>
<p>Los errores de especificación más frecuentes son la omisión de variables relevantes, la inclusión de variables irrelevantes o la elección de una forma funcional incorrecta. En general, estos errores proceden del desconocimiento del verdadero modelo que subyace tras un fenómeno económico, de la escasa disponibilidad de datos o de una teoría económica poco precisa. Un valor bajo del <span class="math inline">\(R^{2}\)</span>, signos incorrectos en las estimaciones de los parámetros o pocas variables significativas pueden ser indicativos de la omisión de variables importantes o de una forma funcional incorrecta. También puede ser útil el examen de los residuos, en los cuales puede detectarse algún patrón de comportamiento anormal, indicativo de la mala especificación del modelo.</p>
</section>
<section id="omisión-de-variables-relevantes" class="level3" data-number="3.2.2">
<h3 data-number="3.2.2" class="anchored" data-anchor-id="omisión-de-variables-relevantes"><span class="header-section-number">3.2.2</span> Omisión de variables relevantes</h3>
<p>Cuando se omiten algunas variables relevantes de una regresión, <span class="math inline">\(z_{1},z_{2},\ldots,z_{s}\)</span>, incluyendo en el modelo solo las variables <span class="math inline">\(x_{1},x_{2},\ldots,x_{K}\)</span>, formalmente la situación que se plantea es la siguiente:</p>
<p>Modelo estimado: <span class="math inline">\(\mathbf{y} = \mathbf{X\beta} + \mathbf{e}\)</span></p>
<p>Modelo verdadero: <span class="math inline">\(\mathbf{y} = \mathbf{X\beta} + \mathbf{Z\gamma} + \mathbf{e}\)</span></p>
<p>En este caso, el estimador MCO puede escribirse como:</p>
<p><span class="math display">\[{\widehat{\mathbf{\beta}}}_{MCO} = \mathbf{\beta} + (\mathbf{X}^{'}\mathbf{X})^{- 1}\mathbf{X}^{'}\mathbf{Z\gamma} + (\mathbf{X}^{'}\mathbf{X})^{- 1}\mathbf{X}^{'}\mathbf{e}\]</span></p>
<p>y, en consecuencia, su esperanza viene dada por <span class="math inline">\(E\lbrack{\widehat{\mathbf{\beta}}}_{MCO}\rbrack = \mathbf{\beta} + (\mathbf{X}^{'}\mathbf{X})^{- 1}\mathbf{X}^{'}\mathbf{Z\gamma} \neq \mathbf{\beta}\)</span>. Por tanto, el estimador MCO es sesgado y además inconsistente, pues el sesgo <span class="math inline">\((\mathbf{X}^{'}\mathbf{X})^{- 1}\mathbf{X}^{'}\mathbf{Z\gamma}\)</span> no desaparece al aumentar el tamaño de la muestra, salvo que se cumpla que <span class="math inline">\(\mathbf{X}^{'}\mathbf{Z} = \mathbf{0}\)</span>, es decir, que las variables omitidas y las incluidas sean ortogonales entre sí, resultado bastante inusual en la práctica.</p>
<p>También se puede demostrar que la varianza, <span class="math inline">\(\sigma^{2}\)</span>, se estima con sesgo, <span class="math inline">\(E({\widehat{\sigma}}^{2}) \neq \sigma^{2}\)</span> Por tanto, los intervalos de confianza y los contrastes de hipótesis son erróneos, lo cual puede dar lugar a conclusiones equivocadas.</p>
</section>
<section id="inclusión-de-variables-irrelevantes" class="level3" data-number="3.2.3">
<h3 data-number="3.2.3" class="anchored" data-anchor-id="inclusión-de-variables-irrelevantes"><span class="header-section-number">3.2.3</span> Inclusión de variables irrelevantes</h3>
<p>Ahora el modelo estimado incluye algunas variables irrelevantes, <span class="math inline">\(z_{1},z_{2},\ldots,z_{s}\)</span>, en su especificación. Es decir, la situación que se plantea es la siguiente:</p>
<p>Modelo verdadero: <span class="math inline">\(\mathbf{y} = \mathbf{X\beta} + \mathbf{e}\)</span></p>
<p>Modelo estimado: <span class="math inline">\(\mathbf{y} = \mathbf{X\beta} + \mathbf{Z\gamma} + \mathbf{e}\)</span></p>
<p>Se puede demostrar que, en este caso, se tienen las siguientes propiedades:</p>
<ul>
<li><p><span class="math inline">\({\widehat{\mathbf{\beta}}}_{MCO}\)</span> es un estimador insesgado y consistente de <strong><em>β</em></strong>, <span class="math inline">\(E\left( {\widehat{\mathbf{\beta}}}_{MCO} \right) = \mathbf{\beta}\)</span>, aunque no es eficiente ya que el modelo verdadero coincide con el modelo estimado si se impone la restricción <strong><em>γ</em></strong> = <strong>0</strong>, siendo entonces aplicables los resultados obtenidos con el estimador de mínimos cuadrados restringidos.</p></li>
<li><p>La varianza residual <span class="math inline">\({\widehat{\sigma}}^{2} = \frac{\sum_{i = 1}^{n}{\widehat{e}}_{i}^{2}}{n - K}\)</span> es un estimador insesgado, <span class="math inline">\(E\left( {\widehat{\sigma}}^{2} \right) = \sigma^{2}\)</span>.</p></li>
<li><p>Los procedimientos usuales de intervalos de confianza y contrastes de hipótesis siguen siendo válidos, aunque hay que tener en cuenta que las varianzas estimadas serán mayores que las del modelo verdadero (la inferencia estadística será, por tanto, menos fiable).</p></li>
</ul>
</section>
<section id="el-contraste-reset-de-especificación-correcta-de-la-forma-funcional" class="level3" data-number="3.2.4">
<h3 data-number="3.2.4" class="anchored" data-anchor-id="el-contraste-reset-de-especificación-correcta-de-la-forma-funcional"><span class="header-section-number">3.2.4</span> El contraste RESET de especificación correcta de la forma funcional</h3>
<p>Un contraste muy utilizado para analizar la correcta especificación funcional de un modelo de regresión es la llamada prueba <em>RESET</em> (REgression Specification Error Test) de Ramsey. La esencia del test se basa en contrastar si en un modelo de regresión deben añadirse regresores adicionales, <span class="math inline">\(z_{1},z_{2},\ldots,z_{s}\)</span>, a los ya considerados en el modelo estándar <span class="math inline">\(x_{1},x_{2},\ldots,x_{K}\)</span>: si se comete un error por omisión de variables relevantes, las variables adicionales <em>z</em>’s deberían ser incluidas en la regresión; si el problema deriva de una forma funcional incorrecta, las variables incluidas deberán ser alguna función (logaritmos, potencias, recíprocos o alguna otra transformación) de las variables <em>x<sub>j</sub></em> ya incluidas en la regresión. En ambos casos, los errores de especificación funcional provocan que el estimador MCO sea sesgado e inconsistente y los procedimientos de inferencia estándar no serán válidos.</p>
<p>Si, <em>a priori</em>, se desconoce si se está cometiendo un error de especificación (del tipo que sea), Ramsey sugiere utilizar como aproximaciones de las posibles variables omitidas las potencias de los valores predichos de la variable dependiente (<span class="math inline">\({\widehat{y}}_{i}^{2},{\widehat{y}}_{i}^{3},\ldots\)</span>) al estimar el modelo <span class="math inline">\(\mathbf{y} = \mathbf{X\beta} + \mathbf{e}\)</span> por MCO, contrastando su significación al añadirlas al modelo original.</p>
<p>Así, en términos operativos, la prueba <em>RESET</em> consiste en contrastar</p>
<p><span class="math display">\[H_{0}:\{ forma\ funcional\ correcta\}\ \ frente\ a\ \ H_{1}:\{ forma\ funcional\ incorrecta\}\]</span></p>
<p>Para llevar a cabo el test se procede del siguiente modo:</p>
<ul>
<li><p>Se estima el modelo original por MCO y se obtienen los valores ajustados de la regresión, <span class="math inline">\(\widehat{y}\)</span>.</p></li>
<li><p>Se estima la regresión de <em>y</em> frente a los regresores originales y las potencias de <span class="math inline">\(\widehat{y}\)</span>. Cuando se incluye sólo la variable <span class="math inline">\({\widehat{y}}^{2}\ \)</span>se denomina al test <em>RESET</em>(1) <span class="math display">\[y_{i} = \beta_{1} + \beta_{2}x_{2i} + ... + \beta_{K}x_{Ki} + \gamma_{1}{\widehat{y}}_{i}^{2} + e_{t}\]</span> mientras que si se incluyen las potencias segunda y tercera (<span class="math inline">\({\widehat{y}}_{i}^{2}\)</span> y <span class="math inline">\({\widehat{y}}_{i}^{3}\)</span>), <em>RESET</em>(2)</p>
<p><span class="math display">\[y_{i} = \beta_{1} + \beta_{2}x_{2i} + ... + \beta_{K}x_{Ki} + \gamma_{1}{\widehat{y}}_{i}^{2} + \gamma_{2}{\widehat{y}}_{i}^{3} + e_{t}\]</span></p>
<p>y así sucesivamente.</p></li>
<li><p>Por último, se contrasta mediante un test <em>F</em> de restricciones lineales la significación de las potencias de <span class="math inline">\(\widehat{y}\)</span>: <span class="math inline">\(H_{0}:\left\{ \gamma_{1} = 0 \right\}\)</span> para el test <em>RESET</em>(1), <span class="math inline">\(H_{0}:\left\{ \gamma_{1} = 0\ ,\ \gamma_{2} = 0 \right\}\)</span> para el test <em>RESET</em>(2), etc.</p></li>
</ul>
<p>Si se rechaza la hipótesis nula, ello implicará que la forma funcional propuesta no es la adecuada, por lo que habrá que investigar si la variable dependiente <em>y</em> o alguna de las variables en la matriz de variables explicativas <em>X</em> deberían transformarse (por ejemplo, tomando logaritmos), si deberían añadirse regresores adicionales (por ejemplo, las potencias de alguna variable explicativa) o, de forma más general, si debería especificarse un modelo no lineal en lugar de una relación lineal.</p>
</section>
</section>
<section id="estabilidad-de-los-parámetros-estructurales" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="estabilidad-de-los-parámetros-estructurales"><span class="header-section-number">3.3</span> Estabilidad de los parámetros estructurales</h2>
<p>En el capítulo 2 se han tratado las cuestiones de cómo estimar el vector de parámetros <strong><span class="math inline">\(\mathbf{\beta}\)</span></strong> del modelo lineal <span class="math inline">\(\mathbf{y} = \mathbf{X\beta} + \mathbf{e}\)</span>, obtener intervalos o regiones de confianza, contrastar diferentes hipótesis y realizar predicciones. Para ello, partíamos de unas hipótesis básicas, que nos permitían obtener el estimador MCO y sus propiedades.</p>
<p>Una de estas hipótesis, la constancia de los parámetros estructurales a lo largo de las observaciones muestrales es, en muchas ocasiones, difícilmente sostenible. Así, por ejemplo, si estimamos una función de producción de las empresas industriales, puede ser una hipótesis demasiado restrictiva considerar que la función es la misma para las empresas pequeñas y grandes; o si se está estudiando la función de consumo de las familias españolas desde 1950 hasta la actualidad, puede ocurrir que ésta no permanezca constante durante todo ese período.</p>
<p>Por otro lado, si el objetivo de un estudio es la determinación de los factores que influyen en las ventas de unos grandes almacenes, debemos aceptar que existe una fuerte estacionalidad en el comportamiento de los consumidores, lo que puede hacer dudar acerca la hipótesis de constancia de los parámetros entre las distintas estaciones.</p>
<p>Por último, pueden existir razones <em>a priori</em> para sospechar que durante el período de observación han existido fenómenos o hechos que han provocado un cambio o una ruptura en la estructura del modelo. Así, por ejemplo, la entrada en la Unión Europea, la implantación del euro, las guerras del petróleo de los años 70, las crisis financieras, la aparición de nuevas tecnologías, el cambio del marco legislativo, etc., pueden ser la causa de que exista cambio estructural.</p>
<p>Si disponemos de datos de series temporales, y han existido uno o varios momentos en los cuales han cambiado las estructuras, en nuestro conjunto de datos existirán dos o más modelos. En el caso de datos de corte transversal, la situación puede ser análoga por la existencia de dos o más subpoblaciones, con diferentes parámetros estructurales cada una de ellas.</p>
<section id="los-contrastes-de-chow-y-quandt-de-cambio-estructural" class="level3" data-number="3.3.1">
<h3 data-number="3.3.1" class="anchored" data-anchor-id="los-contrastes-de-chow-y-quandt-de-cambio-estructural"><span class="header-section-number">3.3.1</span> Los contrastes de Chow y Quandt de cambio estructural</h3>
<p>Dado un modelo lineal, para el cual se dispone de <em>T</em> observaciones temporales, supongamos que se conoce la existencia un momento en el período de observación que nos hace sospechar de un cambio estructural. Si el cambio ha tenido lugar en el momento <em>T</em><sub>1</sub>+1, las primeras <em>T</em><sub>1</sub> observaciones y las <em>T</em><sub>2</sub> últimas (<em>T</em>=<em>T</em><sub>1</sub>+<em>T</em><sub>2</sub>) corresponderán a diferentes estructuras. Analíticamente:</p>
<p><span class="math display">\[y_{t} = \beta_{1}^{1} + \beta_{2}^{1}x_{2t} + \cdots + \beta_{K}^{1}x_{Kt} + e_{t}\ \ para\ \ t = 1,2,\ldots,T_{1}\]</span></p>
<p><span class="math display">\[y_{t} = \beta_{1}^{2} + \beta_{2}^{2}x_{2t} + \cdots + \beta_{K}^{2}x_{Kt} + e_{t}\ \ para\ \ t = T_{1} + 1,\ldots,T\]</span></p>
<p>o en forma matricial</p>
<p><span class="math display">\[\mathbf{y}^{1} = \mathbf{X}^{1}\mathbf{\beta}^{1} + \mathbf{e}^{1}\]</span> <span class="math display">\[\mathbf{y}^{2} = \mathbf{X}^{2}\mathbf{\beta}^{2} + \mathbf{e}^{2}\]</span></p>
<p>donde <span class="math inline">\(\mathbf{\beta}^{i}\)</span>, <em><strong>y</strong><sup>i</sup></em>, <em><strong>X</strong><sup>i</sup></em>, <em><strong>e</strong><sup>i</sup></em> son los vectores y matrices con la estructura, los valores de las observaciones y los errores, respectivamente, para cada período <em>i</em> (<em>i</em> = 1, 2).</p>
<p>De este modo, la hipótesis de estabilidad estructural, o existencia de una sola estructura, vendrá dada por</p>
<p><span class="math display">\[H_{0}:\{\mathbf{\beta}^{1} = \mathbf{\beta}^{2}\}\ \ frente\ a\ \ H_{1}:\{\mathbf{\beta}^{1} \neq \mathbf{\beta}^{2}\}\]</span></p>
<p>Se trata de un contraste de hipótesis de <em>K</em> restricciones lineales que, por lo tanto, se puede realizar mediante el uso del estadístico <em>F</em>, lo cual da lugar a la <em>prueba de Chow</em>, que toma la forma siguiente:</p>
<p><span class="math display">\[CHOW = \frac{(SRCT - (SRC1 + SRC2))/K}{(SRC1 + SRC2)/(T - 2K)}\]</span></p>
<p>donde <em>SRCT</em> es la suma de cuadrados residual de la estimación MCO del modelo <span class="math inline">\(\mathbf{y} = \mathbf{X\beta} + \mathbf{e}\)</span> con las <em>T</em> observaciones, mientras que <em>SRC1</em> y <em>SRC2</em> denotan, respectivamente, las sumas de los cuadrados de los residuos de las estimaciones MCO de los modelos <span class="math inline">\(\mathbf{y}^{1} = \mathbf{X}^{1}\mathbf{\beta}^{1} + \mathbf{e}^{1}\)</span> y <span class="math inline">\(\mathbf{y}^{2} = \mathbf{X}^{2}\mathbf{\beta}^{2} + \mathbf{e}^{2}\)</span>. Al ser un contraste <em>F</em> de restricciones lineales, el estadístico propuesto se distribuye bajo la hipótesis nula <span class="math inline">\(H_{0}:\{\mathbf{\beta}^{1} = \mathbf{\beta}^{2}\}\)</span> como <span class="math inline">\(\ CHOW\sim F_{K,\ \ T - 2K}\)</span> .</p>
<p>Si en lugar de trabajar con datos de series temporales se contase con datos de corte transversal, la prueba de Chow comenzaría ordenando las observaciones respecto a alguna variable concreta (el origen del cambio estructural), que dividiría la muestra en dos subconjuntos distintos, y usando los grupos de las <em>n</em><sub>1</sub> primeras observaciones y de las <em>n</em><sub>2</sub> últimas observaciones para el contraste.</p>
<p>El test de Chow estándar es el más extendido en la literatura econométrica para contrastar la hipótesis de estabilidad estructural. No obstante, su aplicación presenta algunos problemas:</p>
<ul>
<li><p>Tal como se ha expuesto, el test sólo permite contrastar la presencia, ante la hipótesis alternativa, de dos estructuras presentes en la muestra. Sin embargo, el contraste estándar puede extenderse al caso general en que puedan existir <em>m</em> estructuras diferentes en las observaciones muestrales. En esta ocasión, el estadístico de cambio estructural, con puntos de ruptura conocidos, tiene la forma expresión</p>
<p><span class="math display">\[CHOW = \frac{(SRCT - (SRC1 + SRC2 + ... + SRCm)/(m - 1)K}{(SRC1 + SRC2 + ... + SRCm)/(T - mK)}\]</span></p>
<p>y, bajo la hipótesis nula <span class="math inline">\(H_{0}:\{\mathbf{\beta}^{1} = \mathbf{\beta}^{2} = \ldots = \mathbf{\beta}^{m}\}\)</span>, sigue una distribución <span class="math inline">\(CHOW\sim F_{(m - 1)K,\ \ T - mK}\ \)</span>.</p></li>
<li><p>Se necesita conocer el punto de la muestra en el que se produce el cambio estructural. Esto significa que, en el caso de series temporales, se ha de saber la fecha exacta en la que hipotéticamente se produce el cambio en los parámetros de la regresión; o, de igual modo, en el caso de datos de corte transversal, el grupo de observaciones que puede poseer una estructura diferente al resto debe estar localizado <em>a priori</em>. Cuando no se conoce el punto de ruptura en el que se produce el cambio estructural, el contraste de Chow puede modificarse, dando lugar al <em>contraste de Quandt</em>. Este se basa en la aplicación recursiva del test de Chow, calculando los estadísticos <em>F</em> de cambio estructural en ‘todos’ los puntos posibles de la muestra (para que los estadísticos propuestos tengan buenas propiedades asintóticas, en la práctica sólo se calculan los estadísticos sobre un subconjunto reducido de datos, prescindiendo de un porcentaje de observaciones en los extremos de la muestra) y computando después un estadístico resumido de dichos valores. En concreto, si llamamos <span class="math inline">\(CHOW(\tau)\)</span> al test que contrasta la presencia de cambio estructural en la observación <span class="math inline">\(\tau\)</span>, el contraste de Quandt toma la expresión siguiente:</p>
<p><span class="math display">\[QLR = \max_{\tau_{1} \leq \tau \leq \tau_{2}}{CHOW(\tau)}\ \]</span></p>
<p>donde <span class="math inline">\(\tau_{1}\)</span> y <span class="math inline">\(\tau_{2}\)</span> representan las observaciones entre las que se sospecha se puede producir el cambio estructural (se eliminan, por tanto, las observaciones anteriores a <span class="math inline">\(\tau_{1}\)</span>, y la posteriores a <span class="math inline">\(\tau_{2}\)</span>, normalmente un 15% del total). Este estadístico no tiene una distribución estándar y, por tanto, deben utilizarse valores críticos (o <em>P</em>-valores) específicos.</p></li>
</ul>
</section>
<section id="consecuencias-de-la-presencia-de-cambio-estructural" class="level3" data-number="3.3.2">
<h3 data-number="3.3.2" class="anchored" data-anchor-id="consecuencias-de-la-presencia-de-cambio-estructural"><span class="header-section-number">3.3.2</span> Consecuencias de la presencia de cambio estructural</h3>
<p>Una vez visto cómo proceder para detectar la existencia de posibles cambios de estructura, cabe preguntarse cuáles son las consecuencias para el modelo de regresión de un resultado positivo en tal contraste estadístico.</p>
<p>En general, puede decirse que las consecuencias prácticas que origina la presencia de cambio estructural en un modelo de regresión son graves. Si se estima un modelo <span class="math inline">\(\mathbf{y} = \mathbf{X\beta} + \mathbf{e}\)</span>, cuando realmente existen dos (o más) estructuras diferentes, <span class="math inline">\(\mathbf{y}^{1} = \mathbf{X}^{1}\mathbf{\beta}^{1} + \mathbf{e}^{1}\)</span> y <span class="math inline">\(\mathbf{y}^{2} = \mathbf{X}^{2}\mathbf{\beta}^{2} + \mathbf{e}^{2}\)</span>, el estimador mínimo-cuadrático del vector <strong><em>β</em></strong> será una ‘mezcla’ de los correspondientes a cada submuestra y, por tanto, el estimador MCO de los parámetros estructurales será sesgado e inconsistente; del mismo modo, también estarán sesgadas las estimaciones de las varianzas y, por tanto, la inferencia estadística no será fiable, y, en general, se producirán errores en los contrastes de significación y en los intervalos de confianza.</p>
</section>
<section id="regresiones-con-parámetros-variables" class="level3" data-number="3.3.3">
<h3 data-number="3.3.3" class="anchored" data-anchor-id="regresiones-con-parámetros-variables"><span class="header-section-number">3.3.3</span> Regresiones con parámetros variables</h3>
<p>En la práctica econométrica, existen situaciones en que la hipótesis de que los parámetros son constantes a lo largo de todas las observaciones puede no ser válida. Así, para datos de corte transversal puede existir <em>heterogeneidad individual</em>, de forma que los parámetros cambien a través de las diferentes unidades de la sección cruzada. De igual forma, para datos de series temporales pueden presentarse variaciones sistemáticas de los parámetros a lo largo del tiempo. Finalmente, en el caso mixto, para datos de panel puede haber variaciones de los parámetros tanto a lo largo del tiempo como para las distintas unidades.</p>
<section id="modelos-con-variables-ficticias" class="level4" data-number="3.3.3.1">
<h4 data-number="3.3.3.1" class="anchored" data-anchor-id="modelos-con-variables-ficticias"><span class="header-section-number">3.3.3.1</span> Modelos con variables ficticias</h4>
<p>Se han propuesto en la literatura diversos modelos para abordar el problema de la inestabilidad paramétrica en situaciones como las descritas en el párrafo anterior. Uno de los más utilizados consiste en la introducción de las denominadas variables ficticias (también llamadas variables tipo “<em>dummy</em>”) en los modelos econométricos como una forma de modelizar el cambio estructural que puede suponer la pertenencia a un determinado grupo de individuos (sexo, nivel de estudios, <em>status</em> socio-económico, raza, etc.), de empresas (tamaño, sector económico, stock de capital, etc.), de regiones/países (riqueza, ubicación geográfica, capital tecnológico, etc.) o a un determinado período temporal (entrada en la UE, creación del euro, períodos de recesión, crisis petrolíferas o financieras, etc.).</p>
<p>Las variables ficticias permiten construir modelos econométricos en los que uno, varios, o todos los parámetros de la regresión varían en función de alguna característica o pertenencia de las observaciones a algún grupo concreto. En general, una variable ficticia de tipo binario, <em>D</em>, se define como sigue:</p>
<p><span class="math display">\[\ D = \left\{ \begin{matrix}
\ 1 &amp; \text{para\ las\ observaciones\ que\ cumplen\ cierta\ condición\ } \\
\ 0 &amp; \text{para\ las\ observaciones\ que\ no\ cumplen\ dicha\ condición\ } \\
\end{matrix} \right.\ \]</span></p>
<p>Sirvan como ejemplos los siguientes: <em>D<sub>i</sub></em> = 1 si el individuo observado es una mujer (0 si es un hombre); <em>D<sub>i</sub></em> = 1 si la empresa es grande (0 si es pequeña); o <em>D<sub>t</sub></em> = 1 si la observación pertenece a un cierto período de tiempo o es posterior a una cierta fecha (0 si es anterior). En todos los casos, el valor <em>D</em> = 0 define el <em>grupo de referencia</em>, también llamado <em>categoría base</em> de comparación.</p>
<p>Veamos cómo se pueden utilizar estas variables dicotómicas para modelizar el cambio estructural en los modelos de regresión.</p>
<p>Supongamos que nos encontramos con dos estructuras diferentes en un modelo de regresión: <span class="math inline">\(\ \mathbf{y}^{1} = \mathbf{X}^{1}\mathbf{\beta}^{1} + \mathbf{e}^{1}\ \)</span> y <span class="math inline">\(\ \mathbf{y}^{2} = \mathbf{X}^{2}\mathbf{\beta}^{2} + \mathbf{e}^{2}\)</span>. Se puede llegar a estos modelos parciales definiendo un modelo de regresión global con parámetros no constantes dado por</p>
<p><span class="math display">\[\mathbf{y} = {\mathbf{X}\mathbf{\beta}_{i} + \mathbf{e}}_{\mathbf{\ }}\]</span></p>
<p>donde</p>
<p><span class="math display">\[\mathbf{\beta}_{i} = {\mathbf{\beta} + \mathbf{\alpha}D_{i}}_{\mathbf{\ }}\]</span></p>
<p>con <span class="math inline">\(\mathbf{\alpha} = \left( \alpha_{1},\alpha_{2},\ldots,\alpha_{K} \right)^{'}\)</span>, y <em>D<sub>i</sub></em> = 0 para las observaciones de la primera estructura y <em>D<sub>i</sub></em> = 1 para las observaciones de la segunda estructura. Se tiene entonces que <span class="math inline">\(\ \mathbf{\beta}^{1} = \mathbf{\beta}_{\mathbf{\ }}\)</span>, si <em>D<sub>i</sub></em>=0, y <span class="math inline">\(\ \mathbf{\beta}^{2} = {\mathbf{\beta} + \mathbf{\alpha}}_{\mathbf{\ }}\)</span> si <em>D<sub>i</sub></em> = 1. Por tanto, pueden expresarse las dos estructuras mediante una única ecuación dada por</p>
<p><span class="math display">\[y_{t} = {\beta_{1} + \beta_{2}x_{2t} + \ldots + \beta_{K}x_{Kt} + \alpha_{1}D_{t} + \alpha_{2}x_{2t}D_{t} + \ldots + \alpha_{K}x_{Kt}D_{t} + e_{t}}_{\mathbf{\ }}\]</span></p>
<p>o, en forma matricial,</p>
<p><span class="math display">\[\mathbf{y} = {\mathbf{X\beta} + \mathbf{X}^{*}\mathbf{\alpha} + \mathbf{e}}_{\ }\]</span></p>
<p>donde la matriz <span class="math inline">\(\mathbf{X}^{*} = \left( \mathbf{X} \times D \right)\)</span> está compuesta por las que se conocen como <em>variables</em> <em>de interacción</em>, que son producto de los regresores originales <em>x</em>’s multiplicados por la ficticia <em>D</em>, y sus parámetros asociados, <em>α</em>’s, miden el cambio que se produce en las pendientes de la regresión al pasar de un grupo de observaciones al otro.</p>
<p>Para los distintos grupos de observaciones se tiene que:</p>
<p><span class="math display">\[\left\{ \begin{matrix}{\ y}_{t} = \beta_{1} + \beta_{2}x_{2t} + \ldots + \beta_{K}x_{Kt} + e_{t}\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ para\ D_{t} = 0 \\
{\ y}_{t} = {(\beta}_{1} + \alpha_{1}) + {(\beta}_{2} + \alpha_{2})x_{2t} + \ldots + {(\beta}_{K} + \alpha_{K})x_{Kt} + e_{t}\ \ \ \ \ \ \ para\ D_{t} = 1 \\
\end{matrix} \right.\ \]</span></p>
<p>El contraste de la hipótesis <span class="math inline">\(H_{0}:\left\{ \mathbf{\beta}^{1} = \mathbf{\beta}^{2} \right\}\)</span> es equivalente a la prueba de igualdad a cero de los parámetros <em>α</em>’s, <span class="math inline">\(H_{0}:\left\{ \mathbf{\alpha} = \mathbf{0} \right\}\)</span>. De hecho, se puede demostrar que el contraste <em>F</em> de la hipótesis <span class="math inline">\(H_{0}:\left\{ \alpha_{1} = \alpha_{2} = \ldots = \alpha_{K} = 0 \right\}\)</span> coincide numéricamente con el valor del estadístico de <em>CHOW</em> de presencia de cambio estructural.</p>
<p>En general, el modelo <span class="math inline">\(\mathbf{y} = \mathbf{X\beta} + \mathbf{X}^{*}\mathbf{\alpha} + \mathbf{e}\)</span> puede utilizarse para contrastar hipótesis intermedias de igualdad de cualquier subconjunto de parámetros individuales en subpoblaciones (ordenadas en el origen, pendientes, etc.) y, en todos los casos, la hipótesis de estabilidad paramétrica equivaldrá a un contraste <em>F</em> donde la hipótesis nula <em>H</em><sub>0</sub> sería la igualdad a cero de los correspondientes parámetros <em>α</em>’s.</p>
<p>Como ejemplo, consideremos la siguiente regresión transversal <span class="math inline">\(y_{i} = \beta_{1} + \beta_{2}x_{i} + e_{i}\)</span> que representa un modelo familiar de consumo de tipo Keynesiano, donde <em>y</em> denota el consumo total y <em>x</em> la renta disponible; en este caso particular, <span class="math inline">\(\beta_{1}\)</span> podría interpretarse como un “consumo autónomo” y <span class="math inline">\(\beta_{2}\)</span> será la propensión marginal al consumo respecto a la renta. Supongamos que se tiene la intuición de que el modelo de consumo de las familias que viven en un ambiente rural es distinto del modelo de comportamiento que se observa en las familias que residen en las ciudades. Para analizar si esta hipótesis es cierta, puede construirse una variable ficticia, <em>D</em>, que tome los siguientes valores:</p>
<p><span class="math display">\[D_{i} = \left\{ \begin{matrix}
\ 1 &amp; \text{si\ la\ familia\ }i\text{\ vive\ en\ una\ zona\ rural\ } \\
\ 0 &amp; \text{si\ la\ familia\ }i\text{\ vive\ en\ una\ zona\ urbana\ } \\
\end{matrix} \right.\ \]</span></p>
<p>y, a continuación, introducir dicha variable en la regresión original, con lo que se llega al modelo</p>
<p><span class="math display">\[y_{i} = \beta_{1} + \beta_{2}x_{i} + \alpha_{1}D_{i} + \alpha_{2}{x_{i}D}_{i} + e_{i}\]</span></p>
<p>Suponiendo que se rechaza la hipótesis nula de ausencia de cambio estructural, <span class="math inline">\(H_{0}:\left\{ \alpha_{1} = 0,\alpha_{2} = 0 \right\}\)</span>, pueden darse las situaciones siguientes: que se produce sólo un cambio en la propensión marginal al consumo, que varíe solo el consumo autónomo y, finalmente, que varíen ambos parámetros.</p>
</section>
<section id="el-caso-general-donde-los-parámetros-son-función-de-una-variable-conocida" class="level4" data-number="3.3.3.2">
<h4 data-number="3.3.3.2" class="anchored" data-anchor-id="el-caso-general-donde-los-parámetros-son-función-de-una-variable-conocida"><span class="header-section-number">3.3.3.2</span> El caso general donde los parámetros son función de una variable conocida</h4>
<p>Puede generalizarse la especificación anterior asumiendo un modelo lineal del tipo<span class="math inline">\(\mathbf{\ }y_{i} = {\beta_{1i} + \beta_{2i}x_{i} + e_{i}}_{\mathbf{\ }}\)</span>, donde los parámetros <em>β</em>’s dependen de alguna(s) variable(s) conocida(s), no necesariamente ficticia. Así, si se supone que <span class="math inline">\({\ \beta}_{1i} = {\beta_{1} + \alpha_{1}z_{i}}_{\mathbf{\ }}\)</span> y <span class="math inline">\({\ \beta}_{2i} = {\beta_{2} + \alpha_{2}z_{i}}_{\mathbf{\ }}\)</span> para alguna variable <em>z<sub>i</sub></em>, entonces, substituyendo estas expresiones en la ecuación de regresión inicial se llega a la expresión siguiente:</p>
<p><span class="math display">\[y_{i} = {\beta_{1} + \alpha_{1}z_{i} + \beta_{2}x_{i} + \alpha_{2}x_{i}z_{i} + e_{i}}_{\mathbf{\ }}\]</span></p>
<p>Cuando los parámetros <em>α<sub>j</sub></em> son iguales a cero en la regresión anterior, se obtiene el modelo clásico <span class="math inline">\(y_{i} = \beta_{1} + \beta_{2}x_{i} + e_{i}\)</span>. Por tanto, se puede contrastar fácilmente la hipótesis de constancia de los parámetros del modelo de regresión original mediante una prueba <em>F</em> de la hipótesis conjunta <span class="math inline">\(H_{0}:\left\{ \alpha_{1} = \alpha_{2} = 0 \right\}\)</span>.</p>
<p>Un caso particular de regresión en el que uno de los parámetros depende de una variable <em>z</em>, en concreto, de tipo ficticio, lo constituyen los llamados modelos de regresión lineal por tramos. Este tipo de modelos forman parte de una clase general de diseños, las regresiones discontinuas (RD), que se utilizan en el análisis de evaluación del impacto de programas o políticas sobre un indicador de resultados.</p>
<p>Supongamos que partimos de una especificación lineal <span class="math inline">\(y_{t} = \beta_{1} + \beta_{2t}x_{t} + e_{t}\)</span> en la que el parámetro de la pendiente no es constante, sino que se tiene que <span class="math inline">\(\beta_{2t} = \beta_{2} + \alpha_{2}D_{t}\)</span>, donde <em>D</em> es una variable ficticia definida por <span class="math inline">\(D_{t} = \left\{ \begin{matrix} 1\ \ si\ x_{t} \geq x^{*} \\ 0\ \ si\ x_{t} &lt; x^{*} \\ \end{matrix} \right.\ \)</span> . Entonces, el modelo de regresión</p>
<p><span class="math display">\[y_{t} = {\beta_{1} + \beta_{2}x_{t} + \alpha_{2}{\left( x_{t} - x^{*} \right)D}_{t} + e_{t}}_{\mathbf{\ }}\]</span></p>
<p>tomará la forma de una regresión lineal discontinua a tramos que “pivota” sobre el punto <span class="math inline">\(x^{*}\)</span>.</p>
<p>Otro ejemplo de modelos con parámetros dependientes de alguna variable es aquel en el que <em>z</em> representa alguna <em>variable de escala</em> de tipo cuantitativo. Por ejemplo, si se usan datos de corte transversal, la variable <em>z</em> puede describir el tamaño de la empresa, <span class="math inline">\({\ z}_{i} = tamaño_{i}\)</span> , medido a través del número de empleados o el volumen de activos. Para datos de series temporales, el caso más típico es aquel en que <span class="math inline">\({\ z}_{t} = t\ \)</span>, es decir, los parámetros evolucionan en el tiempo siguiendo una tendencia (creciente o decreciente); o el caso más general en el que en lugar de una recta se utiliza una parábola para describir la evolución temporal de los parámetros, <span class="math inline">\({\ \beta}_{ji} = {\beta_{j} + \alpha_{j}^{1}z_{1i} + \alpha_{j}^{2}z_{2i}}_{\mathbf{\ }}\)</span>, donde <span class="math inline">\({\ z}_{1t} = t_{\ }\)</span> y <span class="math inline">\({\ z}_{2t} = t_{\ }^{2\ }\)</span>.</p>
<p>Finalmente, también aparecen parámetros variables ante la presencia de <em>efectos estacionales</em> en modelos que usan datos de series temporales con frecuencia inferior a la anual (trimestres, meses o semanas). En estos casos, la especificación más común de estos efectos es aquella en la que la ordenada en el origen —pero no en las pendientes— varía de una “estación” a otra. Por ejemplo, para el caso de datos trimestrales, es frecuente proponer modelos de regresión del tipo <span class="math inline">\(y_{t} = \beta_{1t} + \beta_{2}x_{t} + e_{t}\)</span>, donde <span class="math inline">\(\ \beta_{1t} = {\beta_{1} + \alpha_{1}^{1}D_{1t} + \alpha_{1}^{2}D_{2t} + \alpha_{1}^{3}D_{3t}}_{\mathbf{\ }}\)</span>, siendo <span class="math inline">\(D_{jt} = 1\)</span> para el <em>j</em>-ésimo trimestre y 0 en el resto de los trimestres. Como puede observarse, al haber introducido el parámetro <em>β</em><sub>1</sub> asociado a la constante, en el modelo sólo deben aparecer tres de las cuatro posibles variables ficticias, tomando como referencia una de ellas, en nuestro ejemplo la variable <span class="math inline">\(D_{4t}\)</span> correspondiente al cuarto trimestre. Así se evita la llamada “trampa de las variables ficticias”, puesto que, si se introdujesen las cuatro variables ficticias, ello implicaría un problema de multicolinealidad perfecta, ya que por definición se cumple que <span class="math inline">\(D_{1t} + D_{2t} + D_{3t} + D_{4t} = 1\)</span>, es decir, son linealmente dependientes.</p>
</section>
</section>
</section>
<section id="distribución-de-los-errores-del-modelo" class="level2" data-number="3.4">
<h2 data-number="3.4" class="anchored" data-anchor-id="distribución-de-los-errores-del-modelo"><span class="header-section-number">3.4</span> Distribución de los errores del modelo</h2>
<section id="no-normalidad-de-las-perturbaciones" class="level3" data-number="3.4.1">
<h3 data-number="3.4.1" class="anchored" data-anchor-id="no-normalidad-de-las-perturbaciones"><span class="header-section-number">3.4.1</span> No normalidad de las perturbaciones</h3>
<p>Al obtener las propiedades del estimador MCO de <span class="math inline">\(\mathbf{\beta}\)</span> en el modelo de regresión<span class="math inline">\(\mathbf{\ }\mathbf{y} = \mathbf{X\beta} + \mathbf{e}\)</span>, <span class="math inline">\({\widehat{\mathbf{\beta}}}_{MCO}\mathbf{=}\left( \mathbf{X}^{'}\mathbf{X} \right)^{- 1}\mathbf{X}^{'}\mathbf{y}\)</span>, se demuestra que es lineal, insesgado y de mínima varianza entre los estimadores lineales e insesgados (teorema de Gauss-Markov), todo ello sin utilizar la hipótesis de normalidad del vector de errores <strong><em>e</em></strong>. Si se cumple además esta última propiedad, <span class="math inline">\(e_{\mathbf{i}}\sim N\)</span>, el estimador MCO es eficiente, es decir, el mejor de todos los estimadores insesgados, incluyendo los estimadores no lineales.</p>
<p>Además, para obtener la distribución de <span class="math inline">\({\widehat{\mathbf{\beta}}}_{MCO}\)</span> se necesita la propiedad de normalidad de la distribución de las perturbaciones, cumpliéndose entonces que <span class="math inline">\({\widehat{\mathbf{\beta}}}_{MCO}\sim N_{n}(\mathbf{\beta},\sigma^{2}\left( \mathbf{X}^{'}\mathbf{X} \right)^{\mathbf{-}\mathbf{1}}\mathbf{)}\)</span>. Este hecho permite realizar inferencias sobre el vector <span class="math inline">\(\mathbf{\beta}\)</span> y sus componentes <span class="math inline">\(\beta_{i},\ i = 1,2,\ldots,K\)</span> (obtención de intervalos de confianza, contraste <em>t</em> sobre los parámetros individuales, test <em>F</em> para la significación global del modelo, restricciones lineales, etc.).</p>
</section>
<section id="consecuencias-de-la-no-normalidad" class="level3" data-number="3.4.2">
<h3 data-number="3.4.2" class="anchored" data-anchor-id="consecuencias-de-la-no-normalidad"><span class="header-section-number">3.4.2</span> Consecuencias de la no normalidad</h3>
<p>Como corolario de los dos resultados anteriores, cuando el término de error <span class="math inline">\(\mathbf{e}\)</span> no se distribuye como una variable aleatoria normal, <span class="math inline">\(e_{\mathbf{i}} ≄ N\)</span>, el estimador MCO de <strong><em>β</em></strong> deja de ser eficiente y, además, no pueden en principio realizarse inferencias fiables por desconocerse la distribución exacta del estimador. Por tanto, los intervalos de confianza y los contrastes <em>t</em> y <em>F</em> dejan de ser válidos.</p>
</section>
<section id="el-test-de-jarque-bera-de-normalidad" class="level3" data-number="3.4.3">
<h3 data-number="3.4.3" class="anchored" data-anchor-id="el-test-de-jarque-bera-de-normalidad"><span class="header-section-number">3.4.3</span> El test de Jarque-Bera de normalidad</h3>
<p>Para saber si en un modelo de regresión se satisface la hipótesis de normalidad de los errores pueden utilizarse diferentes contrastes, entre los que podemos destacar, por su amplia difusión, la <em>prueba de Jarque-Bera</em>. La hipótesis que contrastar es</p>
<p><span class="math display">\[H_{0}:\left\{ e\sim N \right\}\ \ \ frente\ a\ \ \ H_{1}:\left\{ e\sim\ Ρ \right\}\]</span></p>
<p>siendo <span class="math inline">\(Ρ\)</span> la familia Pearson, una clase general de distribuciones probabilísticas continuas que incluye la Normal, Chi-cuadrado, Beta, Gamma, <em>t</em> de Student, <em>F</em> de Fisher-Snedecor, Pareto, etc.</p>
<p>El contraste se basa en el estadístico</p>
<p><span class="math display">\[JB = (n - K)\left( \frac{A^{2}}{6} + \frac{(C - 3)^{2}}{24} \right)\]</span></p>
<p>que se calcula utilizando los coeficientes de <em>apuntamiento</em> o asimetría (<em>A</em>) y <em>curtosis</em> o curvatura (<em>C</em>) de la distribución de residuos mínimo-cuadráticos. Si la distribución de los errores fuese exactamente una normal estándar, debería cumplirse que <em>A</em>=0 y <em>C</em>=3. El estadístico de Jarque-Bera sigue asintóticamente, si la hipótesis nula de normalidad es cierta, una distribución Chi-cuadrado, <span class="math inline">\(JB\overset{as}{\sim}\chi_{2}^{2}\ \)</span>.</p>
<p>Si el valor obtenido para el test de Jarque-Bera es significativo (si se rechaza <em>H</em><sub>0</sub>), los resultados de los contrastes <em>t</em> y <em>F</em> de la estimación MCO no son válidos en sentido estricto. Sin embargo, puede demostrarse que, bajo condiciones muy generales, se verifica que</p>
<p><span class="math display">\[\ \frac{{\widehat{\beta}}_{i} - \beta_{i}}{se({\widehat{\beta}}_{i})}\ \overset{as}{\sim}\ N(0,1)\]</span></p>
<p>razón por la cual, con un tamaño de muestra suficientemente grande, puede seguir utilizándose el estadístico <em>t</em>, aunque usando las tablas de la distribución normal estándar para contrastar hipótesis sobre los parámetros individuales. Por otra parte, en lugar del contraste <em>F</em>, puede utilizarse el test de Wald, que además es un test válido para contrastar cualquier tipo de restricciones, lineales y no lineales, cumpliéndose que <span class="math inline">\(W\ \overset{as}{\sim}\chi_{q}^{2}\ \)</span>, siendo <em>q</em> el número de restricciones.</p>
</section>
<section id="estimación-robusta-del-mrl" class="level3" data-number="3.4.4">
<h3 data-number="3.4.4" class="anchored" data-anchor-id="estimación-robusta-del-mrl"><span class="header-section-number">3.4.4</span> Estimación robusta del MRL</h3>
<p>Cuando la perturbación del MRL no se distribuye normalmente, el estimador MCO de los parámetros <strong><em>β</em></strong> del modelo es aún el mejor estimador lineal insesgado (MELI) y es consistente, pero no eficiente, ni siquiera asintóticamente. También se puede demostrar que el estimador propuesto para <em>σ</em>^2 es insesgado bajo la hipótesis alternativa de errores no normales, y además es consistente.</p>
<p>Por otro lado, del teorema central del límite se obtiene que el estimador MCO de <strong><em>β</em></strong> tiene una distribución asintóticamente normal y que los intervalos de confianza y los contrastes de hipótesis siguen siendo asintóticamente válidos aún si los errores no son normales. Ello significa que todos los resultados del modelo de regresión lineal general valen de forma aproximada para muestras de tamaño elevado sea cual sea la distribución del término de perturbación.</p>
<p>Ahora bien, las estimaciones obtenidas por el método de MCO pueden ser bastante ineficientes (y, por tanto, imprecisas y poco fiables) respecto a otros métodos de estimación.</p>
<p>Se ha criticado al método de MCO por ser excesivamente sensible a la presencia de observaciones atípicas en la muestra, que se estudiarán en un apartado posterior. Estas observaciones anormales pueden ser originadas, entre otras causas, por fenómenos climatológicos excepcionales, huelgas, sucesos políticos, crisis económicas o financieras, guerras, o simplemente por errores en la medición (incluso de transcripción) de las variables que aparecen en el modelo. Precisamente dichas desviaciones extremas suelen ser una de las causas más frecuentes del incumplimiento de la hipótesis de normalidad, ya que su alejamiento de los valores promedio hace que la distribución muestral de los residuos (los cuales serán grandes para esas observaciones) tenga colas más amplias y una varianza mayor (incluso infinita) de la que le correspondería si se tratase de una distribución normal.</p>
<p>Por todo ello, se han propuesto en la literatura econométrica métodos alternativos al de MCO que son más <em>robustos</em> frente a la hipótesis de normalidad, en general, y a la presencia de observaciones extremas, en particular. Estos métodos son capaces de modelizar los errores de una forma más adecuada que el de MCO, permitiendo colas más gruesas y varianzas mayores en las distribuciones de los residuos del modelo. Debe entenderse por “robustez” la propiedad de que estos estimadores funcionan mucho mejor que el de MCO en el caso de no normalidad y de modo similar a aquel en el caso de que la distribución de los errores sea normal.</p>
</section>
</section>
<section id="heteroscedasticidad" class="level2" data-number="3.5">
<h2 data-number="3.5" class="anchored" data-anchor-id="heteroscedasticidad"><span class="header-section-number">3.5</span> Heteroscedasticidad</h2>
<p>Decimos que en el modelo de regresión <span class="math inline">\(y_{i} = \beta_{1} + \beta_{2}x_{2i} + \ldots + \beta_{K}x_{Ki} + e_{i}\)</span> existe heteroscedasticidad cuando la varianza no es constante, sino que se cumple que <span class="math inline">\(Var(e_{i}) = \sigma_{i}^{2}\ \)</span>, siendo el resto de las hipótesis válidas.</p>
<section id="naturaleza-de-la-heteroscedasticidad" class="level3" data-number="3.5.1">
<h3 data-number="3.5.1" class="anchored" data-anchor-id="naturaleza-de-la-heteroscedasticidad"><span class="header-section-number">3.5.1</span> Naturaleza de la heteroscedasticidad</h3>
<p>La heteroscedasticidad aparece casi exclusivamente en el caso de datos de corte transversal, mientras que es poco común en regresiones de series temporales. Por ejemplo, frecuentemente aparecen regresiones con errores heteroscedásticos cuando se estiman funciones de consumo para individuos o familias, ecuaciones de crecimiento entre países o regiones, ecuaciones de productividad entre empresas o industrias, funciones de demanda de horas trabajadas por los empleados de una empresa, etc.</p>
<p>También puede generarse heteroscedasticidad, sobre todo en el caso de datos de series temporales, por una especificación errónea del modelo o debido a la presencia de cambio estructural. Así, la omisión de una variable explicativa relevante hace que el efecto de esta sea asumido por el error y, por tanto, que este varíe a lo largo de la muestra.</p>
</section>
<section id="consecuencias-de-la-heteroscedasticidad" class="level3" data-number="3.5.2">
<h3 data-number="3.5.2" class="anchored" data-anchor-id="consecuencias-de-la-heteroscedasticidad"><span class="header-section-number">3.5.2</span> Consecuencias de la heteroscedasticidad</h3>
<p>En presencia de heteroscedasticidad, pueden establecerse las siguientes conclusiones respecto a la validez del estimador MCO:</p>
<ul>
<li><p><span class="math inline">\({\widehat{\mathbf{\beta}}}_{MCO}\ \)</span> mantiene las propiedades de insesgadez, consistencia y normalidad asintótica.</p></li>
<li><p>La presencia de heteroscedasticidad sesga las estimaciones estándar de las varianzas y covarianzas del estimador MCO.</p></li>
<li><p>Son inválidos los resultados inferenciales obtenidos al usar los estadísticos <em>t</em> y <em>F</em>** de la estimación MCO si no se corrige adecuadamente la matriz de varianzas-covarianzas.</p></li>
<li><p>Aún corrigiendo adecuadamente la matriz de covarianzas, los estimadores MCO de los parámetros no son eficientes.</p></li>
</ul>
</section>
<section id="detección-de-la-heteroscedasticidad" class="level3" data-number="3.5.3">
<h3 data-number="3.5.3" class="anchored" data-anchor-id="detección-de-la-heteroscedasticidad"><span class="header-section-number">3.5.3</span> Detección de la heteroscedasticidad</h3>
<p>Para detectar la presencia de heteroscedasticidad en los errores, se somete el modelo original a diferentes contrastes bajo la hipótesis nula de homoscedasticidad <span class="math inline">\(H_{0}:\{\sigma_{1}^{2} = \sigma_{2}^{2} = \ldots = \sigma_{n}^{2}\}\)</span>, frente a una alternativa específica que depende del procedimiento de estimación considerado y del patrón de comportamiento asumido para las varianzas de los errores.</p>
<section id="examen-gráfico-de-los-residuos-mco" class="level4" data-number="3.5.3.1">
<h4 data-number="3.5.3.1" class="anchored" data-anchor-id="examen-gráfico-de-los-residuos-mco"><span class="header-section-number">3.5.3.1</span> Examen gráfico de los residuos MCO</h4>
<p>Una primera aproximación para detectar si existe algún tipo de heteroscedasticidad consiste en observar el gráfico del valor absoluto, <span class="math inline">\(\left| {\widehat{e}}_{i} \right|\)</span>, o de los cuadrados, <span class="math inline">\({\widehat{e}}_{i}^{2}\)</span>, de los residuos MCO respecto a las variables explicativas del modelo o a otras variables externas al modelo que se crea puedan ser la causa de la heteroscedasticidad. La identificación de modelos sistemáticos en dichas gráficas puede verse como una primera evidencia de la violación de la hipótesis de varianza constante.</p>
</section>
<section id="contraste-general-de-white" class="level4" data-number="3.5.3.2">
<h4 data-number="3.5.3.2" class="anchored" data-anchor-id="contraste-general-de-white"><span class="header-section-number">3.5.3.2</span> Contraste general de White</h4>
<p>El <em>test de White</em> tiene un carácter muy general, ya que</p>
<p><span class="math display">\[H_{0}:\{\sigma_{i}^{2} = \sigma^{2}\ \ \forall i\}\ \ frente\ a\ \ H_{1}:\{\sigma_{i}^{2} \neq \sigma_{j}^{2}\}\]</span></p>
<p>siendo básicamente una prueba de mala especificación del modelo de regresión. En una primera etapa, se realiza la regresión auxiliar de <span class="math inline">\({\widehat{e}}_{i}^{2}\)</span> frente a las variables originales, sus cuadrados y sus productos cruzados</p>
<p><span class="math display">\[ê_{i}^{2} = \alpha_{0} + \sum_{j}^{}{\alpha_{j}x_{ji}} + \sum_{k}^{}{\alpha_{k}x_{ki}^{2}} + \sum_{l,m}^{}{\alpha_{lm}x_{li}x_{mi}} + u_{i}\]</span></p>
<p>En segundo lugar, se calcula el estadístico</p>
<p><span class="math display">\[W = nR^{2}\]</span></p>
<p>siendo <em>R<sup>2</sup></em> el coeficiente de determinación de esta regresión auxiliar. Se puede demostrar que, bajo la hipótesis nula <em>H</em><sub>0</sub>, se cumple que <span class="math inline">\(W\overset{as}{\sim}\chi_{p}^{2}\ \)</span>, donde <em>p</em> es el número de variables explicativas de esta regresión auxiliar, sin incluir la constante.</p>
<p>Por ejemplo, si el modelo propuesto es <span class="math inline">\(y_{i} = \beta_{1} + \beta_{2}x_{i} + \beta_{3}z_{i} + e_{i}\)</span>, y denotamos por <span class="math inline">\({\widehat{e}}_{i}\)</span> a los residuos de su estimación MCO, la regresión auxiliar que debe utilizarse para efectuar el contraste de White viene dada por <span class="math inline">\(ê_{i}^{2} = \alpha_{1} + \alpha_{2}x_{i} + \alpha_{3}z_{i} + \alpha_{4}x_{i}^{2} + \alpha_{5}z_{i}^{2} + \alpha_{6}x_{i}z_{i} + u_{i}\)</span>, debiendo compararse el valor del estadístico <span class="math inline">\(W = nR^{2}\)</span> de esta regresión con el valor crítico de una variable<span class="math inline">\(\chi_{5}^{2}\)</span>.</p>
<p>El problema de este test es su generalidad, ya que, si el estadístico de contraste <em>W</em> resulta significativo, es decir, aceptamos la hipótesis de heteroscedasticidad, no tenemos ninguna información sobre su naturaleza. El contraste puede revelar heteroscedasticidad, pero también puede estar detectando alguna clase de error de especificación en el modelo (omisión de alguna variable relevante, cambio estructural, etc.).</p>
</section>
<section id="contrastes-de-breusch-pagan-harvey-y-glejser" class="level4" data-number="3.5.3.3">
<h4 data-number="3.5.3.3" class="anchored" data-anchor-id="contrastes-de-breusch-pagan-harvey-y-glejser"><span class="header-section-number">3.5.3.3</span> Contrastes de Breusch-Pagan, Harvey y Glejser</h4>
<p>El <em>test de Breusch-Pagan</em> está diseñado para contrastar la hipótesis</p>
<p><span class="math display">\[\ H_{0}:\left\{ \sigma_{i}^{2} = \sigma^{2}\ \ \forall i \right\}\ \ \ frente\ a\ \ \ \ H_{1}:\left\{ \sigma_{i}^{2} = f(\alpha_{0} + \alpha_{1}z_{1i} + \cdots + \alpha_{p}z_{pi}) \right\}\ \]</span></p>
<p>donde la función <em>f</em> es desconocida y los regresores <em>z</em> son variables exógenas que pueden ser la causa de la heteroscedasticidad. El contraste se lleva a cabo mediante los siguientes pasos:</p>
<ul>
<li><p>Se estima el modelo original por MCO y se calculan los errores estimados al cuadrado ‘escalados’, <span class="math inline">\(\frac{{\widehat{e}}_{i}^{2}}{{\widetilde{\sigma}}^{2}}\)</span>, donde <span class="math inline">\({\widetilde{\sigma}}^{2} = \sum_{i = 1}^{n}\frac{{\widehat{e}}_{i}^{2}}{n}\)</span> es la estimación MV de la varianza residual.</p></li>
<li><p>Se estima la regresión auxiliar</p></li>
</ul>
<p><span class="math display">\[\ \frac{{\widehat{e}}_{i}^{2}}{{\widetilde{\sigma}}^{2}} = \alpha_{0} + \alpha_{1}z_{1i} + \cdots + \alpha_{p}z_{pi} + u_{i}\ \]</span></p>
<p>y se calcula el valor del estadístico<span class="math inline">\(\ BP = \ \frac{SCE}{2}\ \)</span>, siendo SCE la suma de cuadrados explicada de dicha regresión.</p>
<ul>
<li>Si no existe heteroscedasticidad del tipo planteado, se tiene que <span class="math inline">\(\ BP\overset{as}{\sim}\chi_{p}^{2}\ \)</span>, donde <em>p</em> es el número de regresores incluidos en la regresión auxiliar, sin incluir el término constante.</li>
</ul>
<p>También suele utilizarse una versión alternativa del contraste anterior, en la que se estima la regresión auxiliar <span class="math inline">\({\widehat{e}}_{i}^{2} = \alpha_{0} + \alpha_{1}z_{1i} + \cdots + \alpha_{p}z_{pi} + u_{i}\)</span> y, a continuación, se calcula el estadístico <span class="math inline">\(nR^{2}\)</span>, que tiene de nuevo una distribución asintótica <span class="math inline">\(\chi_{p}^{2}\)</span> .</p>
<p>Cuando se estima la regresión auxiliar <span class="math inline">\(\ \log{\widehat{e}}_{i}^{2} = \alpha_{0} + \alpha_{1}z_{1i} + \cdots + \alpha_{p}z_{pi} + u_{i}\ \)</span> y se calcula el estadístico <span class="math inline">\(H = nR^{2}\ \)</span>, al contraste resultante se le denomina <em>test de Harvey</em> de heteroscedasticidad multiplicativa, pues dicho contraste va asociado a la hipótesis alternativa <span class="math inline">\(H_{1}:\left\{ \sigma_{i}^{2} = e^{\alpha_{0} + \alpha_{1}z_{1i} + \cdots + \alpha_{p}z_{pi}} \right\}\)</span>. Para este contraste, de nuevo se tiene que bajo la hipótesis nula <span class="math inline">\(H\overset{as}{\sim}\chi_{p}^{2}\ \)</span> .</p>
<p>De igual manera, si se estima la regresión auxiliar <span class="math inline">\(\ \left| {\widehat{e}}_{i} \right| = \alpha_{0} + \alpha_{1}z_{1i} + \cdots + \alpha_{p}z_{pi} + u_{i}\ \)</span> y, a continuación, se calcula el estadístico <span class="math inline">\(G = nR^{2}\)</span> , comparando dicho valor con una distribución <span class="math inline">\(\chi_{p}^{2}\)</span>, a dicho contraste se le conoce en la literatura como <em>test de Glejser</em>, siendo en este caso la hipótesis alternativa la siguiente <span class="math inline">\(H_{1}:\left\{ \sigma_{i} = \alpha_{0} + \alpha_{1}z_{1i} + \cdots + \alpha_{p}z_{pi} \right\}\)</span>.</p>
</section>
<section id="otras-formas-de-heteroscedasticidad-modelos-de-volatilidad-variable-en-el-tiempo" class="level4" data-number="3.5.3.4">
<h4 data-number="3.5.3.4" class="anchored" data-anchor-id="otras-formas-de-heteroscedasticidad-modelos-de-volatilidad-variable-en-el-tiempo"><span class="header-section-number">3.5.3.4</span> Otras formas de heteroscedasticidad: modelos de volatilidad variable en el tiempo</h4>
<p>Cuando se trabaja con datos de corte transversal, la mayor preocupación (en lo que a incumplimiento de las hipótesis clásicas se refiere) se centra en el problema de la heteroscedasticidad, mientras que para datos de series temporales normalmente se analiza en profundidad la verificación de la hipótesis de ausencia de correlación serial en los residuos.</p>
<p>Sin embargo, existen modelos de series temporales, sobre todo en el ámbito de la economía financiera, en los que la varianza (condicional) no es estable en el tiempo. Concretamente, existe una forma especial de heteroscedasticidad en la que la varianza del error de predicción depende del tamaño de la perturbación precedente. A este tipo de modelos se les denomina procesos de <em>heteroscedasticidad autorregresiva condicional (ARCH)</em>.</p>
<p>La versión más simple de un proceso <em>ARCH</em> en un modelo de regresión lineal es aquella en la que los residuos <em>e<sub>t</sub></em> siguen la forma autorregresiva de primer orden del tipo</p>
<p><span class="math display">\[{e_{t} = {u_{t}\left\lbrack \alpha_{0} + \alpha_{1}e_{t - 1}^{2} \right\rbrack}^{1/2}}_{\ }\]</span></p>
<p>donde los errores <span class="math inline">\(u_{t}\)</span> siguen una distribución normal <span class="math inline">\(N(0,1)\)</span> y son independientes entre sí. Otra forma de escribir dicha ecuación es</p>
<p><span class="math display">\[{h_{t} = \alpha_{0} + \alpha_{1}e_{t - 1}^{2}}_{\ }\]</span></p>
<p>siendo <span class="math inline">\({h_{t} = Var\left\lbrack e_{t}|e_{t - 1} \right\rbrack}_{\ }\)</span> la varianza condicional de los errores.</p>
<p>En este modelo se cumple que <span class="math inline">\(E\left\lbrack e_{t}|e_{t - 1} \right\rbrack = 0\)</span> y, por tanto, <span class="math inline">\(E\left\lbrack e_{t} \right\rbrack = 0\)</span> y <span class="math inline">\(E\left\lbrack y_{t} \right\rbrack = {\mathbf{x}^{\mathbf{'}}}_{t}\mathbf{\beta}\)</span>, como en el modelo clásico. Ahora bien, <span class="math inline">\(Var\left\lbrack e_{t}|e_{t - 1} \right\rbrack = \alpha_{0} + \alpha_{1}e_{t - 1}^{2}\)</span>, es decir, condicionada sobre <span class="math inline">\(e_{t - 1}\)</span>, el término de error <span class="math inline">\(e_{t}\)</span> es heteroscedástico, aunque la varianza incondicional de <span class="math inline">\(e_{t}\)</span> es <span class="math inline">\(Var\left\lbrack e_{t} \right\rbrack = \frac{\alpha_{0}}{(1 - \alpha_{1})}\)</span> y, por consiguiente, el término de error es (incondicionalmente) homoscedástico.</p>
<p>Entonces, el modelo cumple todas las hipótesis clásicas del MRL por lo que el estimador MCO sigue siendo el mejor estimador lineal e insesgado de <em>β</em> (MELI). Sin embargo, al ser los errores condicionalmente heteroscedásticos, existe un estimador no lineal más eficiente que el de MCO, que es el estimador de máxima verosimilitud (MV), el cual utiliza la información contenida en la ecuación de la varianza condicional.</p>
<p>Una extensión inmediata del modelo ARCH de primer orden expuesto viene dada por los modelos <em>ARCH</em>(<em>q</em>), en los que <span class="math inline">\({h_{t} = Var\left\lbrack e_{t}|e_{t - 1},e_{t - 2},\ldots,e_{t - q} \right\rbrack}_{\ }\)</span> viene dada por la expresión</p>
<p><span class="math display">\[{h_{t} = \alpha_{0} + \alpha_{1}e_{t - 1}^{2} + \alpha_{2}e_{t - 2}^{2} + \ldots + \alpha_{q}e_{t - q}^{2}}_{\ }\]</span></p>
<p>También se han propuesto en la literatura otras ampliaciones, como los modelos ARCH generalizados (GARCH). Así, un caso sencillo viene dado por un modelo <em>GARCH</em>(1,1), cuya forma es</p>
<p><span class="math display">\[{h_{t} = \alpha_{0} + \alpha_{1}e_{t - 1}^{2} + \delta_{1}h_{t - 1}}_{\ }\]</span></p>
<p>A su vez, estos modelos pueden generalizarse, dando lugar a los modelos <em>GARCH</em>(<em>p</em>,<em>q</em>), con términos de orden superior en las dos dimensiones. Adicionalmente, se presentan otras posibilidades, como los modelos EGARCH, T-ARCH, GARCH-M, etc.</p>
<p>Existe una sencilla prueba de multiplicadores de Lagrange, propuesta por Engle, para contrastar la presencia de modelos ARCH en los errores de un modelo de regresión. Para el caso de un modelo <em>ARCH</em>(1) –la extensión para el caso general resulta inmediata- el <em>test de Engle</em> consiste en estimar la ecuación original <span class="math inline">\(y_{t} = \beta_{1} + \beta_{2}x_{2t} + \cdots + \beta_{K}x_{Kt} + e_{t}\)</span> por MCO, obtener los residuos <span class="math inline">\({\widehat{e}}_{t}\)</span> y estimar la regresión auxiliar siguiente</p>
<p><span class="math display">\[{{\widehat{e}}_{t}^{2} = \alpha_{0} + \alpha_{1}{\widehat{e}}_{t - 1}^{2} + \nu_{t}}_{\ }\]</span></p>
<p>construyéndose a continuación el estadístico</p>
<p><span class="math display">\[E = TR^{2}\]</span></p>
<p>con el <em>R<sup>2</sup></em> de esta última regresión. Este estadístico, bajo la hipótesis nula de homoscedasticidad condicional <span class="math inline">\({H_{0}:\left\{ h_{t} = \alpha_{0} \right\}}_{\ }\)</span> se distribuye asintóticamente como una variable Chi-cuadrado, <span class="math inline">\(E\overset{as}{\sim}\chi_{1}^{2}\ \)</span>.</p>
<p>Si el contraste resulta significativo (aceptándose la hipótesis alternativa <span class="math inline">\({H_{1}:\left\{ h_{t} = \alpha_{0} + \alpha_{1}e_{t - 1}^{2} \right\}}_{\ }\)</span>), se puede seguir utilizando el estimador MCO, pero habrá que tener en cuenta que existe otro estimador, el de MV, que es más eficiente y, por tanto, dará lugar a resultados más fiables que el de mínimos cuadrados ordinarios (estimaciones más precisas, intervalos de predicción más estrechos, etc.).</p>
</section>
</section>
<section id="regresiones-heteroscedásticas" class="level3" data-number="3.5.4">
<h3 data-number="3.5.4" class="anchored" data-anchor-id="regresiones-heteroscedásticas"><span class="header-section-number">3.5.4</span> Regresiones heteroscedásticas</h3>
<p>Usando datos de corte transversal, supongamos que en el modelo de regresión <span class="math inline">\(y_{i} = \beta_{1} + \beta_{2}x_{2i} + \ldots + \beta_{K}x_{Ki} + e_{i}\)</span> existe heteroscedasticidad, es decir, <span class="math inline">\(Var\left( e_{i} \right) = \sigma_{i}^{2}\)</span>, siendo el resto de las hipótesis válidas.</p>
<section id="el-estimador-de-mínimos-cuadrados-corregidos-método-de-white" class="level4" data-number="3.5.4.1">
<h4 data-number="3.5.4.1" class="anchored" data-anchor-id="el-estimador-de-mínimos-cuadrados-corregidos-método-de-white"><span class="header-section-number">3.5.4.1</span> El estimador de mínimos cuadrados corregidos (método de White)</h4>
<p>Como una de las posibles soluciones al problema de la presencia de heteroscedasticidad en los errores del modelo, White (1980) propuso tomar el estimador de <em>MCO corregidos</em>, ajustando la matriz de covarianzas estimada para el vector de parámetros <strong><em>β</em></strong>.</p>
<p>White demostró que, bajo condiciones muy generales, si existe heteroscedasticidad, la matriz de covarianzas correcta del estimador MCO viene dada por <span class="math inline">\(Cov(\widehat{\mathbf{\beta}}) = (\mathbf{X}^{'}\mathbf{X})^{- 1}\left( \mathbf{X}^{'}\Omega\mathbf{X} \right)(\mathbf{X}^{'}\mathbf{X})^{- 1}\)</span>, donde <span class="math inline">\(\Omega = Cov(e)\)</span>, y que ésta puede estimarse de forma consistente por la ‘matriz <em>sandwich</em>’ siguiente:</p>
<p><span class="math display">\[C\widehat{o}v(\widehat{\mathbf{\beta}})_{W} = (\mathbf{X}^{'}\mathbf{X})^{- 1}\mathbf{S}_{0}(\mathbf{X}^{'}\mathbf{X})^{- 1}\]</span></p>
<p>donde</p>
<p><span class="math display">\[\mathbf{S}_{0} = \frac{n}{n - K}\sum_{i = 1}^{n}{ê_{i}^{2}\mathbf{x}_{i}\mathbf{x}_{i}^{'}}\]</span></p>
<p>y <em>ê<sub>i</sub></em> son los residuos MCO y <em><strong>x</strong><sub>i</sub></em> el vector de la <em>i</em>-ésima fila de la matriz <strong><em>X</em></strong>.</p>
</section>
<section id="el-estimador-de-mínimos-cuadrados-ponderados" class="level4" data-number="3.5.4.2">
<h4 data-number="3.5.4.2" class="anchored" data-anchor-id="el-estimador-de-mínimos-cuadrados-ponderados"><span class="header-section-number">3.5.4.2</span> El estimador de mínimos cuadrados ponderados</h4>
<p>El estimador de <em>mínimos cuadrados ponderados (MCP)</em> se obtiene multiplicando cada una de las observaciones del modelo original por la inversa de las estimaciones de las desviaciones típicas, es decir, “ponderando” cada observación por el valor <span class="math inline">\(p_{i} = 1/{\widehat{\sigma}}_{i}\)</span> y aplicando MCO al modelo transformado:</p>
<p><span class="math display">\[y_{i}/{\widehat{\sigma}}_{i} = \beta_{1}(1/{\widehat{\sigma}}_{i}) + \beta_{2}(x_{2i}/{\widehat{\sigma}}_{i}) + ... + \beta_{K}(x_{Ki}/{\widehat{\sigma}}_{i}) + e_{i}^{*}\]</span></p>
<p>Se puede demostrar que, si las perturbaciones del modelo original son normales, el estimador MCP de los parámetros estructurales, <span class="math inline">\({\widehat{\mathbf{\beta}}}_{MCP}\)</span>, es insesgado, consistente y eficiente.</p>
<p>En general, el punto crucial en la aplicación del método MCP es obtener una estimación de la desviación estándar <span class="math inline">\({\widehat{\sigma}}_{i}\)</span> para calcular las ponderaciones <span class="math inline">\(p_{i} = 1/{\widehat{\sigma}}_{i}\)</span>.</p>
<p>Veamos cómo se pueden estimar dichos errores estándar en el caso de asumir una forma especial de heteroscedasticidad. Supongamos que la varianza toma la forma exponencial <span class="math inline">\({\ \sigma_{i}^{2} = e^{\alpha_{o} + \alpha_{1}z_{1i} + \ldots + \alpha_{p}z_{pi}}}_{\ }\)</span>. Esta expresión tiene como caso especialel modelo de <em>heteroscedasticidad multiplicativa</em> <span class="math inline">\(\sigma_{i}^{2} = \sigma^{2}\left( w_{1i} \right)^{\alpha_{1}}\left( w_{2i} \right)^{\alpha_{2}}\cdots\left( w_{pi} \right)^{\alpha_{p}}\)</span>, donde <span class="math inline">\(\sigma^{2} = e^{\alpha_{0}}\)</span> y <span class="math inline">\(z = \log(w)\)</span>, es decir, cuando los regresores <em>z</em>’s vienen expresados en logaritmos.</p>
<p>Para este caso especial, en el que se tiene que <span class="math inline">\({\ \sigma_{i}^{2} = e^{\alpha_{o} + \alpha_{1}z_{1i} + \ldots + \alpha_{p}z_{pi}}}_{\ }\)</span>, se realiza en primer lugar la regresión auxiliar</p>
<p><span class="math display">\[log\ {\widehat{e}}_{i}^{2} = \alpha_{0} + \alpha_{1}z_{1i} + ... + \alpha_{p}z_{pi} + u_{i}\]</span></p>
<p>y, dado que se puede demostrar que el estimador MCO del parámetro <em>α</em><sub>0</sub> está sesgado, se utilizan los valores <span class="math inline">\(e^{{\widehat{\alpha}}_{1}z_{1i} + ... + {\widehat{\alpha}}_{p}z_{pi}}\)</span> para estimar las varianzas <span class="math inline">\(\sigma_{i}^{2}\)</span>.</p>
<p>Entonces, <span class="math inline">\({\widehat{\sigma}}_{i} = \sqrt{e^{{\widehat{\alpha}}_{1}z_{1i} + ... + {\widehat{\alpha}}_{p}z_{pi}}}\)</span> y por tanto, se toman los valores <span class="math inline">\(p_{i} = 1/\sqrt{e^{{\widehat{\alpha}}_{1}z_{1i} + ... + {\widehat{\alpha}}_{p}z_{pi}}}\)</span> como ponderaciones de las variables originales.</p>
</section>
<section id="el-estimador-de-máxima-verosimilitud-para-modelos-de-regresión-con-volatilidad-variable-en-el-tiempo" class="level4" data-number="3.5.4.3">
<h4 data-number="3.5.4.3" class="anchored" data-anchor-id="el-estimador-de-máxima-verosimilitud-para-modelos-de-regresión-con-volatilidad-variable-en-el-tiempo"><span class="header-section-number">3.5.4.3</span> El estimador de máxima verosimilitud para modelos de regresión con volatilidad variable en el tiempo</h4>
<p>Cuando se trabaja con datos de series temporales, si se parte de un modelo de regresión lineal <span class="math inline">\(y_{t} = \beta_{1} + \beta_{2}x_{2t} + \cdots + \beta_{K}x_{Kt} + e_{t}\)</span> en el que la varianza condicional de los residuos <em>e<sub>t</sub></em> no es constante</p>
<p><span class="math display">\[{h_{t} = Var\left\lbrack e_{t}|e_{t - 1},e_{t - 2},\ldots,e_{t - q} \right\rbrack \neq cte.}_{\ }\]</span></p>
<p>puede proponerse una estructura <em>ARCH</em> del tipo</p>
<p><span class="math display">\[{h_{t} = \alpha_{0} + \alpha_{1}e_{t - 1}^{2} + \alpha_{2}e_{t - 2}^{2} + \ldots + \alpha_{q}e_{t - q}^{2}}_{\ }\]</span></p>
<p>o una <em>forma GARCH</em> más general que incluiría términos autorregresivos para la varianza condicional</p>
<p><span class="math display">\[{h_{t} = \alpha_{0} + \alpha_{1}e_{t - 1}^{2} + \alpha_{2}e_{t - 2}^{2} + \ldots + \alpha_{q}e_{t - q}^{2} + \delta_{1}h_{t - 1} + \delta_{2}h_{t - 2} + \ldots + \delta_{p}h_{t - p}}_{\ }\]</span></p>
<p>En estos casos, debe utilizarse el estimador de máxima verosimilitud (MV) en lugar del estimador MCO, puesto que será un estimador eficiente, aparte de ser consistente y con una distribución asintóticamente normal.</p>
<p>El método de máxima verosimilitud toma como criterio de optimización maximizar el logaritmo de la función de verosimilitud asociada al modelo de regresión, que para los modelos con efectos ARCH o GARCH toma la expresión</p>
<p><span class="math display">\[L(\mathbf{\beta},\mathbf{\alpha},\mathbf{\delta}) = f(y_{1},y_{2},\ldots,y_{T}) = f(y_{1})f(y_{1}|y_{2})f(y_{1}|y_{2},y_{3})\ldots f(y_{1}|y_{2},y_{3},\ldots,y_{T})\]</span></p>
<p>Entonces, tomando logaritmos en la expresión anterior, la función a optimizar es la siguiente:</p>
<p><span class="math display">\[\log L(\mathbf{\beta},\mathbf{\alpha},\mathbf{\delta}) = \sum_{t = 1}^{T}{\log f(y_{t}|Y_{t - 1})}\]</span></p>
<p>donde la función <em>f</em> será la función de densidad asociada a una distribución <span class="math inline">\(N(0,1)\)</span>.</p>
</section>
</section>
</section>
<section id="autocorrelación" class="level2" data-number="3.6">
<h2 data-number="3.6" class="anchored" data-anchor-id="autocorrelación"><span class="header-section-number">3.6</span> Autocorrelación</h2>
<p>Decimos que en el modelo de regresión <span class="math inline">\(y_{t} = \beta_{1} + \beta_{2}x_{2t} + \ldots + \beta_{K}x_{Kt} + e_{t}\)</span> existe <em>autocorrelación</em> cuando existen covarianzas no nulas entre los errores, es decir, <span class="math inline">\(Cov(e_{t},e_{s}) = \sigma_{ts} \neq 0\ \)</span>.</p>
<p>Los procesos que habitualmente describen el comportamiento de errores con correlación serial en el tiempo son los denominados modelos <em>autorregresivos de media móvil</em>, <em>ARMA</em>(<em>p</em>,<em>q</em>), que incluyen los modelos autorregresivos, <em>AR</em>(<em>p</em>), y los de medias móviles, <em>MA</em>(<em>q</em>), como casos particulares.</p>
<p>La forma general de un modelo autorregresivo de orden <em>p</em>, <em>AR</em>(<em>p</em>), es</p>
<p><span class="math display">\[e_{t} = \rho_{1}e_{t - 1} + \rho_{2}e_{t - 2} + \ldots + \rho_{p}e_{t - p} + \varepsilon_{t}\]</span></p>
<p>con <span class="math inline">\(\varepsilon_{t} \approx N(0,\sigma_{\varepsilon}^{2})\)</span>, cumpliéndose que <span class="math inline">\(Cov(\mathbf{\varepsilon}) = \sigma_{\varepsilon}^{2}\mathbf{I}\)</span>. Los casos más tratados en la literatura econométrica son los modelos <em>AR</em>(1) y <em>AR</em>(2).</p>
<p>Por otro lado, los modelos de medias móviles de orden <em>q</em>, <em>MA</em>(<em>q</em>), toman la expresión general</p>
<p><span class="math display">\[e_{t} = \varepsilon_{t} - \theta_{1}\varepsilon_{t - 1} - \theta_{2}\varepsilon_{t - 2} - \ldots - \theta_{q}\varepsilon_{t - q}\]</span></p>
<p>siendo las más utilizadas las formas <em>MA</em>(1) y <em>MA</em>(2).</p>
<p>En este apartado nos centraremos únicamente en el análisis de procesos <em>AR</em>(1), <span class="math inline">\(e_{t} = \rho e_{t - 1} + \varepsilon_{t}\)</span>, pero muchos de los resultados son fácilmente extrapolables al caso general de modelos <em>AR</em>(<em>p</em>); para los modelos <em>MA</em>(<em>q</em>), las modificaciones son mayores, sobre todo en lo referente a la estimación.</p>
<section id="naturaleza-y-consecuencias-de-la-autocorrelación" class="level3" data-number="3.6.1">
<h3 data-number="3.6.1" class="anchored" data-anchor-id="naturaleza-y-consecuencias-de-la-autocorrelación"><span class="header-section-number">3.6.1</span> Naturaleza y consecuencias de la autocorrelación</h3>
<p>La existencia de autocorrelación en los errores es un problema frecuente con datos de series temporales. Viene originada, en muchas ocasiones, por una mala especificación de la dinámica temporal de interacción entre la variable dependiente <em>y</em> y los regresores <em>x</em>’s, aunque también puede deberse a situaciones como las siguientes:</p>
<ul>
<li><p>Naturaleza inercial del comportamiento de los agentes económicos.</p></li>
<li><p>Especificación errónea del modelo (variables omitidas, por ejemplo) o errores sistemáticos de medida en alguna(s) de las variables del modelo.</p></li>
<li><p>Semejanzas de comportamiento en el espacio.</p></li>
<li><p>‘Manipulación’ de datos (agregación, desestacionalización, eliminación de la tendencia, etc.).</p></li>
</ul>
<p>Las consecuencias de utilizar el estimador MCO de <strong><em>β</em></strong> en presencia de autocorrelación son las mismas que en el caso de existencia de heteroscedasticidad, al tratarse de un mismo caso de matriz de covarianzas no escalar: el estimador MCO será insesgado y consistente pero las varianzas y covarianzas MCO estarán sesgadas, de manera que cualquier inferencia que se haga sobre la base de dichas estimaciones será inválida. Además, aunque se corrija la estimación de la matriz de covarianzas estimada, el estimador MCO sigue siendo ineficiente.</p>
</section>
<section id="detección-de-la-autocorrelación" class="level3" data-number="3.6.2">
<h3 data-number="3.6.2" class="anchored" data-anchor-id="detección-de-la-autocorrelación"><span class="header-section-number">3.6.2</span> Detección de la autocorrelación</h3>
<section id="examen-gráfico-de-los-residuos-mco-1" class="level4" data-number="3.6.2.1">
<h4 data-number="3.6.2.1" class="anchored" data-anchor-id="examen-gráfico-de-los-residuos-mco-1"><span class="header-section-number">3.6.2.1</span> Examen gráfico de los residuos MCO</h4>
<p>La representación gráfica de los residuos <span class="math inline">\({\widehat{e}}_{t}\)</span> respecto al tiempo puede mostrarnos de forma clara el comportamiento autocorrelacionado de los errores. Así, existirían indicios de autocorrelación positiva de orden uno si los residuos MCO se agrupan en rachas con un signo determinado (positivos o negativos), seguidos de rachas del signo contrario. Por otra parte, será una señal de autocorrelación negativa de orden uno en los residuos si se observa que estos van alternando su signo con respecto a la observación anterior.</p>
<p>También puede ser útil representar <span class="math inline">\({\widehat{e}}_{t}\)</span> frente a <span class="math inline">\({\widehat{e}}_{t - 1}\)</span>: una agrupación de los puntos del gráfico alrededor de una línea creciente sería un indicio de que existe una correlación positiva en el tiempo, mientras que una agrupación alrededor de una recta con pendiente negativa sería un síntoma de autocorrelación negativa.</p>
</section>
<section id="contraste-de-durbin-watson" class="level4" data-number="3.6.2.2">
<h4 data-number="3.6.2.2" class="anchored" data-anchor-id="contraste-de-durbin-watson"><span class="header-section-number">3.6.2.2</span> Contraste de Durbin-Watson</h4>
<p>Está diseñado para contrastar sólo la presencia de procesos autorregresivos del tipo <em>AR</em>(1) en los errores, es decir, partiendo del modelo <span class="math inline">\(e_{t} = \rho e_{t - 1} + \varepsilon_{t}\)</span>, se contrasta</p>
<p><span class="math display">\[H_{0}:\{\rho = 0\}\ \ frente\ a\ \ H_{1}:\{\rho \neq 0\}\]</span></p>
<p>El estadístico propuesto por Durbin y Watson (1970) viene dado por:</p>
<p><span class="math display">\[DW = \frac{\sum_{t = 2}^{T}{(ê_{t} - ê_{t - 1})^{2}}}{\sum_{t = 1}^{T}ê_{t}^{2}}\]</span></p>
<p>cumpliéndose que <span class="math inline">\(DW \cong 2(1 - \widehat{\rho})\)</span>, donde <span class="math inline">\(\widehat{\rho}\)</span> es una estimación del parámetro <em>ρ</em> basada en aplicar MCO a la regresión auxiliar <span class="math inline">\({\widehat{e}}_{t} = \rho{\widehat{e}}_{t - 1} + \varepsilon_{t}\)</span>.</p>
<p>Entonces, se puede observar fácilmente que el estadístico <em>DW</em> tiene un recorrido entre 0 (<span class="math inline">\(\widehat{\rho} = 1\)</span>) y 4 (<span class="math inline">\(\widehat{\rho} = - 1\)</span>), y que <span class="math inline">\(DW &lt; 2 \ (&gt; 2)\)</span> es indicativo de autocorrelación positiva (negativa) de los errores <span class="math inline">\(e_{t}\)</span>,mientras que <span class="math inline">\(DW \cong 2\)</span> señala la ausencia de correlación (<span class="math inline">\(\widehat{\rho} = 0\)</span>).</p>
<p>El problema que plantea este contraste es que la distribución del estadístico de Durbin-Watson, <span class="math inline">\(f(d)\)</span>, depende de la matriz de datos <span class="math inline">\(\mathbf{X}\)</span>, aparte del número de observaciones (<em>T</em>) y del número de variables del modelo (<em>K</em>) y, por tanto, no existen tablas para el test exacto. Sin embargo, para cada aplicación concreta pueden calcularse el valor crítico y el <em>P</em>-valor específicos para dicho conjunto de datos (es decir, para la matriz <strong><em>X</em></strong> y los valores de <em>T</em> y <em>K</em> concretos se calculan el valor del estadístico <span class="math inline">\(d_{c}\left( \mathbf{X},T,K \right)\)</span> y del P-valor <span class="math inline">\(\ P\left\lbrack d &lt; d_{c}(\mathbf{X},T,K) \right\rbrack\)</span>), y se toma la decisión respecto al contraste planteado (no rechazar <em>H</em><sub>0</sub> o “aceptar” <em>H</em><sub>1</sub>) en base a esos valores concretos.</p>
</section>
<section id="contraste-general-de-breusch-godfrey" class="level4" data-number="3.6.2.3">
<h4 data-number="3.6.2.3" class="anchored" data-anchor-id="contraste-general-de-breusch-godfrey"><span class="header-section-number">3.6.2.3</span> Contraste general de Breusch-Godfrey</h4>
<p>Se trata de un test válido para detectar autocorrelación generada no sólo por procesos <em>AR</em>(1) sino también por procesos de orden general <em>AR</em>(<em>p</em>) (<span class="math inline">\(p &gt; 1\)</span>), y es especialmente adecuado cuando el test de Durbin-Watson no lleva a ninguna conclusión o resulta inaplicable, como es la situación en que la variable dependiente retardada aparece entre los regresores, en cuyo caso el estadístico <em>DW</em> está sesgado hacia 2 aún en presencia de autocorrelación).</p>
<p>Además, este contraste también tiene potencia para detectar modelos de correlación de medias móviles, <em>MA</em>(<em>q</em>), y modelo mixtos del tipo <em>ARMA</em>(<em>p</em>,<em>q</em>).</p>
<p>Para aplicar el <em>test de Breusch y Godfrey</em> se llevan a cabo los siguientes pasos:</p>
<ul>
<li><p>Se aplican MCO a la regresión <span class="math inline">\(y_{t} = \beta_{1} + \beta_{2}x_{2t} + \ldots + \beta_{K}x_{Kt} + e_{t}\)</span> y se obtienen los residuos estimados <span class="math inline">\({\widehat{e}}_{t}\)</span>.</p></li>
<li><p>Se estima por MCO la regresión auxiliar <span class="math display">\[\ {\widehat{e}}_{t} = \alpha_{0} + \alpha_{1}{\widehat{e}}_{t - 1} + \cdots + \alpha_{p}{\widehat{e}}_{t - p} + \delta_{2}x_{2t} + \cdots + \delta_{K}x_{Kt} + u_{t}\ \]</span> rellenando los valores omitidos para los residuos retardados con ceros.</p></li>
<li><p>De esta regresión auxiliar se toma el coeficiente <span class="math inline">\(\ R^{2}\ \)</span>, y se calcula el estadístico <span class="math inline">\(\ BG = T \times R^{2}\ \)</span>.</p></li>
<li><p>Bajo la hipótesis nula de ausencia de correlación, <span class="math inline">\(H_{0}:\{ Cov(e_{t},e_{s}) = 0\}\)</span>, se cumple que <span class="math inline">\(BG\overset{as}{\sim}\chi_{p}^{2}\ \)</span>.</p></li>
</ul>
<p>Tal como está diseñado, el test de Breusch-Godfrey tiene como hipótesis alternativa el caso general <span class="math inline">\(H_{1}:\{ AR(p),\ MA(p)\ \ o\ \ ARMA(p,p)\}\)</span>, es decir, el tipo de correlación puede ser autorregresiva, de medias móviles o una mezcla de ambos procesos.</p>
</section>
<section id="otras-formas-de-autocorrelación-modelos-autorregresivos-con-dependencia-espacial" class="level4" data-number="3.6.2.4">
<h4 data-number="3.6.2.4" class="anchored" data-anchor-id="otras-formas-de-autocorrelación-modelos-autorregresivos-con-dependencia-espacial"><span class="header-section-number">3.6.2.4</span> Otras formas de autocorrelación: modelos autorregresivos con dependencia espacial</h4>
<p>En este apartado se propone una familia de modelos diseñada para tratar una forma especial de correlación en los errores que aparece en el ámbito del análisis de datos de corte transversal referenciados geográficamente, la <em>autocorrelación espacial</em>.</p>
<p>En la vida real existen muchos procesos económicos que pueden dar lugar a aspectos espaciales en los datos. Por ejemplo, variables de tipo inobservable (como el clima, la calidad y riqueza del suelo o la disponibilidad de ciertas materias primas) pueden estar relacionadas ‘espacialmente’ y, por tanto, producir correlación en los errores; asimismo, patrones de mimetismo en el comportamiento económico de las unidades económicas pueden originar correlación espacial en las regresiones que describan dicho comportamiento.</p>
<p>Los patrones de comportamiento espacial a menudo pueden describirse de forma adecuada en términos de <em>dependencia espacial</em>, pudiendo ésta definirse como la correlación positiva o negativa entre los valores de una variable para los individuos o las regiones de un espacio geográfico (tal espacio debe estar dividido en zonas que completen el mismo y que no se solapen).</p>
<p>Por ejemplo, si consideramos las Comunidades Autónomas que componen España, un proceso de dependencia espacial puede caracterizarse por la relación entre los valores que toma una variable <em>y</em> en una comunidad y los valores que toma dicha variable en otras comunidades; así, una relación de primer orden entre la comunidad 2 y sus vecinas las comunidades 1, 8 y 3 puede describirse mediante la ecuación <em>y</em><sub>2</sub><em>=φy</em><sub>1</sub><em>+φy</em><sub>8</sub><em>+φy</em><sub>3</sub><em>+e</em>, donde <em>e</em> es un término de error con las propiedades estándar.</p>
<p>La correlación espacial en una variable podría en sí misma no tener mucho interés para los economistas; por ejemplo, si las familias pobres de una ciudad tienden a vivir en la periferia, resulta poco informativo encontrar que el modelo de consumo/ahorro de tales familias es muy parecido. Lo interesante podría ser contrastar la presencia de correlación espacial en los residuos una vez que se ha descontado el efecto riqueza, es decir, encontrar patrones de comportamiento espacial en los errores de un modelo del tipo <span class="math inline">\(\mathbf{y} = \mathbf{X\beta} + \mathbf{e}\)</span>, donde la matriz <span class="math inline">\(\mathbf{X}\)</span> contiene las <em>K</em> variables que explican el comportamiento de la variable <em>y</em>.</p>
<p>El contraste más simple de correlación espacial es el estadístico propuesto por Moran (1948), el cual mide la covariación existente entre los errores de zonas ‘vecinas’ con relación a la varianza total de los errores.</p>
<p>Técnicamente, asignando pesos unitarios a zonas vecinas (<span class="math inline">\(w_{ij} = 1\)</span> si las zonas <em>i</em> y <em>j</em> tienen una frontera o un vértice común) y nulos en caso contrario (<span class="math inline">\(w_{ij} = 0\)</span> si las zonas <em>i</em> y <em>j</em> no comparten frontera o vértice), y asumiendo por definición que <span class="math inline">\(w_{ii} = 0\)</span> , el <em>test de Moran</em> se define como</p>
<p><span class="math display">\[I = \frac{\frac{\sum_{i = 1}^{n}{\sum_{j = 1}^{n}{{\widehat{e}}_{i}w_{ij}{\widehat{e}}_{j}}}}{2F}}{\frac{\sum_{i = 1}^{n}{\widehat{e}}_{i}^{2}}{n}}\]</span></p>
<p>donde <em>F</em> es el número total de fronteras o vértices y <em>n</em> es el número total de zonas.</p>
<p>Bajo la hipótesis nula de ausencia de correlación espacial, <span class="math inline">\(H_{0}:\{ Cov(e_{i},e_{j}) = 0\}\)</span>, el estadístico <span class="math inline">\(I\)</span> se distribuye asintóticamente como una variable aleatoria normal, <span class="math inline">\(I\overset{as}{\sim}N(\mu_{I},\sigma_{I}^{2})\ \)</span>, cuya media (<span class="math inline">\(\mu_{I}\)</span>) y varianza (<span class="math inline">\(\sigma_{I}^{2}\)</span>) se pueden calcular de forma relativamente sencilla. Tales valores dependen de la matriz <span class="math inline">\(\mathbf{X}\)</span> de variables explicativas, de la matriz <span class="math inline">\(\mathbf{W}\)</span> de pesos espaciales, y de los valores de <em>F</em>, <em>n</em> y <em>K</em>.</p>
<p>Cuando el estadístico <span class="math inline">\(I\)</span> resulta significativo (<span class="math inline">\(H_{1}:\{ Cov(e_{i},e_{j}) \neq 0\}\)</span>), los valores positivos implican <em>concentración espacial</em>, mientras que valores negativos señalan <em>dispersión en el espacio</em>. Estos fenómenos de concentración-dispersión puede ser consistentes con varias estructuras espaciales, una de las cuales resulta de interés en el contexto de la autocorrelación.</p>
<p>Se trata de procesos en los que el origen de la dependencia espacial está en el término de error y tiene la forma autorregresiva espacial en los errores, <em>SEM</em>, siguiente:</p>
<p><span class="math display">\[\mathbf{y} = \mathbf{X\beta} + \mathbf{e}\]</span> <span class="math display">\[\mathbf{e} = \lambda\mathbf{We} + \mathbf{u}\]</span></p>
<p>siendo <span class="math inline">\(\mathbf{W}\)</span> la matriz de pesos espaciales, <em>λ</em> el parámetro que recoge la intensidad de las interdependencias espaciales en el término de perturbación, y <span class="math inline">\(\mathbf{u}\sim N(\mathbf{0},\sigma^{2}\mathbf{I})\)</span>.</p>
<p>En la práctica, como matriz <span class="math inline">\(\mathbf{W}\)</span> se suele tomar la matriz de contigüidad <em>tipo reina</em> definida en el test de Moran, aunque en la literatura especializada (<em>econometría espacial</em>) se han propuesto otras definiciones de la <em>matriz de pesos espaciales</em> alternativas, algunas basadas en la utilización de la distancia entre regiones —geográfica o económica— o, más generalmente, definidas de forma específica para cada aplicación concreta.</p>
</section>
</section>
<section id="regresiones-con-autocorrelación-en-los-errores-y-modelos-econométricos-dinámicos" class="level3" data-number="3.6.3">
<h3 data-number="3.6.3" class="anchored" data-anchor-id="regresiones-con-autocorrelación-en-los-errores-y-modelos-econométricos-dinámicos"><span class="header-section-number">3.6.3</span> Regresiones con autocorrelación en los errores y modelos econométricos dinámicos</h3>
<p>Supongamos que en el modelo de regresión <span class="math inline">\(y_{t} = \beta_{1} + \beta_{2}x_{2t} + \ldots + \beta_{K}x_{Kt} + e_{t}\)</span> existe autocorrelación temporal en los errores, es decir, se cumple que <span class="math inline">\(Cov\left( e_{t},e_{s} \right) = \sigma_{ts} \neq 0\)</span> .</p>
<section id="el-estimador-de-mínimos-cuadrados-corregidos-método-de-newey-west" class="level4" data-number="3.6.3.1">
<h4 data-number="3.6.3.1" class="anchored" data-anchor-id="el-estimador-de-mínimos-cuadrados-corregidos-método-de-newey-west"><span class="header-section-number">3.6.3.1</span> El estimador de mínimos cuadrados corregidos (método de Newey-West)</h4>
<p>Igual que en el caso de heteroscedasticidad, en presencia de autocorrelación en los errores puede utilizarse el estimador de <em>MCO corregidos</em>, modificando apropiadamente la matriz de covarianzas estimada.</p>
<p>Así, Newey y West (1987) demostraron que la matriz de covarianzas correcta del estimador MCO, <span class="math inline">\(Cov(\widehat{\mathbf{\beta}}) = (\mathbf{X}'\mathbf{X})^{- 1}\left( \mathbf{X}'\Omega\mathbf{X} \right)(\mathbf{X}'\mathbf{X})^{- 1}\)</span>, puede estimarse de forma consistente mediante la <em>matriz ‘sandwich’</em></p>
<p><span class="math display">\[{Cov}^{\land}(\widehat{\mathbf{\beta}})_{NW} = (\mathbf{X}^{'}\mathbf{X})^{- 1}\mathbf{S}^{*}(\mathbf{X}^{'}\mathbf{X})^{- 1}\]</span></p>
<p>donde <span class="math inline">\(\mathbf{S}^{*} = \mathbf{S}_{0} + \frac{T}{T - K}\sum_{j = 1}^{L}{\sum_{t = j + 1}^{T}{w(j){\widehat{e}}_{t}{\widehat{e}}_{t - j}\left\lbrack \mathbf{x}_{t - j}\mathbf{x}_{t}^{'} + \mathbf{x}_{t}\mathbf{x}_{t - j}^{'} \right\rbrack}}\)</span> siendo <span class="math inline">\(\mathbf{S}_{0}\)</span> la matriz definida por White para el caso de heteroscedasticidad, <span class="math inline">\(\mathbf{S}_{0} = \frac{T}{T - K}\sum_{t = 1}^{T}{ê_{t}^{2}\mathbf{x}_{t}\mathbf{x}_{t}^{'}}\)</span>, y <span class="math inline">\(w(j) = 1 - \left\lbrack j/(L + 1) \right\rbrack\)</span>.</p>
<p>El coeficiente <em>L</em> debe tomar un valor suficientemente elevado como para que las autocorrelaciones en retardos mayores que <em>L</em> sean despreciables. En general, valores de <em>L</em> iguales a 3 o 4 serán suficientes, aunque también pueden utilizarse reglas de corte, como la propuesta por los propios Newey y West (1994), quienes sugieren utilizar el valor <span class="math inline">\(L \cong {4\left( \frac{T}{100} \right)}^{\frac{2}{9}}\)</span>.</p>
<p>El estimador resultante para la matriz de covarianzas es consistente, tanto en presencia de heteroscedasticidad como de autocorrelación de formas desconocidas.</p>
</section>
<section id="el-estimador-de-mínimos-cuadrados-generalizados" class="level4" data-number="3.6.3.2">
<h4 data-number="3.6.3.2" class="anchored" data-anchor-id="el-estimador-de-mínimos-cuadrados-generalizados"><span class="header-section-number">3.6.3.2</span> El estimador de mínimos cuadrados generalizados</h4>
<p>Si se detecta la presencia de autocorrelación en los errores del modelo, en lugar de MCO corregidos puede aplicarse el <em>estimador de mínimos cuadrados generalizados (MCG)</em>, <span class="math inline">\({\widehat{\mathbf{\beta}}}_{MCG}\)</span>, el cual garantiza la obtención de estimadores lineales, insesgados, consistentes y de mínima varianza (eficientes).</p>
<p>Cuando el proceso de autocorrelaciones sigue un modelo <em>AR</em>(1), <span class="math inline">\(e_{t} = \rho e_{t - 1} + \varepsilon_{t}\)</span>, el método MCG consiste en tomar <span class="math inline">\(\mathbf{\rho}\)</span>-diferencias, esto es, en transformar el modelo original mediante el procedimiento <span class="math inline">\(z_{t}^{*} = z_{t} - \rho z_{t - 1}\)</span>, donde <span class="math inline">\(z\)</span> representa a cualquiera de las variables del modelo, y aplicar MCO al modelo transformado.</p>
<p>Así, si se parte de una estimación del coeficiente <span class="math inline">\(\widehat{\rho}\)</span> a partir de la regresión</p>
<p><span class="math display">\[{\widehat{e}}_{t} = \rho{\widehat{e}}_{t - 1} + \varepsilon_{t}\]</span></p>
<p>el método de MCG consiste en transformar el modelo original mediante las operaciones <span class="math inline">\(y_{t} - \widehat{\rho}y_{t - 1}\)</span> y <span class="math inline">\(x_{mt} - \widehat{\rho}x_{m,t - 1}\)</span>, para <em>m</em> = 2, ..., <em>K</em>, y aplicar MCO al modelo transformado</p>
<p><span class="math display">\[y_{t} - \widehat{\rho}y_{t - 1} = \beta_{1}\left( 1 - \widehat{\rho} \right) + \beta_{2}\left( x_{2t} - \widehat{\rho}x_{2,t - 1} \right) + \ldots + \beta_{K}\left( x_{Kt} - \widehat{\rho}x_{K,t - 1} \right) + e_{t}^{*}\]</span></p>
<p>En la práctica, el método más utilizado para llevar a cabo la estimación MCG consiste en repetir el proceso anterior un número suficiente de veces hasta conseguir la “convergencia” de las estimaciones (minimizando la suma de los cuadrados de los residuos); este procedimiento recibe el nombre de <em>método iterativo de Cochrane-Orcutt</em> (1949).</p>
<p>También puede optarse por utilizar directamente el método de máxima verosimilitud (MV) que, aparte de sus buenas propiedades estadísticas (consistencia, eficiencia y normalidad asintótica), tiene la ventaja respecto a los métodos de MCG iterativos de que puede usarse para estimar cualquier modelo autorregresivo, bien sea del tipo <em>AR</em>(<em>p</em>), <em>MA</em>(<em>q</em>), la mezcla de ambos, <em>ARMA</em>(<em>p</em>,<em>q</em>), o modelos con otros tipos de autocorrelación, como los expuestos a continuación.</p>
</section>
<section id="modelos-con-correlación-espacial-en-los-errores" class="level4" data-number="3.6.3.3">
<h4 data-number="3.6.3.3" class="anchored" data-anchor-id="modelos-con-correlación-espacial-en-los-errores"><span class="header-section-number">3.6.3.3</span> Modelos con correlación espacial en los errores</h4>
<p>Supongamos que se parte de un modelo de regresión que tiene la forma autorregresiva espacial siguiente, denominada <em>tipo SEM en los errores</em> del modelo:</p>
<p><span class="math display">\[\mathbf{y} = \mathbf{X\beta} + \mathbf{e}\]</span></p>
<p><span class="math display">\[\mathbf{e} = \lambda\mathbf{We} + \mathbf{u}\]</span></p>
<p>En estos casos, en lugar de usar el estimador MCO, puede utilizarse uno más eficiente, el estimador MV, <span class="math inline">\({\widehat{\mathbf{\beta}}}_{MV}\)</span>, que garantiza las propiedades de consistencia, eficiencia y normalidad asintótica.</p>
</section>
<section id="modelos-con-dinámica-en-el-tiempo" class="level4" data-number="3.6.3.4">
<h4 data-number="3.6.3.4" class="anchored" data-anchor-id="modelos-con-dinámica-en-el-tiempo"><span class="header-section-number">3.6.3.4</span> Modelos con dinámica en el tiempo</h4>
<p>Una de las causas más comunes de la presencia de autocorrelación temporal en los errores es la mala especificación dinámica del modelo de regresión. Por dicho motivo, en este epígrafe desarrollaremos los aspectos básicos de los modelos econométricos dinámicos.</p>
<p>En el modelo de regresión lineal estándar, <span class="math inline">\(y_{t} = \beta_{1} + \beta_{2}x_{2t} + \ldots + \beta_{K}x_{Kt} + e_{t}\)</span>, cuando se produce un cambio unitario en una de las variables explicativas, <span class="math inline">\(x_{j}\)</span>, el correspondiente valor medio de la variable endógena, <span class="math inline">\(y\)</span>, incrementa o disminuye <em>instantáneamente</em> aproximadamente en <span class="math inline">\(\beta_{j}\)</span> unidades:</p>
<p><span class="math display">\[\beta_{j} \cong \Delta E(y)\ \ \ cuando\ \ \ \Delta x_{j} = 1\ \ \left( y\ \ \Delta x_{i} = 0\ \ \ i \neq j \right)\]</span></p>
<p>En otros términos, dada la forma en la que se especifica el modelo, el tiempo que transcurre desde que se produce el cambio en la variable explicativa <em>x<sub>j</sub></em> hasta que se alcanza el equilibrio en el nuevo valor de la variable <em>y</em> es muy pequeño. Por tanto, este tipo de modelos es estático por su propia naturaleza.</p>
<p>Sin embargo, en economía la dinámica de respuesta de <em>y</em> ante cambios en las variables <em>x</em> raras veces es inmediata y el ajuste de la posición inicial del sistema a la nueva situación de equilibrio se “<em>distribuye</em>” de forma considerable en el tiempo.</p>
<p>Una de las formas de introducir <em>dinámica</em> en el modelo de regresión estándar (estático) consiste en incluir variables retardadas entre los regresores. Así se llega a los <em>modelos de retardos distribuidos</em> <em>(</em>DL<em>)</em>, si aparecen las variables exógenas retardadas, o a los <em>modelos autorregresivos</em> (AR<em>)</em>, si aparece la endógena retardada entre las variables explicativas. La “mezcla” de ambos tipos de estructuras dinámicas forma un grupo más general, los denominados modelos <em>autorregresivos</em> <em>con retardos distribuidos (</em>ARDL). Un caso particular de este tipo de modelos viene dado por los <em>modelos con corrección del error</em> <em>(</em>MCE<em>)</em>, que son el tipo de modelos más utilizados en la econometría moderna puesto que, aparte de integrar en una sola especificación las relaciones de corto y de largo plazo existentes entre un conjunto de variables, representan la solución matemática al problema de las regresiones espurias que durante mucho tiempo preocupó a los investigadores en economía aplicada.</p>
</section>
<section id="modelos-con-retardos-en-las-variables-explicativas" class="level4" data-number="3.6.3.5">
<h4 data-number="3.6.3.5" class="anchored" data-anchor-id="modelos-con-retardos-en-las-variables-explicativas"><span class="header-section-number">3.6.3.5</span> Modelos con retardos en las variables explicativas</h4>
<p>En general, la justificación económica de este tipo de modelos viene dada por el hecho de que en muchas ocasiones el efecto total sobre la variable endógena de un cambio en una variable explicativa puede repartirse en diferentes períodos de tiempo por razones de inercia, tecnología o debido a razones de tipo institucional.</p>
<p>En el caso de una regresión lineal con una única variable explicativa <em>x</em>, si el efecto de la variable <em>x</em> sobre <em>y</em> se acaba agotando en el tiempo después de transcurridos <em>q</em> períodos, el modelo a utilizar es el de retardos distribuidos de orden <em>q</em>, <em>DL(</em>q<em>)</em>, que toma la expresión</p>
<p><span class="math display">\[y_{t} = \alpha_{0} + \beta_{0}x_{t} + \beta_{1}x_{t - 1} + \ldots + \beta_{q}x_{t - q} + e_{t}\]</span></p>
<p>En este caso, la función de respuesta de <em>y</em> ante un impulso en la variable exógena <em>x</em> toma la forma de la sucesión <span class="math inline">\(\left\{ \beta_{0},\beta_{1},\ldots,\beta_{q - 1},\beta_{q},0,\ 0,\ldots \right\}\)</span>: si en un cierto instante del tiempo se produce un cambio unitario en la variable explicativa <em>x</em> a partir de una hipotética situación de equilibrio, la respuesta de la variable endógena, en términos de desviación de su posición inicial de equilibrio, será <em>β</em><sub>0</sub> en el instante inicial, <em>β</em><sub>1</sub> en el siguiente período, y así sucesivamente hasta el período <em>q</em>, <em>β<sub>q</sub></em>, siendo 0 a partir de ese instante.</p>
<p>Para estos modelos de retardos distribuidos se pueden diferenciar dos efectos:</p>
<ul>
<li>El efecto a corto plazo o <em>multiplicador de impacto</em>, dado por</li>
</ul>
<p><span class="math display">\[\beta_{0} \cong \Delta^{cp}E(y)\ \ \ cuando\ \ \ \Delta x = 1\]</span></p>
<ul>
<li>El efecto a largo plazo (total) o <em>multiplicador de equilibrio</em>, dado por</li>
</ul>
<p><span class="math display">\[\beta_{0} + \beta_{1} + \ldots + \beta_{q} \cong \Delta^{lp}E(y)\ \ \ cuando\ \ \ \Delta x = 1\]</span></p>
<p>Aunque se ha simplificado la notación al objeto de llevar a cabo una exposición más sencilla, el modelo de retardos puede contener varias variables explicativas, cada una de ellas con el orden de retardos correspondiente.</p>
<p>Por otra parte, la estimación de los modelos <em>DL</em> plantea varios problemas generales:</p>
<ul>
<li><p>Debe determinarse la longitud concreta (<em>q</em>) de los retardos: la búsqueda del retardo óptimo no es adecuada si no se tiene en cuenta que los contrastes son secuenciales, no siendo válidos los niveles de significación estándar.</p></li>
<li><p>Se produce una reducción importante en el número de grados de libertad por la pérdida de las observaciones iniciales necesarias para construir las variables retardadas. Esto hará inviable la estimación salvo que se disponga de una muestra de tamaño elevado.</p></li>
<li><p>Aún con muestras grandes, la probable alta correlación que se observará entre los retardos de una misma variable ocasionará graves problemas de multicolinealidad, impidiendo una estimación precisa de los parámetros <em>β</em> del modelo.</p></li>
</ul>
<p>Para evitar o, al menos, amortiguar algunos de estos problemas, se pueden establecer algunas hipótesis adicionales sobre el comportamiento de los parámetros, de manera que en la práctica pueden imponorse algún tipo de restricción sobre los coeficientes <em>β</em>, como por ejemplo las restricciones incluidas en los <em>modelos de retardos polinomiales de Almon</em> o las contenidas en los <em>modelos de retardos geométricos de Koyck</em>, ambos propuestos en la literatura econométrica sobre el tema.</p>
</section>
<section id="modelos-con-retardos-en-la-variable-endógena" class="level4" data-number="3.6.3.6">
<h4 data-number="3.6.3.6" class="anchored" data-anchor-id="modelos-con-retardos-en-la-variable-endógena"><span class="header-section-number">3.6.3.6</span> Modelos con retardos en la variable endógena</h4>
<p>Un modelo autorregresivo de orden <em>p</em>, <em>AR(</em>p<em>)</em>, tiene la forma general</p>
<p><span class="math display">\[y_{t} = \alpha_{0} + \alpha_{1}y_{t - 1} + \alpha_{2}y_{t - 2} + ... + \alpha_{p}y_{t - p} + \beta_{0}x_{t} + e_{t}\]</span></p>
<p>es decir, el valor la variable dependiente <em>y</em> en el instante <em>t</em> depende no sólo del valor de la variable explicativa <em>x</em> en dicho instante, sino también de sus propios valores pasados.</p>
<p>Para estos modelos autorregresivos se pueden distinguir de nuevo dos efectos:</p>
<ul>
<li>El efecto a corto plazo sobre la media de <em>y</em> de un cambio unitario en <em>x</em> es</li>
</ul>
<p><span class="math display">\[\beta_{0} \cong \Delta^{cp}E(y)\ \ \ cuando\ \ \ \Delta x = 1\]</span></p>
<ul>
<li>El efecto a largo plazo viene dado por</li>
</ul>
<p><span class="math display">\[\frac{\beta_{0}}{1 - \alpha_{1} - \alpha_{2} - \ldots - \alpha_{p}} \cong \Delta^{lp}E(y)\ \ \ cuando\ \ \ \Delta x = 1\]</span></p>
<p>En general, este tipo de modelos puede estimarse por MCO, salvo en el caso de que los errores del modelo, <em>e<sub>t</sub></em>, estén autocorrelacionados, lo que provoca la inconsistencia del estimador MCO y, por tanto, la necesidad de utilizar un estimador alternativo, el de variables instrumentales (VI) que se expondrá posteriormente.</p>
<p>En consecuencia, resulta de vital importancia el contraste de la hipótesis de ausencia de correlación en los modelos autorregresivos. En este sentido, hay que tener en cuenta que el test de Durbin-Watson, <em>DW</em>, está sesgado hacia 2 ante la presencia de retardos de la variable endógena entre los regresores, por lo que para estos casos puede utilizarse el contraste de Breusch-Godfrey, o el test <em>h</em> de Durbin (1970), que toma la expresión <span class="math inline">\(h = \left( 1 - \frac{DW}{2} \right)\ \sqrt{\frac{T}{(1 - T\left\lbrack Var\left( {\widehat{\alpha}}_{1} \right) \right\rbrack)}}\)</span> y que, bajo la hipótesis nula de ausencia de correlación, tiene una distribución asintótica <em>N</em>(0,1).</p>
</section>
<section id="modelos-autorregresivos-con-retardos-distribuidos" class="level4" data-number="3.6.3.7">
<h4 data-number="3.6.3.7" class="anchored" data-anchor-id="modelos-autorregresivos-con-retardos-distribuidos"><span class="header-section-number">3.6.3.7</span> Modelos autorregresivos con retardos distribuidos</h4>
<p>Una clase general de modelos de regresión que anida tanto los modelos con retardos finitos en las variables exógenas como los modelos con retardos en la endógena la constituyen los llamados modelos autorregresivos con retardos distribuidos (ARDL). En estos modelos la variable dependiente, <em>y<sub>t</sub></em>, se expresa como función de sus propios valores pasados, así como del valor presente y retardado de otras variables explicativas. Así, un modelo <em>ARDL</em>(<em>p</em>,<em>q</em>) con un único regresor tiene la forma siguiente:</p>
<p><span class="math display">\[y_{t} = \alpha_{0} + \alpha_{1}y_{t - 1} + \ldots + \alpha_{p}y_{t - p} + \beta_{0}x_{t} + \beta_{1}x_{t - 1} + \ldots + \beta_{q}x_{t - q} + e_{t}\]</span></p>
<p>la cual tiene una extensión inmediata al caso general de <em>K</em> variables explicativas, donde cada una de ellas podrá tener su propio retardo.</p>
<p>Para estos modelos generales los efectos tienen la forma siguiente:</p>
<ul>
<li>El efecto a corto plazo sobre la media de <em>y</em> de un cambio unitario en <em>x</em> es</li>
</ul>
<p><span class="math display">\[\beta_{0} \cong \Delta^{cp}E(y)\ \ \ cuando\ \ \ \Delta x = 1\]</span></p>
<ul>
<li>El efecto a largo plazo viene dado por</li>
</ul>
<p><span class="math display">\[\frac{\beta_{0} + \beta_{1} + \ldots + \beta_{q}}{1 - \alpha_{1} - \alpha_{2} - \ldots - \alpha_{p}} \cong \Delta^{lp}E(y)\ \ \ cuando\ \ \ \Delta x = 1\]</span></p>
<p>A partir de la expresión general de un modelo <em>ARDL</em> pueden obtenerse como casos particulares los modelos autorregresivos (<em>β<sub>j</sub></em> = 0 para todo <em>j</em>) y los modelos con retardos distribuidos finitos (<em>α<sub>j</sub></em> = 0 para todo <em>j</em> ≠ 0), aunque también pueden lograrse otras especificaciones económicamente interesantes según los valores de los <em>β</em>’s o los <em>α</em>’s.</p>
</section>
<section id="modelos-vectoriales-autorregresivos" class="level4" data-number="3.6.3.8">
<h4 data-number="3.6.3.8" class="anchored" data-anchor-id="modelos-vectoriales-autorregresivos"><span class="header-section-number">3.6.3.8</span> Modelos vectoriales autorregresivos</h4>
<p>Una clase más general que los modelos <em>ARDL</em> uniecuacionales viene dada por los llamados modelos vectoriales autorregresivos (<em>VAR</em>), que son la base del actual análisis multivariante de series temporales.</p>
<p>En el caso más simple, supongamos que tenemos dos variables <em>x</em> e <em>y</em> que se determinan simultáneamente y, por tanto, la especificación econométrica apropiada sería la forma estructural siguiente (por simplicidad, suponemos una dinámica <em>ARDL</em>(1,1) para cada variable):</p>
<p><span class="math display">\[\mathbf{\ }\left\{ \begin{matrix} {\ y}_{t} = \alpha_{10} + \alpha_{11}y_{t - 1} + \beta_{10}x_{t} + \beta_{11}x_{t - 1} + e_{1t} \\ {\ x}_{t} = \alpha_{20} + \alpha_{21}x_{t - 1} + \beta_{20}y_{t} + \beta_{21}y_{t - 1} + e_{2t} \\ \end{matrix} \right.\ \]</span></p>
<p>donde asumiremos que <span class="math inline">\(Cov\left( e_{1t},e_{2t} \right) = 0\)</span>. Este sistema puede escribirse matricialmente como</p>
<p><span class="math display">\[\begin{bmatrix}
1 &amp; - \beta_{10} \\
- \beta_{20} &amp; 1 \\
\end{bmatrix}\begin{pmatrix}
y_{t} \\
x_{t} \\
\end{pmatrix} = \begin{pmatrix}
\alpha_{10} \\
\alpha_{20} \\
\end{pmatrix} + \begin{bmatrix}
\alpha_{11} &amp; \beta_{11} \\
\beta_{21} &amp; \alpha_{21} \\
\end{bmatrix}\begin{pmatrix}
y_{t - 1} \\
x_{t - 1} \\
\end{pmatrix} + \begin{pmatrix}
e_{1t} \\
e_{2t} \\
\end{pmatrix}\]</span></p>
<p>o escrito en forma compacta como <span class="math inline">\(\mathbf{Bz}_{t} = \mathbf{\Gamma}_{0} + \mathbf{\Gamma}_{1}\mathbf{z}_{t - 1} + \mathbf{e}_{t}\)</span>, donde <span class="math inline">\(\mathbf{B},\ \mathbf{\Gamma}_{0}\ y\ \mathbf{\Gamma}_{1}\)</span> son las correspondientes matrices del sistema anterior y <span class="math inline">\(\mathbf{z}_{t} = \begin{pmatrix} y_{t} \\ x_{t} \\ \end{pmatrix}\ \)</span>. Multiplicando por <span class="math inline">\(\mathbf{B}^{- 1}\)</span> el sistema anterior se llega a la expresión siguiente:</p>
<p><span class="math display">\[\mathbf{z}_{t} = \mathbf{A}_{0} + \mathbf{A}_{1}\mathbf{z}_{t - 1} + \mathbf{u}_{t}\]</span></p>
<p>que se conoce como modelo <em>VAR</em> en forma reducida (en el sistema anterior <span class="math inline">\(\mathbf{A}_{0} = \mathbf{B}^{- 1}\ \mathbf{\Gamma}_{0}\)</span>, <span class="math inline">\(\mathbf{A}_{1} = \mathbf{B}^{- 1}\ \mathbf{\Gamma}_{1}\)</span> y <span class="math inline">\(\mathbf{u}_{t} = \mathbf{B}^{- 1}\mathbf{e}_{t}\)</span>).</p>
<p>El modelo <em>VAR</em> anterior no es más que un modelo autorregresivo de primer orden para el vector de variables endógenas <span class="math inline">\(\mathbf{z}\)</span>, que se puede generalizar de forma sencilla hacia un modelo <em>VAR</em>(<em>p</em>) que incluye <em>p</em> retardos en el vector <span class="math inline">\(\mathbf{z}\)</span>:</p>
<p><span class="math display">\[\mathbf{z}_{t} = \mathbf{A}_{0} + \mathbf{A}_{1}\mathbf{z}_{t - 1} + \mathbf{A}_{2}\mathbf{z}_{t - 2} + \ldots\mathbf{A}_{p}\mathbf{z}_{t - p} + \mathbf{u}_{t}\]</span></p>
<p>En la práctica, el uso que se hace de los modelos <em>VAR</em> no consiste en la explotación estructural de los mismos (interpretación de los parámetros estimados), sino que suelen emplearse para el análisis de correlación temporal entre variables macroeconómicas (<em>causalidad de Granger</em>), para la simulación de los efectos de <em>shocks</em> externos sobre un sistema económico (<em>funciones de respuesta al impulso</em> y de <em>descomposición de la varianza</em>), o para realizar predicciones a futuro de un conjunto de variables del modelo, etc.</p>
</section>
<section id="e.-modelos-con-corrección-del-error" class="level4 list-paragraph" data-number="3.6.3.9">
<h4 class="list-paragraph anchored" data-number="3.6.3.9" data-anchor-id="e.-modelos-con-corrección-del-error"><span class="header-section-number">3.6.3.9</span> Modelos con corrección del error</h4>
<p>En las últimas décadas, este tipo de modelos ha adquirido una importancia capital en el campo de la macroeconomía aplicada ya que proporcionan al económetra una herramienta adecuada para tratar con los problemas de <em>no estacionariedad</em> de las series temporales económicas y de existencia de <em>regresiones espurias</em> (erróneas o falsas). Además, son la pieza fundamental de la llamada <em>teoría de la cointegración</em>, que analiza la validez de las regresiones entre series económicas temporales con tendencias estocásticas. El desarrollo completo de estos conceptos excede el alcance de este manual, pero a continuación desarrollaremos algunas ideas básicas.</p>
<p>Como punto de partida, consideremos un modelo <em>ARDL</em>(1,1), <span class="math inline">\(y_{t} = \alpha_{0} + \alpha_{1}y_{t - 1} + \beta_{0}x_{t} + \beta_{1}x_{t - 1} + e_{t}\)</span>, el cual puede escribirse como</p>
<p><span class="math display">\[\Delta y_{t} = \alpha_{0} + \beta_{0}\Delta x_{t} - \lambda(y_{t - 1} - \theta x_{t - 1}) + e_{t}\]</span></p>
<p>donde <span class="math inline">\(\lambda = 1 - \alpha_{1}\)</span> y <span class="math inline">\(\theta = (\beta_{0} + \beta_{1})/(1 - \alpha_{1})\)</span>. Esta relación se conoce como <em>representación en forma de corrección del error (MCE)</em> del modelo <em>ARDL</em>(1,1), el término <span class="math inline">\((y_{t - 1} - \theta x_{t - 1})\)</span> como <em>error de desequilibrio</em> (o término de corrección del error) y el parámetro <span class="math inline">\(\lambda\)</span> como <em>velocidad del ajuste</em> del sistema hacia el equilibrio a largo plazo. Hay que notar que el término de corrección representa la desviación de las variables <em>y</em> y <em>x</em>, en el período <em>t</em> − 1, respecto a su posición de equilibrio a largo plazo, dada por la ecuación <span class="math inline">\(y^{*} = \frac{\alpha_{0}}{1 - \alpha_{1}} + \frac{\beta_{0} + \beta_{1}}{1 - \alpha_{1}}x^{*}\)</span>.</p>
<p>El punto clave en la teoría de la cointegración es el análisis de la validez de tal ecuación estable a largo plazo (conocida como <em>relación de cointegración</em>), para lo cual las tendencias estocásticas de las series temporables <em>y</em> y <em>x</em> deben “cancelarse”, lo que equivale a decir que los errores de desequilibrio deben ser estacionarios (técnicamente, las variables estacionarias se dice que no contienen <em>raíces unitarias</em>).</p>
<p>En los modelos MCE, el efecto a largo plazo sobre <em>y</em> de un cambio unitario en <em>x</em> viene dado por la expresión</p>
<p><span class="math display">\[\frac{\partial y^{*}}{\partial x^{*}} = \left( \frac{\beta_{0} + \beta_{1}}{1 - \alpha_{1}} \right) = \theta\]</span></p>
<p>mientras que, directamente de la expresión del modelo MCE, se tiene que el efecto a corto plazo de un cambio unitario en <em>x</em> sobre la media de <em>y</em> vendrá dado por</p>
<p><span class="math display">\[\Delta E\lbrack y\rbrack \cong \beta_{0}\ cuando\ \Delta x = 1\]</span></p>
<p>Los modelos de corrección del error, al ser no lineales, pueden estimarse por el método de MV, aunque su estimación suele realizarse siguiendo un procedimiento de dos etapas conocido como <em>método de Engle y Granger</em>.</p>
<p>Este enfoque consiste en estimar en la primera etapa, utilizando el método de MCO, la relación a largo plazo <span class="math inline">\(y_{t} = \theta x_{t} + u_{t}\)</span>; y en la segunda etapa, también por MCO, el modelo en forma de corrección del error, substituyendo los errores de desequilibrio teóricos, <span class="math inline">\((y_{t - 1} - \theta x_{t - 1})\)</span>, por sus análogos estimados, <span class="math inline">\(\Delta y_{t} = \alpha_{0} + \beta_{0}\Delta x_{t} - \lambda{\widehat{u}}_{t - 1} + e_{t}\)</span>.</p>
<p>Para el caso general, donde existen varios regresores como factores explicativos de la variable <em>y</em>, el enfoque de Engle-Granger funciona de forma análoga al procedimiento bi-etápico anterior:</p>
<p>- En primer lugar, se estima** por MCO la ecuación de largo plazo siguiente:</p>
<p><span class="math display">\[\mathbf{\ }y_{t} = \theta_{1} + \theta_{2}x_{2t} + \ldots + \theta_{K}x_{Kt} + u_{t}\ \]</span></p>
<p>y, a continuación, se calculan los errores estimados, <span class="math inline">\({\widehat{u}}_{t}\)</span>.</p>
<p>- En la segunda etapa, se estima también por MCO el modelo dinámico de corrección del error (MCE), que será un modelo ARDL en las primeras diferencias de la variable dependiente y las variables explicativas , <span class="math inline">\(\mathrm{\Delta}y_{t}\)</span> y <span class="math inline">\(\mathrm{\Delta}x_{jt}\)</span>, más el término de corrección del error <span class="math inline">\(- \lambda{\widehat{u}}_{t - 1}\)</span> y otros regresores <em>z</em>’s (en primeras diferencias o no) que aporten información que permita explicar las variaciones de la variable <em>y</em> en el corto plazo:</p>
<p><span class="math display">\[\mathrm{\Delta}y_{t} = \alpha_{0} + \sum_{i = 1}^{p}{\alpha_{i}\mathrm{\Delta}y_{t - i} +}\sum_{i = 1}^{q_{1}}{\beta_{1,i}\mathrm{\Delta}x_{1,t - i} +}\ldots + \sum_{i = 1}^{q_{K}}{\beta_{K,i}\mathrm{\Delta}x_{K,t - i} -  }\]</span> <span class="math display">\[\mathrm{-\lambda{\widehat{u}}_{t - 1} + \gamma_{1}z_{1t} + \ldots + \gamma_{m}z_{mt} + e_{t}}\mathbf{\ }\]</span></p>
<p>Una condición necesaria para que exista cointegración, es decir, una relación estable a largo plazo entre la variable <em>y</em> y las <em>x</em>’s, es que el coeficiente de ajuste <span class="math inline">\(\lambda\)</span> sea estadísticamente significativo; si <span class="math inline">\(\lambda = 0\)</span> entonces no hay ajuste y la relación a largo plazo no tiene sentido, sino sólo la de corto plazo. Solo en el caso de que exista cointegración podrá hablarse de forma fiable de las relaciones a corto y a largo plazo y del correspondiente MCE, que es el que “guía” el movimiento simultáneo de las variables del sistema hacia su posición de equilibrio a largo plazo.</p>
<p>Técnicamente la forma correcta de determinar si las variables están cointegradas es realizar el correspondiente test de la hipótesis de estacionariedad de los residuos <span class="math inline">\({\widehat{u}}_{t}\)</span>, lo que debe hacerse a través de los llamados <em>contrastes de raíces unitarias</em>. Uno de estos contrastes es el llamado <em>test de Dickey-Fuller aumentado</em>, que consiste en estimar por el método MCO la regresión auxiliar</p>
<p><span class="math display">\[\mathrm{\Delta}{\widehat{u}}_{t} = \phi_{0}{\widehat{u}}_{t - 1} + \sum_{i = 1}^{p}{\phi_{i}\mathrm{\Delta}{\widehat{u}}_{t - i} + \nu_{t}}\]</span></p>
<p>y comprobar si el estadístico <em>t</em><sub>0</sub> asociado al parámetro <span class="math inline">\({\widehat{\phi}}_{0}\)</span> resulta estadísticamente significativo. Para ello deben utilizarse valores críticos o <em>P</em>-valores específicos, puesto que la distribución del estadístico <em>t</em> bajo la hipótesis nula no es la <em>t</em> de Student convencional sino que sigue una distribución no estándar con valores críticos y probabilidades asociadas que dependen del tipo de regresión auxiliar realizada. A modo de ejemplo, el valor crítico al 5% de significación está entre −3.4 y −3.2, mucho más negativo que el valor crítico estándar de la <em>t</em> de Student asociado a la hipótesis nula de que <span class="math inline">\(\phi_{0} = 0\)</span>, cercano a −1.7.</p>
</section>
<section id="modelos-con-dinámica-en-el-espacio" class="level4" data-number="3.6.3.10">
<h4 data-number="3.6.3.10" class="anchored" data-anchor-id="modelos-con-dinámica-en-el-espacio"><span class="header-section-number">3.6.3.10</span> Modelos con dinámica en el espacio</h4>
<p>Supongamos que, en lugar de partir de un modelo de regresión con autocorrelación espacial de primer orden en los errores, se propone un <em>modelo con retardo espacial en la variable endógena</em>, el llamado modelo <em>SAR</em>:</p>
<p><span class="math display">\[\mathbf{y} = \rho\mathbf{Wy} + \mathbf{X\beta} + \mathbf{e}\]</span></p>
<p>siendo <span class="math inline">\(\mathbf{W}\)</span> la matriz de pesos espaciales, <span class="math inline">\(\rho\)</span> el parámetro que mide la intensidad de la correlación espacial en la variable dependiente y <span class="math inline">\(\mathbf{e} \approx N(\mathbf{0}\mathbf{,}\sigma^{2}\mathbf{I})\)</span>.</p>
<p>En estos casos, igual que ocurre con los modelos espaciales <em>SEM</em> para los errores, debe utilizarse el estimador de máxima verosimilitud, que cumple las propiedades de consistencia, eficiencia y normalidad asintótica; además, en este caso el estimador MCO no solo pierde fiabilidad, sino que además es sesgado e inconsistente, por lo que resulta obligatoria su sustitución por el estimador MV.</p>
</section>
</section>
</section>
<section id="información-muestral" class="level2" data-number="3.7">
<h2 data-number="3.7" class="anchored" data-anchor-id="información-muestral"><span class="header-section-number">3.7</span> Información muestral</h2>
<section id="falta-de-observaciones" class="level3" data-number="3.7.1">
<h3 data-number="3.7.1" class="anchored" data-anchor-id="falta-de-observaciones"><span class="header-section-number">3.7.1</span> Falta de observaciones</h3>
<p>Es frecuente en el trabajo econométrico encontrarse con el problema de la falta de una o varias observaciones muestrales (<em>missing data</em>) para alguna de las variables del modelo, bien sea la endógena, o alguna(s) de las explicativas. Esta ausencia debe ser de tipo aleatorio, pues en caso contrario estaríamos ante un problema de selección muestral, el cual se tratará con posterioridad.</p>
<p>En general, hay que señalar que no se da el mismo tratamiento al problema de falta de información muestral si se trabaja con datos de corte transversal o de series temporales. Entre los diferentes métodos que se han propuesto en la literatura para tratar el mismo podemos mencionar los siguientes:</p>
<ul>
<li><p><em>MCO de la muestra truncada</em>. Las estimaciones se obtienen solamente con las observaciones completas. Esta solución supone pérdida de observaciones y de grados de libertad, pero el estimador MCO sigue teniendo todas las propiedades habituales, siempre y cuando los “huecos” en los datos no sean sistemáticos, sino aleatorios (sin causa específica alguna), y la muestra truncada siga siendo representativa de la población investigada.</p></li>
<li><p><em>Regresión de orden cero</em>. Se substituyen los valores perdidos de cada de una de las variables por su media (la de las observaciones completas). Si denotamos por (<em><strong>y</strong><sub>A</sub></em>, <em><strong>X</strong><sub>A</sub></em>) las observaciones completas, por (<em><strong>y</strong><sub>B</sub></em>, <em><strong>X</strong><sub>B</sub></em>) las observaciones para las cuales faltan los valores de <em>y</em> pero no de las <em>x</em>’s, y por (<em><strong>y</strong><sub>C</sub></em>, <em><strong>X</strong><sub>C</sub></em>) las observaciones para las cuales faltan los valores de las <em>x</em>’s pero no de <em>y</em>, se procede del siguiente modo: (1) se substituye <em><strong>y</strong><sub>B</sub></em> por la media de las observaciones <em><strong>y</strong><sub>A</sub></em> e <em><strong>y</strong><sub>C</sub></em>, (2) se substituye <em><strong>X</strong><sub>C</sub></em> por la media de las observaciones <em><strong>X</strong><sub>A</sub></em> y <em><strong>X</strong><sub>B</sub></em> y (3) se aplican MCO para todas las observaciones, las completas y las “completadas”.</p></li>
<li><p><em>Regresión de primer orden</em>. Se realiza la regresión de <em>y</em> sobre las <em>x</em>’s con las observaciones completas (<em><strong>y</strong><sub>A</sub></em>, <em><strong>X</strong><sub>A</sub></em>) y también de cada <em>x</em> sobre un conjunto de regresores <em>z</em>’s que se considere oportuno, <em><strong>Z</strong><sub>A</sub></em>, y se substituyen los valores perdidos por los correspondientes valores de predicción. Por último, se lleva a cabo la regresión con todos los datos, completos y completados.</p></li>
<li><p>Otros métodos de imputación: máxima verosimilitud, variables ficticias, interpolación, etc.</p></li>
</ul>
<p>Puede decirse que, en términos generales, no se conocen bien las propiedades estadísticas de los estimadores que se obtienen con los distintos métodos de imputación propuestos en la literatura, aunque se ha demostrado que la mayoría de ellos reportan pocos beneficios, cuando no empeoran las propiedades básicas, como por ejemplo la insesgadez, respecto a la opción de utilizar solo las observaciones completas (muestra truncada).</p>
</section>
<section id="agregación-de-datos" class="level3" data-number="3.7.2">
<h3 data-number="3.7.2" class="anchored" data-anchor-id="agregación-de-datos"><span class="header-section-number">3.7.2</span> Agregación de datos</h3>
<p>Consideremos el modelo de regresión estándar, <span class="math inline">\(y_{i} = \beta_{1} + \beta_{2}x_{2i} + \ldots + \beta_{K}x_{Ki} + e_{i}\)</span>. Si la muestra inicial era de <em>n</em> observaciones, a veces ocurre que no se dispone de los datos originales sino de <em>datos elaborados</em> por agregación a partir de la información de origen (por ejemplo, para guardar el anonimato de los informantes, por problemas de coste, etc.), como pueden ser los valores medios o totales correspondientes a <em>g</em> grupos de <span class="math inline">\(n_{1},\ldots,n_{g}\)</span> observaciones cada uno de ellos.</p>
<p>En esta situación no podemos aplicar directamente los resultados conocidos de la estimación MCO, ya que dejan de satisfacerse algunas de las hipótesis básicas.</p>
<p>Supongamos que disponemos de los valores medios, <span class="math inline">\({\overline{y}}_{i},{\overline{x}}_{2i},\ldots,{\overline{x}}_{Ki}\text{\ \ \ }i = 1,\ldots,g\)</span>, en lugar de los datos originales <span class="math inline">\(y_{i},x_{2i},\ldots,x_{Ki}\text{\ \ \ }i = 1,\ldots,n\)</span>. En este caso, la ecuación a estimar será</p>
<p><span class="math display">\[\ {\overline{y}}_{i} = \beta_{1} + \beta_{2}{\overline{x}}_{2i} + \ldots + \beta_{K}{\overline{x}}_{Ki} + {\overline{e}}_{i}\ \]</span></p>
<p>que, como puede apreciarse, tiene los mismos parámetros que el modelo original, aunque se tiene que <span class="math inline">\(\ {\overline{e}}_{i} = \frac{\sum_{i}^{}e_{ij}}{n_{i}}\ \)</span> y, por tanto, <span class="math inline">\(\ Var\left( {\overline{e}}_{i} \right) = \frac{\sigma^{2}}{n_{i}}\ \)</span>.</p>
<p>Es decir, que en el modelo con datos agrupados no se satisface la hipótesis de homoscedasticidad, sino que, por el contrario, existe heteroscedasticidad con una forma concreta. Sin embargo, en esta situación particular, el problema puede resolverse fácilmente aplicando el método de mínimos cuadrados ponderados, utilizando como ponderaciones los valores <span class="math inline">\(\ p_{i} = \sqrt{n_{i}}\ \)</span>.</p>
</section>
<section id="multicolinealidad" class="level3" data-number="3.7.3">
<h3 data-number="3.7.3" class="anchored" data-anchor-id="multicolinealidad"><span class="header-section-number">3.7.3</span> Multicolinealidad</h3>
<section id="naturaleza-y-causas-de-la-multicolinealidad" class="level4" data-number="3.7.3.1">
<h4 data-number="3.7.3.1" class="anchored" data-anchor-id="naturaleza-y-causas-de-la-multicolinealidad"><span class="header-section-number">3.7.3.1</span> Naturaleza y causas de la multicolinealidad</h4>
<p>Se utiliza el término <em>multicolinealidad exacta</em> para designar la situación que se da cuando las variables explicativas de un modelo de regresión son linealmente dependientes, es decir, ∃<em>λ</em><sub>1</sub>,…,<em>λ<sub>K</sub></em> no todos nulos tales que</p>
<p><span class="math display">\[\lambda_{1}\mathbf{x}_{1} + \lambda_{2}\mathbf{x}_{2} + \ldots + \lambda_{K}\mathbf{x}_{K} = \mathbf{0}\]</span></p>
<p>Como consecuencia, el rango de la matriz <span class="math inline">\(\mathbf{X}\)</span> de datos de las variables explicativas (y, por tanto, de la matriz <span class="math inline">\(\mathbf{X}'\mathbf{X}\)</span>) es menor que <em>K</em> y, por consiguiente, el sistema <span class="math inline">\(\left( \mathbf{X}'\mathbf{X} \right)\mathbf{\beta} = \mathbf{X}'\mathbf{y}\)</span> no tiene solución única [puesto que <span class="math inline">\((\mathbf{X}'\mathbf{X})^{- 1} =\frac{1}{det(\mathbf{X}'\mathbf{X})}adj(\mathbf{X}'\mathbf{X})^{'}\)</span>, si el determinante de la matriz <span class="math inline">\(\mathbf{X}'\mathbf{X}\)</span> es cero, no existe la matriz inversa]. En consecuencia, no existe un único estimador MCO y, además, la varianza de las estimaciones será “infinita”.</p>
<p>Si la dependencia lineal no es exacta, pero se encuentra muy próxima a ella, hablaremos de <em>multicolinealidad aproximada</em>, o simplemente de multicolinealidad, teniéndose entonces que ∃ λ<sub>1</sub>,…,λ<em><sub>K</sub></em> no todos nulos, tales que</p>
<p><span class="math display">\[\lambda_{1}\mathbf{x}_{1} + \lambda_{2}\mathbf{x}_{2} + \ldots + \lambda_{K}\mathbf{x}_{K} \cong \mathbf{0}\]</span></p>
<p>En este caso, en lugar de cumplirse que <span class="math inline">\(det(\mathbf{X}^{\mathbf{'}}\mathbf{X}) = 0\)</span>, como en el caso de multicolinealidad exacta, este determinante tendrá valores muy próximos a cero:</p>
<p><span class="math display">\[\det(\mathbf{X}^{\mathbf{'}}\mathbf{X}) \cong \mathbf{0}\]</span></p>
<p>Como en la práctica el caso de multicolinealidad exacta no se observa salvo en circunstancias excepcionales (como el caso de la ‘trampa de las variables ficticias’ tratado en el capítulo tres), en lo que sigue trataremos solo el caso aproximado, utilizando para esta situación el término multicolinealidad, a secas.</p>
<p>En cuanto a las causas de la multicolinealidad, en general podemos afirmar que la colinealidad es un problema de los datos y no del modelo, siendo la norma —más que la excepción— cuando se trabaja con variables económicas temporales. Algunos de los motivos por los que la multicolinealidad puede aparecer son los siguientes:</p>
<ul>
<li><p>Comportamiento tendencial en el tiempo de las series económicas.</p></li>
<li><p>Sobredeterminación: la misma información se encuentra en diferentes variables explicativas de un mismo modelo que, por tanto, estarán fuertemente correlacionadas.</p></li>
<li><p>Métodos de obtención de la información.</p></li>
<li><p>Especificación del modelo: aparecen como regresores diferentes potencias de una misma variable explicativa o se usan múltiples variables ficticias.</p></li>
</ul>
</section>
<section id="consecuencias-de-la-multicolinealidad" class="level4" data-number="3.7.3.2">
<h4 data-number="3.7.3.2" class="anchored" data-anchor-id="consecuencias-de-la-multicolinealidad"><span class="header-section-number">3.7.3.2</span> Consecuencias de la multicolinealidad</h4>
<p>Cuando existe multicolinealidad se originan los problemas siguientes:</p>
<ul>
<li><p>Aunque el estimador MCO siguen siendo insesgado y consistente, existe una pérdida importante de precisión en las estimaciones ya que, al venir la matriz de covarianzas del estimador dada por <span class="math inline">\(Cov({\widehat{\mathbf{\beta}}}_{MCO}) = \sigma^{2}(\mathbf{X}'\mathbf{X})^{- 1}\)</span>, las varianzas <span class="math inline">\(Var({\widehat{\beta}}_{i})\)</span>y covarianzas <span class="math inline">\(Cov({\widehat{\beta}}_{i},{\widehat{\beta}}_{j})\)</span> tomarán valores muy elevados, puesto que en el cálculo de la matriz <span class="math inline">\((\mathbf{X}'\mathbf{X})^{- 1}\)</span> aparece como divisor el número <span class="math inline">\(det(\mathbf{X}'\mathbf{X})\)</span>. Por tanto, existirán en general pocos coeficientes significativos (los estadísticos <em>t</em> serán bajos), aunque pueda observarse un <em>R</em><sup>2</sup> elevado. Además, los intervalos de confianza serán anormalmente amplios, lo que implica una sobre-tendencia a aceptar cualquier hipótesis nula.</p></li>
<li><p>Existen, entonces, ‘sesgos’ en el contraste de hipótesis paramétricas, en el sentido de que cada contraste estadístico será menos preciso que si no hubiese colinealidad entre las variables explicativas.</p></li>
<li><p>Pequeñas variaciones en los datos pueden dar lugar a grandes variaciones en las estimaciones, es decir, las estimaciones se vuelven muy inestables.</p></li>
<li><p>No se pueden separar los efectos individuales de las variables y, por tanto, se hace difícil utilizar el modelo como una herramienta para el análisis estructural. Sin embargo, no se ven afectadas las capacidades ni explicativa ni predictiva del modelo.</p></li>
</ul>
</section>
<section id="detección-de-la-multicolinealidad" class="level4" data-number="3.7.3.3">
<h4 data-number="3.7.3.3" class="anchored" data-anchor-id="detección-de-la-multicolinealidad"><span class="header-section-number">3.7.3.3</span> Detección de la multicolinealidad</h4>
<p>Se puede decir que, en economía, dada la naturaleza no experimental de los datos, siempre existe algún grado de colinealidad entre las variables de un modelo de regresión. Por tanto, el problema de la detección de la multicolinealidad no consiste en analizar su existencia, sino en determinar cuándo su nivel de presencia puede ser suficientemente alto como para perjudicar el proceso inferencial asociado a la estimación de un modelo.</p>
<p>Pueden considerarse como indicios de multicolinealidad los siguientes fenómenos:</p>
<ul>
<li><p>Pequeñas variaciones en la información muestral dan lugar a grandes fluctuaciones en las estimaciones de los parámetros y en los errores estándar estimados.</p></li>
<li><p>Los coeficientes tienen errores estándar elevados y niveles de significación estadística bajos (<em>t</em>-ratios cercanos a cero), aún cuando todas las variables resulten conjuntamente significativas (test <em>F</em>) y el <em>R</em><sup>2</sup> sea elevado.</p></li>
<li><p>Los coeficientes estimados pueden tomar valores anormales en tamaño o incluso signos opuestos al esperado.</p></li>
<li><p>Se observan altas correlaciones* parciales, <span class="math inline">\(r_{ij} = Corr(x_{i},x_{j})\ \)</span>, entre las variables explicativas del modelo. Se considera que valores superiores a 0.8 para datos de series temporales, o mayores que 0.5 para datos de corte transversal, señalan un nivel de multicolinealidad importante.</p></li>
<li><p>Por otra parte, se puede demostrar que <span class="math display">\[Var({\widehat{\beta}}_{j}) = \frac{{\widehat{\sigma}}^{2}}{S_{jj}}\left( \frac{1}{1 - R_{j}^{2}} \right)\]</span> donde $R_{j}^{2}$ es el coeficiente de determinación de la regresión de la variable <em>x<sub>j</sub></em> frente al resto de variables explicativas del modelo y <span class="math inline">\(S_{jj} = \sum_{i}^{}{(x_{ij} - {\overline{x}}_{j})^{2}}\)</span>. Por tanto, valores altos de los coeficientes <span class="math inline">\(R_{j}^{2}\)</span> están asociados con valores elevados de las varianzas estimadas. Normalmente se utilizan, en lugar de los <span class="math inline">\(R_{j}^{2}\)</span>, los llamados <em>factores de inflación de la varianza</em> (<em>FIV</em>), que se definen como <span class="math display">\[FIV_{j} = \frac{1}{1 - R_{j}^{2}}\]</span>considerándose como señal de multicolinealidad débil valores de los <em>FIV</em> inferiores a 5, moderada si los FIV están entre 5 y 10, y fuerte si existen valores por encima de 10 (lo que equivale a que los <span class="math inline">\(R_{j}^{2}\)</span> sean superiores a 0.9).</p></li>
</ul>
</section>
<section id="corrección-de-multicolinealidad" class="level4" data-number="3.7.3.4">
<h4 data-number="3.7.3.4" class="anchored" data-anchor-id="corrección-de-multicolinealidad"><span class="header-section-number">3.7.3.4</span> Corrección de multicolinealidad</h4>
<p>En general, podemos afirmar que la multicolinealidad es la norma, más que la excepción, cuando se trabaja con datos de series temporales. Además, se puede decir que, dada la naturaleza no experimental de los datos económicos, siempre existe algún grado de colinealidad entre las variables explicativas de un modelo de regresión.</p>
<p>La cuestión que planteamos en este apartado es saber qué hacer cuando el nivel de multicolinealidad es tan alto como para perjudicar al proceso inferencial asociado a un modelo de regresión.</p>
<p>Pueden considerarse distintas posibilidades:</p>
<ul>
<li><p>Uso de <em>información a priori</em> o extramuestral (de trabajos empíricos anteriores o de la propia teoría económica). Un ejemplo de este tipo de solución puede ser la incorporación de información transversal en el modelo temporal, o la imposición de restricciones paramétricas en el proceso de estimación. Sin embargo, si las restricciones son falsas, puede generarse aún más ineficiencia que con la sola presencia de la multicolinealidad.</p></li>
<li><p><em>Eliminación de algún regresor</em> fuertemente correlacionado con alguna(s) de las restantes variables explicativas. En este caso, “puede ser peor el remedio que la enfermedad” ya que pueden aparecer problemas de mala especificación funcional derivados de la exclusión de alguna variable relevante (sesgo e inconsistencia, no solo imprecisión).</p></li>
<li><p><em>Transformar las variables del modelo</em>, por ejemplo, tomando primeras diferencias, ha sido hasta hace pocos años una práctica rutinaria para eliminar la tendencia en las series temporales económicas y, por tanto, para aliviar los problemas de multicolinealidad que provoca su presencia. Sin embargo, esta costumbre genera nuevos problemas como la presencia de autocorrelación en los errores o, lo que es más importante, la pérdida de información valiosa sobre la relación a largo plazo entre las variables del modelo.</p></li>
<li><p><em>Utilización de métodos de regularización</em>, lo que conduce al estimador <em>ridge</em> (también llamado estimador “cresta”) o al estimador <em>lasso</em>, entre otros.</p>
<p>El <em>estimador ridge</em> se obtiene el vector de parámetros que minimiza la función</p>
<p><span class="math display">\[{\ \mathbf{(}\mathbf{y}\mathbf{-}\mathbf{X\beta}\mathbf{)'(}\mathbf{y}\mathbf{-}\mathbf{X\beta}\mathbf{)}}{\  + r\sum_{j}^{}\beta_{j}^{2}}\]</span></p>
<p>para algún valor <span class="math inline">\(r \geq 0\)</span>. Gráficamente, el estimador <em>ridge</em> se obtiene expandiendo las elipses centradas en el estimador MCO hasta lograr la intersección con el círculo que representa el espacio de parámetros asociado a la restricción de penalización <span class="math inline">\(\sum_{j}^{}\beta_{j}^{2} \leq t^{2}\)</span> (el valor <em>t</em> juega el mismo papel que <span class="math inline">\(r\)</span>).</p>
<p>El vector que resuelve el problema de minimización penalizada propuesto viene dado por la expresión <span class="math display">\[{\widehat{\mathbf{\beta}}}_{ridge} = (\mathbf{X}'\mathbf{X} + r\mathbf{I})^{- 1}\mathbf{X}'\mathbf{y}\]</span></p>
<p>pudiendo comprobarse que para <em>r</em> = 0 el estimador <em>ridge</em> coincide con el estándar MCO. Aunque <span class="math inline">\({\widehat{\mathbf{\beta}}}_{ridge}\)</span> es esgado, se puede demostrar que su matriz de covarianzas</p>
<p><span class="math display">\[Cov({\widehat{\mathbf{\beta}}}_{ridge}) = \sigma^{2}(\mathbf{X}'\mathbf{X} + r\mathbf{I})^{- 1}\mathbf{X}'\mathbf{X}(\mathbf{X}'\mathbf{X} + r\mathbf{I})^{- 1}\]</span></p>
<p>“es menor” que la del estimador MCO, en el sentido de que el error cuadrático medio (ECM) a que da lugar es inferior. Por tanto, el aumento del sesgo puede quedar compensado con la menor varianza si se tiene en cuenta el criterio del error cuadrático medio en lugar del criterio del sesgo.</p>
<p>El <em>estimador lasso</em> , <span class="math inline">\({\widehat{\mathbf{\beta}}}_{lasso}\)</span>, se obtiene al minimizar la función</p>
<p><span class="math display">\[{\ \mathbf{(}\mathbf{y}\mathbf{-}\mathbf{X\beta}\mathbf{)'(}\mathbf{y}\mathbf{-}\mathbf{X\beta}\mathbf{)}}{\  + r\sum_{j}^{}\left| \beta_{j} \right|}\]</span></p>
<p>para alguna elección de <span class="math inline">\(r \geq 0\)</span>. En este caso, la solución (que no tiene una fórmula explícita para el problema de optimización planteado) se obtiene en el punto de intersección de las elipses entradas en el estimador MCO con el cuadrado asociado a la restricción <span class="math inline">\(\sum_{j}^{}\left| \beta_{j}\  \right| \leq t\)</span>.</p></li>
<li><p>Uso de métodos multivariantes de reducción-contracción (<em>shrinkage</em>), en particular el <em>análisis de componentes principales</em> (PCA en inglés) o el método de <em>mínimos cuadrados parciales</em> (PLS en inglés). Estas técnicas consisten en reducir la matriz original de datos <span class="math inline">\(\mathbf{X}\)</span>, de orden <span class="math inline">\(n \times K\)</span>, a una matriz <span class="math inline">\(\mathbf{Z}\)</span>, de orden <span class="math inline">\(n \times M\)</span> con <em>M</em> &lt; <em>K</em>, en la que las columnas de la nueva matriz sean <em>ortogonales</em> entre sí (multicolinealidad cero). La diferencia entre ellas radica en que mientras el PCA ignora la variable dependiente a la hora de buscar la matriz <span class="math inline">\(\mathbf{Z}\)</span>, el método PLS explícitamente elige las <em>z</em>’s para predecir la variable respuesta <em>y</em> lo mejor posible. Aplicando cualquiera de estos métodos se pasa, por tanto, del modelo original, <span class="math inline">\(\mathbf{y} = \mathbf{X\beta} + \mathbf{e}\)</span>, a otro modelo, <span class="math inline">\(\mathbf{y} = \mathbf{Z\gamma} + \mathbf{e}\)</span>, en el que ya no existe multicolinealidad. El inconveniente que se plantea es que no resulta fácil en muchas ocasiones la interpretación de las nuevas variables del modelo y menos aún la de los parámetros <em>γ</em> del modelo transformado.</p></li>
</ul>
</section>
</section>
</section>
<section id="regresores-endógenos-errores-de-medida-yo-simultaneidad-en-las-variables-del-modelo" class="level2" data-number="3.8">
<h2 data-number="3.8" class="anchored" data-anchor-id="regresores-endógenos-errores-de-medida-yo-simultaneidad-en-las-variables-del-modelo"><span class="header-section-number">3.8</span> Regresores endógenos: errores de medida y/o simultaneidad en las variables del modelo</h2>
<p>Hasta el momento se ha supuesto que, a la hora de realizar un trabajo econométrico empírico, se dispone de los verdaderos valores de las variables. Sin embargo, es frecuente que existan, por diferentes motivos (ajenos o intrínsecos a la investigación), errores de medida en las observaciones de las variables que pueden inducir sesgos en las estimaciones MCO que pueden ser altos. Un ejemplo de tal situación es el que produce el empleo de variables aproximadas (<em>proxy</em>) para cuantificar conceptos (variables latentes) sobre los que no se dispone de información numérica (inteligencia, habilidad, nivel de educación, etc.).</p>
<section id="errores-de-medida-en-la-variable-dependiente" class="level3" data-number="3.8.1">
<h3 data-number="3.8.1" class="anchored" data-anchor-id="errores-de-medida-en-la-variable-dependiente"><span class="header-section-number">3.8.1</span> Errores de medida en la variable dependiente</h3>
<p>Consideremos, sin pérdida de generalidad, un modelo de regresión teórico dado por <span class="math inline">\(y_{i} = \beta_{1} + \beta_{2}x_{i} + e_{i}\)</span>. Si medimos la variable <em>y</em> con error, no dispondremos de los verdaderos valores <span class="math inline">\(y_{i}\)</span>, sino de los valores observados <span class="math inline">\(y_{i}^{*} = y_{i} + u_{i}\)</span>, donde <span class="math inline">\(u_{i}\)</span> es el error de medida. Entonces</p>
<p><span class="math display">\[\left\{ \begin{matrix} y_{i} = \beta_{1} + \beta_{2}x_{i} + e_{i} \\ y_{i}^{*} = y_{i} + u_{i} \\ \end{matrix} \right.\ \]</span></p>
<p>y, como consecuencia, el modelo que puede ser estimado será</p>
<p><span class="math display">\[y_{i}^{*} = \beta_{1} + \beta_{2}x_{i} + v_{i}\]</span></p>
<p>donde <span class="math inline">\(\nu_{i}^{} = e_{i} + u_{i}\)</span>.</p>
<p>En este nuevo modelo los parámetros estructurales son los mismos, pero la perturbación aleatoria, incrementada ahora con el error de medida en <em>y</em>, tendrá una varianza dada por <em>Var</em>(<em>v<sub>i</sub></em>)<em>=Var</em>(<em>e<sub>i</sub></em>)<em>+Var</em>(<em>u<sub>i</sub></em>)<em>+</em>2<em>Cov</em>(<em>e<sub>i</sub></em>,<em>u<sub>i</sub></em>).</p>
<p>Si se supone que no existe correlación entre el error de medida y la perturbación, <em>Cov</em>(<em>e<sub>i</sub></em>,<em>u<sub>i</sub></em>)=0, se tendrá entonces que <span class="math inline">\(Var(\nu_{i}) = \sigma^{2} + \sigma_{u}^{2}\)</span>.</p>
<p>Por tanto, en presencia de errores de medida en la variable dependiente, el estimador MCO sigue siendo insesgado y consistente, si bien las varianzas estimadas serán mayores que cuando no existen estos errores. En consecuencia, los ajustes del modelo serán en general peores y las estimaciones serán más imprecisas que en el caso ideal de ausencia de errores de medición en la variable a explicar.</p>
</section>
<section id="errores-de-medida-en-las-variables-explicativas" class="level3" data-number="3.8.2">
<h3 data-number="3.8.2" class="anchored" data-anchor-id="errores-de-medida-en-las-variables-explicativas"><span class="header-section-number">3.8.2</span> Errores de medida en las variables explicativas</h3>
<p>Si en el modelo de regresión <span class="math inline">\(y_{i} = \beta_{1} + \beta_{2}x_{i} + e_{i}\)</span> ahora fuese la variable explicativa <em>x</em> la que se midiese con error, se tendría que <span class="math inline">\(x_{i}^{*} = x_{i} + w_{i}\)</span> y, entonces,</p>
<p><span class="math display">\[\left\{ \begin{matrix} y_{i} = \beta_{1} + \beta_{2}x_{i} + e_{i} \\ x_{i}^{*} = x_{i} + w_{i} \\ \end{matrix} \right.\ \]</span></p>
<p>Por tanto, el modelo que se estimará en la práctica es</p>
<p><span class="math display">\[y_{i} = \beta_{1} + \beta_{2}x_{i}^{*} + v_{i}\]</span></p>
<p>donde <span class="math inline">\(\nu_{i} = e_{i} - \beta_{2}w_{i}\)</span></p>
<p>Se tiene pues que <span class="math inline">\(Var(\nu_{i}) = \sigma^{2} + \beta_{2}^{2}\sigma_{w}^{2}\)</span>, por lo que, de nuevo, las varianzas estimadas serán mayores que cuando no existen errores de medida: se tendrá un peor ajuste y una mayor imprecisión en las estimaciones. Pero, además, se cumple que <span class="math inline">\(Cov(x_{i}^{*},v_{i}) = Cov(x_{i} + w_{i},e_{i} - \beta_{2}w_{i}) = - \beta_{2}\sigma_{w}^{2} \neq 0\)</span>, implicando este resultado el incumplimiento de una de las hipótesis básicas del modelo de regresión lineal ya que, siendo ahora la variable explicativa <span class="math inline">\(x_{i}^{*}\)</span> estocástica, no se verifica que la covarianza de esta con el término de error <span class="math inline">\(\nu_{i}\)</span> sea nula y, por tanto, el estimador MCO será entonces sesgado e inconsistente.</p>
<p>En general, si en un modelo de regresión <span class="math inline">\(y_{i} = \beta_{1} + \beta_{2}x_{2i} + ... + \beta_{K}x_{Ki} + e_{i}\)</span> alguna de las variables explicativas se mide con error, se tendrá que <span class="math inline">\(x_{ji}^{*} = x_{ji} + w_{ji}\)</span> y, por tanto, el modelo que realmente se estimará viene dado por</p>
<p><span class="math display">\[y_{i} = \beta_{1} + \beta_{2}x_{2i} + \ldots + \beta_{j}x_{ji}^{*} + \ldots + \beta_{K}x_{Ki} + \nu_{i}\]</span></p>
<p>donde el término de error tomará la expresión <span class="math inline">\(\nu_{i} = e_{i} - \beta_{j}w_{ji}\)</span>.</p>
<p>En este caso se cumple que <span class="math inline">\(Cov(x_{ji}^{*},v_{i}) = - \beta_{j}\sigma_{jw}^{2} \neq 0\)</span>, lo que implica la violación de una de las hipótesis básicas del modelo de regresión lineal ya que, siendo ahora la variable explicativa <span class="math inline">\(x_{j}^{*}\)</span> estocástica, su covarianza con el término de error <span class="math inline">\(\nu_{i}\)</span> no es nula y, por tanto, el estimador MCO será sesgado e inconsistente.</p>
</section>
<section id="modelos-de-múltiples-ecuaciones-el-problema-de-la-simultaneidad" class="level3" data-number="3.8.3">
<h3 data-number="3.8.3" class="anchored" data-anchor-id="modelos-de-múltiples-ecuaciones-el-problema-de-la-simultaneidad"><span class="header-section-number">3.8.3</span> Modelos de múltiples ecuaciones: el problema de la simultaneidad</h3>
<p>Un <em>modelo de ecuaciones simultáneas</em> es un conjunto de ecuaciones de regresión e identidades matemáticas que representan las relaciones de un sistema económico. Este conjunto de relaciones se conoce como <em>forma o ecuaciones estructurales del sistema</em>, ya que recogen de modo directo las relaciones entre las variables del sistema económico bajo estudio.</p>
<p>Un ejemplo de sistema multiecuacional puede ser un modelo Keynesiano simple para explicar el consumo agregado de una economía cerrada, el cual viene representado por las dos ecuaciones estructurales siguientes (como puede observarse, la segunda es una identidad, sin parámetros a estimar):</p>
<p><span class="math display">\[\left\{ \begin{matrix} C_{t} = \beta_{1} + \beta_{2}Y_{t} + e_{t} \\ Y_{t} = C_{t} + I_{t} \\ \end{matrix} \right.\ \]</span></p>
<p>donde <em>C</em> es el consumo, <em>Y</em> la renta e <em>I</em> la inversión (privada y pública). Las variables <em>C</em> e <em>Y</em> son las variables endógenas del sistema, cuyos valores se determinan de forma conjunta e interdependiente en el modelo, y la variable <em>I</em> es la variable exógena, que se determina fuera del sistema e independientemente de <em>C</em>, <em>Y</em> y del término de error del modelo, <em>e</em>.</p>
<p>Suponiendo un hipotético equilibrio para <em>C<sup>*</sup></em> e <em>Y<sup>*</sup></em> en un determinado instante de tiempo <em>t</em>, dado un cierto valor de la inversión, <em>I<sup>*</sup></em>, cada vez que se produzcan cambios en la inversión, también se producirán variaciones en la renta de equilibrio <em>Y<sup>*</sup></em> y, por tanto, a través de la función de consumo se tendrá un nuevo valor de equilibrio para <em>C<sup>*</sup>.</em></p>
<p>Otro ejemplo de sistema con múltiples ecuaciones viene dado por un modelo básico para el análisis del mercado en equilibrio de un determinado bien, en el que las ecuaciones de demanda y oferta, y la relación de equilibrio, se expresan como</p>
<p><span class="math display">\[\left\{ \begin{matrix}
Q_{t}^{d} = \beta_{1} + \beta_{2}P_{t} + \beta_{3}Y_{t} + e_{1t} \\
Q_{t}^{s} = \gamma_{1} + \gamma_{2}P_{t} + \gamma_{3}PF_{t} + e_{2t} \\
Q_{t}^{d} = Q_{t}^{s} \\
\end{matrix} \right.\ \]</span></p>
<p>donde <em>Q<sup>d</sup></em>, <em>Q<sup>s</sup></em>, <em>P</em>, <em>Y</em> y <em>PF</em> son, respectivamente, las cantidades demandada y ofertada, el precio, la renta disponible (o el gasto total) y el precio de los factores de producción.</p>
<p>En este modelo, los valores de equilibrio <em>Q<sup>*</sup></em> y <em>P<sup>*</sup></em> en cada instante de tiempo <em>t</em> se determinan dentro del sistema, de forma simultánea y endógena, mientras que los valores de <em>Y<sup>*</sup></em> y <em>PF<sup>*</sup></em> se determinan fuera del mismo, exógenamente.</p>
<p>Supongamos que se intentase aplicar el método de mínimos cuadrados ordinarios para estimar la función Keynesiana de consumo de un país, o para estimar las funciones de demanda y oferta asociadas al mercado en equilibrio de un determinado producto. Aparte del hecho de estar ignorando la información de las identidades macroeconómicas en ambos modelos, lo que ya de por sí supondría una pérdida de eficiencia en el estimador, se puede demostrar que los estimadores MCO de los parámetros estructurales de la función de consumo o de las funciones de demanda y oferta son sesgados e inconsistentes, pues se cumple que <span class="math inline">\(\ Cov\left( Y_{t},e_{t} \right) \neq 0\ \)</span> para la función de consumo y <span class="math inline">\(\ Cov\left( P_{t},e_{1t} \right) \neq 0\ \)</span> y <span class="math inline">\(\ Cov\left( P_{t},e_{2t} \right) \neq 0\ \)</span> para las funciones de demanda y oferta, respectivamente.</p>
</section>
<section id="el-caso-general-de-variables-explicativas-estocásticas-correlacionadas-con-la-perturbación" class="level3" data-number="3.8.4">
<h3 data-number="3.8.4" class="anchored" data-anchor-id="el-caso-general-de-variables-explicativas-estocásticas-correlacionadas-con-la-perturbación"><span class="header-section-number">3.8.4</span> El caso general de variables explicativas estocásticas correlacionadas con la perturbación</h3>
<p>El caso de errores de medida en las variables explicativas o la simultaneidad no son sino situaciones particulares del problema general de existencia de variables explicativas estocásticas correlacionadas con el término de error, hablándose entonces de <em>regresores endógenos</em>.</p>
<p>Puede demostrarse que, en el modelo de regresión <span class="math inline">\(\mathbf{y} = \mathbf{X\beta} + \mathbf{e}\)</span>, cuando las variables explicativas son estocásticas, si <span class="math inline">\(\mathbf{X}\)</span> y <span class="math inline">\(\mathbf{e}\)</span> están correlacionados (por tanto <span class="math inline">\(\ Cov\left( \mathbf{X},\mathbf{e} \right) \neq \mathbf{0}\ \)</span>), entonces el estimador MCO será sesgado e inconsistente. Es decir, para cada parámetro estructural <span class="math inline">\(\beta\)</span> se tiene que <span class="math inline">\(E(\widehat{\beta}) \neq \beta\)</span> y <span class="math inline">\(\lim_{n \rightarrow \infty}{E(\widehat{\beta}}) \neq \beta\)</span> y, por tanto, existe un sesgo en la estimación persistente, es decir, no desaparece aunque aumente el tamaño de la muestra.</p>
</section>
<section id="las-regresiones-con-variables-instrumentales" class="level3" data-number="3.8.5">
<h3 data-number="3.8.5" class="anchored" data-anchor-id="las-regresiones-con-variables-instrumentales"><span class="header-section-number">3.8.5</span> Las regresiones con variables instrumentales</h3>
<p>Ante la situación creada para el estimador MCO cuando los regresores están medidos con error o, de modo más general, cuando existen regresores estocásticos correlacionados con el término de error, se hace necesaria la construcción de un nuevo estimador con buenas propiedades. Un método de estimación que conduce a un estimador consistente para <strong><em>β</em></strong> es el conocido como <em>método de variables instrumentales (VI)</em>.</p>
<p>En términos intuitivos, este método consiste en ‘substituir’ las variables originales del modelo, <span class="math inline">\(\mathbf{X}\)</span>, por otro conjunto de variables (<em>instrumentos o variables instrumentales</em>), <span class="math inline">\(\mathbf{Z}\)</span>, que serán el nuevo conjunto de variables explicativas, de modo que dichas variables instrumentales cumplan que (a) tengan información común con las variables originales (correlacionadas con ellas) y (b) no estén correlacionadas (al menos asintóticamente) con los errores del modelo.</p>
<section id="el-test-de-hausman-de-endogeneidad" class="level4" data-number="3.8.5.1">
<h4 data-number="3.8.5.1" class="anchored" data-anchor-id="el-test-de-hausman-de-endogeneidad"><span class="header-section-number">3.8.5.1</span> El test de Hausman de endogeneidad</h4>
<p>Hausman (1978) propuso una prueba para contrastar la inconsistencia del estimador MCO originada por la correlación contemporánea no nula entre la perturbación y alguno(s) de los regresores incluidos en la ecuación a estimar (problema de endogeneidad). Así, si se parte del modelo de regresión <span class="math inline">\(\mathbf{y} = \mathbf{X\beta} + \mathbf{e}\)</span>, el <em>test de Hausman</em> intenta contrastar</p>
<p><span class="math display">\[H_{0}:\{ Cov(\mathbf{X},\mathbf{e}) = \mathbf{0}\}\ \ frente\ a\ \ H_{1}:\{ Cov(\mathbf{X},\mathbf{e}) \neq \mathbf{0}\}\]</span></p>
<p>Básicamente, el estadístico propuesto compara las estimaciones MCO con las estimaciones VI para comprobar el grado de inconsistencia de las estimaciones mínimo-cuadráticas:</p>
<p><span class="math display">\[H = \ \left( {\widehat{\mathbf{\beta}}}_{\mathbf{VI}}\mathbf{-}{\widehat{\mathbf{\beta}}}_{\mathbf{MCO}} \right)^{'}\left\lbrack \mathbf{Cov}\left( {\widehat{\mathbf{\beta}}}_{\mathbf{VI}} \right)\mathbf{-}\mathbf{Cov}\left( {\widehat{\mathbf{\beta}}}_{\mathbf{MCO}} \right) \right\rbrack^{\mathbf{-}\mathbf{1}}({\widehat{\mathbf{\beta}}}_{\mathbf{VI}}\mathbf{-}{\widehat{\mathbf{\beta}}}_{\mathbf{MCO}})\]</span></p>
<p>Bajo la hipótesis nula de ausencia de correlación, <span class="math inline">\(H_{0}:\{ Cov(\mathbf{X},\mathbf{e}) = \mathbf{0}\}\)</span>, se cumple que <span class="math inline">\(H\overset{as}{\sim}\chi_{K_{1}}^{2}\ \)</span>, siendo <span class="math inline">\(K_{1}\)</span> el número de variables explicativas en la matriz <span class="math inline">\(\mathbf{X}\)</span> potencialmente correlacionadas con el término de error:</p>
<p>- Si no se rechaza la hipótesis nula, podremos confiar en la validez de la hipótesis de ausencia de correlación entre las variables explicativas y el término de error y seguir usando el estimador MCO, que mantendrá sus buenas propiedades (MELI, consistente y, además, eficiente si se verifica la hipótesis de normalidad de los errores).</p>
<p>- Por el contrario, si se rechaza la hipótesis nula, ello implicará la invalidez del estimador MCO (que es sesgado e inconsistente), que debe ser substituido por el estimador VI (que será consistente siempre que se verifiquen las condiciones de relevancia y exogeneidad de los instrumentos).</p>
</section>
<section id="el-estimador-vi-de-variables-instrumentales-mínimos-cuadrados-en-dos-etapas-mc2e" class="level4" data-number="3.8.5.2">
<h4 data-number="3.8.5.2" class="anchored" data-anchor-id="el-estimador-vi-de-variables-instrumentales-mínimos-cuadrados-en-dos-etapas-mc2e"><span class="header-section-number">3.8.5.2</span> El estimador VI de variables instrumentales (mínimos cuadrados en dos etapas, MC2E)</h4>
<p>Un procedimiento que conduce a un estimador consistente para <strong><em>β</em></strong> cuando <span class="math inline">\(\mathbf{X}\)</span> y <span class="math inline">\(\mathbf{e}\)</span> están correlacionados es el conocido como <em>método de estimación por variables instrumentales (VI)</em>. El método de VI consiste en encontrar una matriz <span class="math inline">\(\mathbf{Z}\)</span> de orden <span class="math inline">\(n \times L\)</span> (<span class="math inline">\(L \geq K\)</span>) que verifique las siguientes condiciones:</p>
<p>- <em>Relevancia de los instrumentos</em>: las variables contenidas en <span class="math inline">\(\mathbf{Z}\)</span> están correlacionadas con las de <span class="math inline">\(\mathbf{X}\)</span>: <span class="math inline">\(\ Cov\left( \mathbf{X},\mathbf{Z} \right) \neq 0\ \)</span>.</p>
<p>- <em>Exogeneidad de los instrumentos</em>: en el límite se debe cumplir que <span class="math inline">\(\ Cov\left( \mathbf{Z},\mathbf{e} \right) = \mathbf{0}\ \)</span>.</p>
<p>Si se encuentra una matriz <span class="math inline">\(\mathbf{Z}\)</span> que cumpla tales condiciones, se puede demostrar que el modelo transformado <span class="math inline">\(\mathbf{Z}^{\mathbf{'}}\mathbf{y} = \mathbf{Z}^{\mathbf{'}}\mathbf{X\beta} + \mathbf{Z}^{\mathbf{'}}\mathbf{e}\)</span> ya no “padece”, al menos asintóticamente, el problema de correlación no nula entre los regresores y el término de error, aunque este último es heteroscedástico, ya que se cumple que <span class="math inline">\(Var(\mathbf{Z}^{\mathbf{'}}\mathbf{e}) = \sigma^{2}(\mathbf{Z}^{\mathbf{'}}\mathbf{Z})\)</span>, por lo cual se requiere el uso del método de mínimos cuadrados ponderados para la estimación del nuevo modelo.</p>
<p>Ahora, la función a minimizar para obtener el correspondiente estimador MCP viene dada por <span class="math inline">\(S^{*}(\mathbf{b}) = (\mathbf{y} - \mathbf{Xb})'\mathbf{P}_{Z}(\mathbf{y} - \mathbf{Xb})\)</span>, donde <span class="math inline">\(\mathbf{P}_{Z} = \mathbf{Z}(\mathbf{Z}^{\mathbf{'}}\mathbf{Z})^{- 1}\mathbf{Z}^{\mathbf{'}}\)</span> es la matriz de ponderaciones. Se puede demostrar que el estimador que se obtiene en este caso, llamado <em>de variables instrumentales</em> (<em>VI</em>), tiene la expresión</p>
<p><span class="math display">\[{\widehat{\mathbf{\beta}}}_{VI} = (\mathbf{X}'\mathbf{P}_{Z}\mathbf{X})^{- 1}\mathbf{X}'\mathbf{P}_{Z}\mathbf{y}\]</span></p>
<p>y su matriz de varianzas-covarianzas viene dada por</p>
<p><span class="math display">\[\mathbf{\ \ }Cov\left( {\widehat{\mathbf{\beta}}}_{\mathbf{VI}} \right) = \sigma^{2}\left( \mathbf{X}^{'}\mathbf{P}_{Z}\mathbf{X} \right)^{- 1}\ \]</span></p>
<p>donde <span class="math inline">\(\sigma^{2}\)</span> puede estimarse de forma consistente mediante</p>
<p><span class="math display">\[{\widehat{\sigma}}_{VI}^{2} = (\mathbf{y} - \mathbf{X}{\widehat{\mathbf{\beta}}}_{VI})'(\mathbf{y} - \mathbf{X}{\widehat{\mathbf{\beta}}}_{VI})/(n - K)\]</span></p>
<p>El estimador VI, <span class="math inline">\(\ {\widehat{\mathbf{\beta}}}_{VI}\ \)</span>, sigue siendo sesgado, pero se puede demostrar que es consistente, aunque en general no será eficiente. Por otra parte, también puede demostrarse la distribución asintóticamente normal del mismo, siempre que se cumplan ciertas condiciones sobre el comportamiento de la matriz <span class="math inline">\(\mathbf{Z}\)</span> de instrumentos.</p>
<p>El estimador VI también puede verse como el resultado de aplicar MCO en dos pasos consecutivos razón por la que también se le conoce como estimador de <em>mínimos cuadrados en dos etapas (MC2E)</em>:</p>
<ul>
<li><p>En la primera etapa se “regresan” las variables contenidas en la matriz <span class="math inline">\(\mathbf{X}\)</span> sobre todas las variables instrumentales <span class="math inline">\(\mathbf{Z}\)</span>, <span class="math inline">\(\ \mathbf{X} = \mathbf{Z}\mathbf{\Pi} + \mathbf{u}\ \)</span>, y se calculan los valores ajustados de dichas regresiones, <span class="math inline">\(\ \widehat{\mathbf{X}}\ \)</span>.</p></li>
<li><p>En la segunda etapa se aplican MCO al modelo transformado <span class="math inline">\(\ \mathbf{y} = \widehat{\mathbf{X}}\mathbf{\beta} + \mathbf{e}\ \)</span>, cumpliéndose que <span class="math inline">\(\ {\widehat{\mathbf{\beta}}}_{VI} = \left( {\widehat{\mathbf{X}}}^{'}\widehat{\mathbf{X}} \right)^{- 1}\widehat{\mathbf{X}}\mathbf{y}\ \)</span>.</p></li>
</ul>
</section>
<section id="el-test-de-sargan-de-validez-de-los-instrumentos" class="level4" data-number="3.8.5.3">
<h4 data-number="3.8.5.3" class="anchored" data-anchor-id="el-test-de-sargan-de-validez-de-los-instrumentos"><span class="header-section-number">3.8.5.3</span> El test de Sargan de validez de los instrumentos</h4>
<p>Bajo la hipótesis nula de que los instrumentos son independientes del término de error, <span class="math inline">\(\ H_{0}:\left\{ Cov\left( \mathbf{Z},\mathbf{e} \right) = \mathbf{0} \right\}\)</span> (<em>condición de exogeneidad</em>), debería esperarse que la regresión del término de error <span class="math inline">\(\mathbf{e}\)</span> sobre las variables que componen la matriz <span class="math inline">\(\mathbf{Z}\)</span> tuviese un ajuste bajo en términos del coeficiente <em>R<sup>2</sup></em>; en caso contrario (cuando se acepta <span class="math inline">\(\ H_{1}:\left\{ Cov\left( \mathbf{Z},\mathbf{e} \right) \neq \mathbf{0} \right\}\ \)</span>), este coeficiente de determinación podría ser significativo.</p>
<p>El <em>test de Sargan</em> consiste en substituir los errores inobservables <span class="math inline">\(\mathbf{e}\)</span> por su contrapartida observable <span class="math inline">\({\widehat{\mathbf{e}}}_{VI}\)</span>, los residuos del modelo <span class="math inline">\(\mathbf{y} = \mathbf{X\beta} + \mathbf{e}\)</span> estimados por VI, y estimar por MCO la regresión auxiliar</p>
<p><span class="math display">\[\ {\widehat{\mathbf{e}}}_{VI} = \delta_{1}\mathbf{z}_{1} + \ldots + \delta_{L}\mathbf{z}_{L} + \mathbf{u}\ \]</span></p>
<p>cumpliéndose que, bajo la hipótesis nula <span class="math inline">\(H_{0}:\left\{ Cov\left( \mathbf{Z},\mathbf{e} \right) = \mathbf{0} \right\}\)</span>, el estadístico</p>
<p><span class="math display">\[S = (n - K)R^{2}\ \overset{as}{\sim\ }\chi_{r}^{2}\ \]</span></p>
<p>donde <span class="math inline">\(r = L - K\)</span> es el número de restricciones de sobre-identificación, que se obtiene restando al número de instrumentos en la matriz <span class="math inline">\(\mathbf{Z}\)</span>, <em>L</em>, el número de variables explicativas en la matriz <span class="math inline">\(\mathbf{X}\)</span>, <em>K</em>. Al valor <em>S</em> se le conoce comúnmente en la literatura econométrica como <em>estadístico J</em>.</p>
</section>
<section id="el-contraste-de-cragg-donald-de-debilidad-de-los-instrumentos" class="level4" data-number="3.8.5.4">
<h4 data-number="3.8.5.4" class="anchored" data-anchor-id="el-contraste-de-cragg-donald-de-debilidad-de-los-instrumentos"><span class="header-section-number">3.8.5.4</span> El contraste de Cragg-Donald de debilidad de los instrumentos</h4>
<p>Tal como se ha anotado en párrafos anteriores, cuando el estimador MCO no es válido por la presencia de correlación entre alguno(s) de los regresores y el término de error, ha de utilizarse como alternativa el estimador VI, cuyas propiedades dependen en buena medida de una elección correcta de los instrumentos, los cuales deben estar fuertemente correlacionados con los regresores originales y tener una correlación nula con el término de error.</p>
<p>Por un lado, los estadísticos <em>F</em> asociados a las hipótesis de significación de los parámetros de las variables <em>z</em>’s introducidas para instrumentar a las variables potencialmente endógenas (sin incluir las variables explicativas exógenas) en las regresiones de la primera etapa del método VI, <span class="math inline">\(x_{j} = \pi_{1}^{j}z_{1} + \ldots + \pi_{L}^{j}z_{L} + u_{j}\)</span>, permiten examinar la condición de relevancia de los instrumentos, <span class="math inline">\(Cov\left( \mathbf{X},\mathbf{Z} \right) \neq 0\)</span>: cuanto mayores sean los estadísticos <em>F</em> de significación conjunta de los parámetros <em>π</em>’s, los instrumentos serán más relevantes; y viceversa, cuanto menores sean los estadísticos <em>F</em>, más débiles serán los instrumentos propuestos, puesto que explicarán una pequeña proporción de la variación de cada <em>x<sub>j</sub></em> endógena.</p>
<p>Los estadísticos <em>F</em> de las regresiones auxiliares de la primera son sólo un indicador parcial de la fortaleza o debilidad de los instrumentos elegidos, pero no son un contraste en el sentido estadístico estricto. Cragg y Donald (1993) han propuesto un <em>contraste de identificación débil</em> que generaliza el método de los estadísticos <em>F</em> propuesto anteriormente y que permite detectar situaciones donde los instrumentos seleccionados tienen una correlación baja con los regresores endógenos de la regresión.</p>
<p>Dicho contraste está basado en la búsqueda de la menor correlación canónica (<span class="math inline">\(r_{B}\)</span>) entre los instrumentos ‘externos’ (aquellos no incluidos como variables exógenas en el modelo) y las variables explicativas endógenas, y posteriormente en el cálculo del estadístico</p>
<p><span class="math display">\[CD = \left\lbrack \frac{(n - G - L´)}{L´} \right\rbrack\left\lbrack \frac{r_{B}^{2}}{\left( 1 - r_{B}^{2} \right)} \right\rbrack\]</span></p>
<p>donde <em>n</em> es el tamaño muestral, <em>G</em> el número de regresores exógenos, <em>B</em> el número de regresores endógenos, y <em>L’</em> es el número de instrumentos externos. Este estadístico no sigue ninguna distribución conocida y, por tanto, deben utilizarse valores críticos específicos (que dependen de los valores <em>B</em> y <em>L’</em>), los cuales han sido tabulados por Stock y Yogo (2005).</p>
<p>Si no se rechaza la hipótesis nula, es decir, si los instrumentos son débiles, las consecuencias prácticas sobre el estimador VI puede ser importantes, haciendo el mismo poco fiable: por un lado, pueden producirse sesgos en las estimaciones VI en relación con el sesgo del estimador MCO; y, por otro lado, la tasa de rechazo puede verse incrementada, siendo el tamaño real de los contrastes (error tipo I) muy superior al nivel de rechazo nominal elegido a priori (por ejemplo, <span class="math inline">\(\alpha = 0.05\)</span>).</p>
<p>En este sentido, los valores críticos de Stock-Yogo son distintos según que se elija el sesgo relativo máximo o el tamaño del contraste máximo como criterio de valoración de la debilidad o fortaleza de los instrumentos (debe anotarse que no hay valores críticos si <span class="math inline">\(L´&lt; 3\)</span> para el criterio del sesgo relativo máximo y, por tanto, para esos casos sólo podrá valorarse si los instrumentos son débiles valorando la distorsión que se produce en el tamaño real de lo contrastes).</p>
</section>
</section>
</section>
<section id="observaciones-atípicas" class="level2" data-number="3.9">
<h2 data-number="3.9" class="anchored" data-anchor-id="observaciones-atípicas"><span class="header-section-number">3.9</span> Observaciones atípicas</h2>
<p>En el conjunto de datos disponible para estimar un modelo de regresión podemos encontrarnos, en muchas ocasiones, con observaciones atípicas que pueden corresponder a individuos o acontecimientos anómalos o de especial trascendencia. La característica principal de estas observaciones especiales es que pueden llegar a tener una gran influencia en los resultados de la estimación por MCO del modelo, de tal modo que su inclusión o exclusión en la información muestral puede provocar grandes cambios en las estimaciones de los parámetros, en el ajuste del modelo, en las varianzas estimadas, etc.</p>
<p>Hablaremos de <em>outliers</em> cuando los datos especiales corresponden a observaciones atípicas en los valores de la variable dependiente y de <em>leverages</em> para observaciones con valores anormales en alguna(s) de las variables explicativas del modelo.</p>
<section id="identificación-y-análisis-de-la-influencia-de-las-observaciones-atípicas" class="level3" data-number="3.9.1">
<h3 data-number="3.9.1" class="anchored" data-anchor-id="identificación-y-análisis-de-la-influencia-de-las-observaciones-atípicas"><span class="header-section-number">3.9.1</span> Identificación y análisis de la influencia de las observaciones atípicas</h3>
<section id="detección-de-observaciones-atípicas-en-las-variables-explicativas" class="level4" data-number="3.9.1.1">
<h4 data-number="3.9.1.1" class="anchored" data-anchor-id="detección-de-observaciones-atípicas-en-las-variables-explicativas"><span class="header-section-number">3.9.1.1</span> Detección de observaciones atípicas en las variables explicativas</h4>
<p>Para detectar la presencia de <em>leverages</em> se suelen utilizar los valores <em>h<sub>i</sub></em>, definidos por</p>
<p><span class="math display">\[\ h_{i} = \mathbf{x}_{i}^{'}\left( \mathbf{X}^{'}\mathbf{X} \right)^{- 1}\mathbf{x}_{i}\ \]</span></p>
<p>los cuales miden la distancia del vector <span class="math inline">\(\mathbf{x}_{1} = \left( x_{1i},x_{2i},\ldots,x_{Ki} \right)\)</span> al centroide de los datos, <span class="math inline">\(\overline{\mathbf{x}} = \left( {\overline{x}}_{1},{\overline{x}}_{2},\ldots,{\overline{x}}_{K} \right)\)</span>; por tanto, tenderán a ser altos para aquellas observaciones con valores que son bastante diferentes de las medias de las variables explicativas.</p>
<p>En concreto, se puede demostrar que <span class="math inline">\(0 \leq h_{i} \leq 1\)</span> y <span class="math inline">\(\sum_{i}^{}h_{i} = K\)</span> y, por consiguiente, el promedio de los <em>h<sub>i</sub></em> es <span class="math inline">\(\overline{h} = \frac{K}{n}\)</span>. Se considera que un valor crítico adecuado para medir la desviación respecto a este punto central es dos veces dicha cuantía, es decir, se considera que una observación toma valores atípicos para alguna variable explicativa si se cumple que</p>
<p><span class="math display">\[\ h_{i} &gt; 2\left( \frac{K}{n} \right)\ \]</span></p>
</section>
<section id="detección-de-observaciones-atípicas-en-la-variable-dependiente" class="level4" data-number="3.9.1.2">
<h4 data-number="3.9.1.2" class="anchored" data-anchor-id="detección-de-observaciones-atípicas-en-la-variable-dependiente"><span class="header-section-number">3.9.1.2</span> Detección de observaciones atípicas en la variable dependiente</h4>
<p>Para detectar la presencia de <em>outliers</em> pueden utilizarse los llamados <em>residuos estandarizados</em>, que están diseñados para señalar las observaciones que originan valores grandes (en valor absoluto) en los residuos del modelo, es decir, el valor observado de la variable dependiente difiere substancialmente del valor esperado según el modelo de regresión.</p>
<p>Estos se obtienen re-escalando los residuos MCO del modelo mediante la expresión</p>
<p><span class="math display">\[\ r_{i} = \frac{{\widehat{e}}_{i}}{\widehat{\sigma}}\ \]</span></p>
<p>Se puede demostrar que <span class="math inline">\(r_{i}\sim N(0,1)\)</span> y, por tanto, grandes valores de estos residuos</p>
<p><span class="math display">\[\ \left| r_{i} \right| &gt; 2.5\ \]</span></p>
<p>son indicativos de la presencia de observaciones extremas en la variable dependiente.</p>
</section>
<section id="influencia-real-de-las-observaciones-atípicas" class="level4" data-number="3.9.1.3">
<h4 data-number="3.9.1.3" class="anchored" data-anchor-id="influencia-real-de-las-observaciones-atípicas"><span class="header-section-number">3.9.1.3</span> Influencia real de las observaciones atípicas</h4>
<p>Para detectar cómo influyen las observaciones atípicas (<em>outliers</em> o <em>leverages</em>) sobre los resultados MCO de una regresión, se han propuesto en la literatura diferentes estadísticos: <em>DFBETAS</em>, para valorar la repercusión sobre los parámetros estimados; <em>DFFITS</em>, para ver cómo afectan los atípicos a las predicciones; <em>DCOOK</em>, para analizar la influencia conjunta sobre parámetros y predicciones; etc.</p>
<p>En particular, para valorar los efectos de las observaciones atípicas sobre los valores ajustados del modelo, se pueden utilizar los estadísticos <span class="math inline">\(\left( \frac{h_{i}}{1 - h_{i}} \right){\widehat{e}}_{i}\)</span>, o una versión escalada de estos valores, los llamados <span class="math inline">\(\ {DFFITS}_{i} = \sqrt{\frac{h_{i}}{1 - h_{i}}}{\widehat{e}}_{i}^{s}\ \)</span>, donde <span class="math inline">\({\widehat{e}}_{i}^{s} = \frac{{\widehat{e}}_{i}}{\widehat{\sigma}\sqrt{1 - h_{i}}}\)</span> son los llamados <em>residuos estudentizados</em>, de forma que se puede demostrar que una observación es influyente si se cumple que</p>
<p><span class="math display">\[\ \left| {DFFITS}_{i} \right| &gt; 2\sqrt{\frac{K}{n}}\ \]</span></p>
</section>
</section>
<section id="cuestiones-que-plantea-la-presencia-de-observaciones-atípicas" class="level3" data-number="3.9.2">
<h3 data-number="3.9.2" class="anchored" data-anchor-id="cuestiones-que-plantea-la-presencia-de-observaciones-atípicas"><span class="header-section-number">3.9.2</span> Cuestiones que plantea la presencia de observaciones atípicas</h3>
<p>¿Qué problemas acarrea la presencia de observaciones atípicas? En principio, el “único” problema es la falta de robustez de los resultados ante la presencia de dichas observaciones, ya que el estimador MCO mantiene todas sus propiedades.</p>
<p>¿Qué hacer ante la presencia de observaciones atípicas? Son cuatro las posibilidades:</p>
<ol type="1">
<li><p>No hacer nada con dichas observaciones, manteniéndolas en la muestra, pero siendo conscientes de que su presencia puede condicionar fuertemente los resultados.</p></li>
<li><p>Eliminar de la muestra los datos atípicos, lo que supondría eliminar el peligro potencial de su influencia, pero a costa de perder información que puede ser valiosa tanto en términos económicos como estadísticos (menores errores estándar en las estimaciones y, por tanto, mayor eficiencia).</p></li>
<li><p>Tratar dichas observaciones de una forma sencilla, por ejemplo, a través de la introducción de variables ficticias que consideren la especificidad de los valores atípicos.</p></li>
<li><p>Utilizar métodos más sofisticados, como los estimadores robustos, las regresiones cuantilíticas o las regresiones con errores no normales (como las distribuciones <em>t</em> con colas “gruesas”).</p></li>
</ol>
<p>Uno de los estimadores robustos más usados en la literatura econométrica es el denominado <em>estimador de desviaciones absolutas mínimas (MDA)</em>, que se obtiene minimizando la suma absoluta de los errores</p>
<p><span class="math display">\[\ \sum_{i = 1}^{n}\left| y_{i} - \beta_{1} - \beta_{2}x_{2i} - \ldots - \beta_{K}x_{Ki} \right|\ \]</span></p>
<p>en lugar de la suma de cuadrados <span class="math inline">\(\sum_{i = 1}^{n}\left( y_{i} - \beta_{1} - \beta_{2}x_{2i} - \ldots - \beta_{K}x_{Ki} \right)^{2}\)</span>, como en el caso estándar del estimador MCO. En este caso, se estiman los parámetros de la mediana condicional de <em>y</em>, y no de la media; por tanto, cada parámetro estimado <em>β<sub>j</sub></em> medirá el efecto parcial del regresor <em>x<sub>j</sub></em> sobre la mediana de <em>y</em>.</p>
<p>Se puede demostrar que el estimador MDA, que es no lineal, es asintóticamente insesgado, consistente y con una distribución normal para muestras grandes. Además, su varianza asintótica es menor que la del estimador MCO para una amplia clase de distribuciones con “colas densas”. Lógicamente, cuando la distribución de los errores es normal, el estimador MDA es ineficiente, pudiéndose demostrar que su varianza (asintótica) es un 57% mayor que la del estimador MCO.</p>
</section>
</section>
<section id="variable-dependiente-discreta-o-limitada" class="level2" data-number="3.10">
<h2 data-number="3.10" class="anchored" data-anchor-id="variable-dependiente-discreta-o-limitada"><span class="header-section-number">3.10</span> Variable dependiente discreta o limitada</h2>
<section id="variable-dependiente-discreta" class="level3" data-number="3.10.1">
<h3 data-number="3.10.1" class="anchored" data-anchor-id="variable-dependiente-discreta"><span class="header-section-number">3.10.1</span> Variable dependiente discreta</h3>
<p>Los modelos de variable dependiente discreta son aquellos en que la variable de respuesta, <em>y</em>, toma únicamente valores discretos. Esta situación aparece comúnmente en modelos donde la variable dependiente es de tipo cualitativo (cumplimiento de una propiedad, elección entre varias alternativas, impacto deseado de una medida de política económica, etc.), de tipo ordinal (los resultados de un suceso representan categorías ordenadas siguiendo un cierto rango) o de recuento (el número de veces que ocurre un fenómeno).</p>
<p>Ejemplos de variables discretas u ordenadas son los siguientes:</p>
<ul>
<li><p><em>y</em> = 1 si una familia compra una vivienda en propiedad, <em>y</em> = 0 si la alquila.</p></li>
<li><p><em>y</em> = 1 si se tiene empleo, <em>y</em> = 0 si no se tiene.</p></li>
<li><p><em>y</em> = 1 si una empresa tiene beneficios, <em>y</em> = 0 si no los tiene.</p></li>
<li><p><em>y</em> = 1 si el efecto de una medida económica ha sido el esperado, <em>y</em> = 0 si no lo ha sido.</p></li>
<li><p><em>y</em> = 1 si elige ir en coche al trabajo, <em>y</em> = 2 si va en autobús, <em>y</em> = 3 si va en tren de cercanías.</p></li>
<li><p>Opinión del consumidor ante un nuevo producto: <em>y</em> = 0 (muy malo), <em>y</em> = 1 (malo), <em>y</em> = 2 (normal), <em>y</em> = 3 (bueno), <em>y</em> = 4 (muy bueno).</p></li>
<li><p><em>y</em> = <em>n</em> el número de huelgas en un cierto mes del año, o las veces que un paciente acude al médico en una semana.</p></li>
</ul>
<p>En este apartado analizaremos solo los modelos econométricos donde la variable dependiente es del tipo cualitativo y, además, dicotómica, es decir, que toma únicamente dos valores distintos, según se satisfaga o no una determinada propiedad o condición. Para este caso, se expondrán los modelos <em>Logit</em> y <em>Probit</em>, que son los más utilizados para tratar este tipo de casos de elección binaria, aunque previamente expondremos los problemas a los que se enfrenta el modelo clásico si se intenta aplicar a este tipo de variables.</p>
<section id="el-modelo-de-probabilidad-lineal" class="level4" data-number="3.10.1.1">
<h4 data-number="3.10.1.1" class="anchored" data-anchor-id="el-modelo-de-probabilidad-lineal"><span class="header-section-number">3.10.1.1</span> El modelo de probabilidad lineal</h4>
<p>Cuando se aplica el modelo de regresión lineal estándar <span class="math inline">\(y_{i} = \beta_{1} + \beta_{2}x_{2i} + \ldots + \beta_{K}x_{Ki} + e_{i}\)</span> al caso donde la variable dependiente es de tipo dicotómico (1/0), éste recibe el nombre de modelo de probabilidad lineal (MPL). En este caso, la distribución de <em>y</em> condicionada al conjunto de valores observados de las variables explicativas viene dada por</p>
<p><span class="math display">\[y_{i} = \left\{ \begin{matrix} \ 1\ \ \ con\ \ \ \ \ \ P_{i} = P(y = 1) \\ \ \ \ \ \ \  0\ \ \ con\ \ \ \ \ 1 - P_{i} = P(y = 0) \\ \end{matrix} \right.\ \]</span></p>
<p>Se llama modelo de probabilidad lineal porque, si se cumple que <span class="math inline">\(E\left\lbrack e_{i} \right\rbrack = 0\)</span>, la función de regresión poblacional está dada por</p>
<p><span class="math display">\[E\left\lbrack y_{i} \right\rbrack = 1 \times P_{i} + 0 \times \left( 1 - P_{i} \right) = P_{i}\]</span></p>
<p>luego se tendrá que</p>
<p><span class="math display">\[P_{i} = \beta_{1} + \beta_{2}x_{2i} + \cdots + \beta_{K}x_{Ki}\]</span></p>
<p>y, por tanto, el parámetro <em>β<sub>j</sub></em> representa el incremento en la probabilidad de que la variable <em>y</em> tome el valor 1 cuando se incrementa la variable <em>x<sub>j</sub></em> en una unidad (manteniéndose constante el resto de las variables explicativas), <span class="math inline">\(\frac{\partial P_{i}}{\partial x_{j}} = \beta_{j}\)</span>.</p>
<p>Si estimamos el modelo MPL por mínimos cuadrados ordinarios tendremos varios inconvenientes: la no normalidad de los errores, la heteroscedasticidad y, lo más importante, la probabilidad estimada de que la variable <em>y</em> tome el valor 1, dada por <span class="math inline">\({\widehat{P}}_{i} = {\widehat{\beta}}_{1} + {\widehat{\beta}}_{2}x_{2i} + \ldots + {\widehat{\beta}}_{K}x_{Ki}\)</span>, puede no pertenecer al intervalo [0,1] para algunas observaciones de la muestra. Lógicamente, si eso ocurre en una aplicación, el MPL queda invalidado.</p>
</section>
<section id="modelos-logit-y-probit" class="level4" data-number="3.10.1.2">
<h4 data-number="3.10.1.2" class="anchored" data-anchor-id="modelos-logit-y-probit"><span class="header-section-number">3.10.1.2</span> Modelos Logit y Probit</h4>
<p>Una alternativa frente al modelo de probabilidad lineal es construir un modelo donde la probabilidad de que la variable dependiente tome el valor 1 venga dada por una función que obligatoriamente tome valores entre 0 y 1 y que no sea necesariamente lineal en las variables explicativas,</p>
<p><span class="math display">\[P_{i} = P\left\lbrack y_{i} = 1 \right\rbrack = F(\beta_{1} + \beta_{2}x_{2i} + \ldots + x_{Ki}) = F(I_{i})\]</span></p>
<p>Las candidatas naturales para utilizar son la función logística, que origina el modelo <em>Logit</em>, o las funciones de distribución acumulada de variables aleatorias unidimensionales, como la normal tipificada, que da lugar al modelo <em>Probit</em>, u otras funciones como la distribución asimétrica log-Weibull, que da lugar al modelo <em>Gompit</em>.</p>
</section>
<section id="el-modelo-logit" class="level4" data-number="3.10.1.3">
<h4 data-number="3.10.1.3" class="anchored" data-anchor-id="el-modelo-logit"><span class="header-section-number">3.10.1.3</span> El modelo Logit</h4>
<p>La función logística está dada por <span class="math inline">\(\Lambda(z) = \frac{1}{1 + e^{- z}}\)</span>, de modo que el modelo <em>Logit</em> tendrá una función de probabilidad dada por</p>
<p><span class="math display">\[P_{i} = P\lbrack y_{i} = 1\rbrack = \Lambda(\beta_{1} + \beta_{2}x_{2i} + \ldots + \beta_{K}x_{Ki}) = \frac{1}{1 + e^{- (\beta_{1} + \beta_{2}x_{2i} + \ldots + \beta_{K}x_{Ki})}}\]</span></p>
<p>El valor <span class="math inline">\(I_{i} = \beta_{1} + \beta_{2}x_{2i} + \ldots + \beta_{K}x_{Ki}\)</span> puede interpretarse como un <em>indicador de utilidad</em>, de modo que la probabilidad de que <em>y</em> tome el valor 1 depende de la utilidad que tenga para el individuo la opción correspondiente. Es inmediato que <span class="math inline">\(\Lambda(0) = 0.5\)</span>, de modo que la preferencia por <em>y</em> = 1 será mayor cuando el indicador sea positivo (<span class="math inline">\(I &gt; 0\)</span>), y viceversa, aunque el incremento de probabilidad no es constante para crecimientos iguales de la utilidad dado que la función logística no es lineal.</p>
<p>De hecho, para el modelo <em>Logit</em> se tiene que</p>
<p><span class="math display">\[\frac{\partial P_{i}}{\partial x_{j}} = \beta_{j}\lambda(\beta_{1} + \beta_{2}x_{2i} + \ldots + \beta_{K}x_{Ki})\]</span></p>
<p>donde <span class="math inline">\(\lambda(z) = \frac{e^{- z}}{\left( 1 + e^{- z} \right){}^{2}}\)</span> es la “función de densidad” de la curva logística [en el sentido de que se cumple que <span class="math inline">\(\Lambda^{'}(z) = \lambda(z)\)</span>].</p>
<p>Por consiguiente, el efecto marginal no coincide con el valor del parámetro, tal como ocurría en el MPL, sino que depende del valor que tomen las variables explicativas en cada observación: cuando cambia una variable <em>x<sub>j</sub></em>, el valor de la función <span class="math inline">\(\lambda\left( \beta_{1} + \beta_{2}x_{2i} + \ldots + \beta_{K}x_{Ki} \right)\)</span> también varía (<em>ceteris paribus</em>) y, por tanto, el efecto combinado viene dado por el producto <span class="math inline">\(\beta_{j}\lambda\left( \beta_{1} + \beta_{2}x_{2i} + \ldots + \beta_{K}x_{Ki} \right)\)</span>.</p>
<p>Notar que, si <em>β<sub>j</sub></em> es positivo, también lo será el efecto marginal <span class="math inline">\(\frac{\partial P_{i}}{\partial x_{j}}\)</span> ya que la función <span class="math inline">\(\lambda( z )\)</span> siempre toma valores positivos. Asimismo, puesto que dicha función toma el máximo valor en <em>z</em> = 0 y en dicho valor se tiene que <span class="math inline">\(P = \Lambda(0) = 0.5\)</span>, el mayor efecto marginal (en valor absoluto), dado el parámetro <em>β<sub>j</sub></em>, se produce para aquellas observaciones donde existe indecisión (<em>P</em> = 0.5) entre las dos alternativas posibles, <em>y</em> = 1 o <em>y</em> = 0.</p>
<p>Además, al verificarse la igualdad <span class="math inline">\(\lambda(z) = \Lambda(z)\left\lbrack 1 - \Lambda(z) \right\rbrack\)</span>, se tiene que</p>
<p><span class="math display">\[\frac{\partial P_{i}}{\partial x_{j}} = \beta_{j}P_{i}(1 - P_{i})\]</span></p>
<p>la cual es una función cuadrática en la variable <em>P</em>. Entonces, no solo los efectos marginales son no lineales, sino que el crecimiento de la probabilidad es mayor en la zona central (cerca de <em>P</em> = 0.5), disminuyendo progresivamente conforme nos alejamos hacia la izquierda o hacia la derecha (hacia <em>P</em> = 0 o <em>P</em> = 1, respectivamente).</p>
<p>Por otra parte, también se tiene que</p>
<p><span class="math display">\[\frac{\beta_{j}}{\beta_{k}} = \frac{\partial P/\partial x_{j}}{\partial P/\partial x_{k}}\]</span></p>
<p>y, por tanto, la razón de coeficientes estimados proporciona una medida del cambio relativo en las probabilidades cuando se producen variaciones en las variables explicativas.</p>
</section>
<section id="el-modelo-probit" class="level4" data-number="3.10.1.4">
<h4 data-number="3.10.1.4" class="anchored" data-anchor-id="el-modelo-probit"><span class="header-section-number">3.10.1.4</span> El modelo Probit</h4>
<p>En el modelo <em>Probit</em>, la probabilidad de que <em>y</em> tome el valor 1 viene dada por la función de distribución de una variable aleatoria normal tipificada, <span class="math inline">\(\Phi(z) = \int_{- \infty}^{z}{\frac{1}{\sqrt{2\pi}}e^{- \frac{\ t^{2}}{2}}}dt\)</span>, es decir</p>
<p><span class="math display">\[P_{i} = P\lbrack y_{i} = 1\rbrack = \Phi(\beta_{1} + \beta_{2}x_{2i} + \ldots + \beta_{K}x_{Ki}) = \int_{- \infty}^{\beta_{1} + \beta_{2}x_{2i} + \ldots + \beta_{K}x_{Ki}}{\frac{1}{\sqrt{2\pi}}e^{- \frac{\ t^{2}}{2}}}dt\]</span></p>
<p>Para este modelo, se tiene que</p>
<p><span class="math display">\[\frac{\partial P_{i}}{\partial x_{j}} = \beta_{j}\varphi(\beta_{1} + \beta_{2}x_{2i} + \ldots + \beta_{K}x_{Ki})\]</span></p>
<p>donde <span class="math inline">\(\phi\)</span> es la función de densidad de la distribución <em>N</em>(0,1), <span class="math inline">\(\varphi(t) = \frac{1}{\sqrt{2\pi}}e^{- \frac{\ t^{2}}{2}}\)</span>. De nuevo, el efecto marginal no coincide con el valor del parámetro, sino que es no lineal y alcanza el máximo valor absoluto para <em>P</em> = 0.5, es decir, en el valor de indecisión entre las dos alternativas.</p>
</section>
<section id="consideraciones-especiales-en-los-modelos-logit-y-probit" class="level4" data-number="3.10.1.5">
<h4 data-number="3.10.1.5" class="anchored" data-anchor-id="consideraciones-especiales-en-los-modelos-logit-y-probit"><span class="header-section-number">3.10.1.5</span> Consideraciones especiales en los modelos Logit y Probit</h4>
<section id="estimación" class="level5" data-number="3.10.1.5.1">
<h5 data-number="3.10.1.5.1" class="anchored" data-anchor-id="estimación"><span class="header-section-number">3.10.1.5.1</span> Estimación</h5>
<p>Dado que los modelos <em>Logit</em> y <em>Probit</em> no son lineales, podría utilizarse el método de MCNL para estimarlos. Sin embargo, en general se emplea el método de máxima verosimilitud (MV), que consiste en maximizar el logaritmo de la función de verosimilitud del modelo, que viene dado para los dos modelos en cuestión por la expresión siguiente</p>
<p><span class="math display">\[\log L(\beta) = \sum_{i = 1}^{n}\left\lbrack y_{i}\log F(\beta_{1} + \beta_{2}x_{2i} + \ldots + \beta_{K}x_{Ki}) \right\rbrack + \]</span></p>
<p><span class="math display">\[+ \sum_{i = 1}^{n}\left\lbrack (1 - y_{i})log(1 - F(\beta_{1} + \beta_{2}x_{2i} + \ldots + \beta_{K}x_{Ki}) \right\rbrack\]</span></p>
<p>donde <em>F</em>(<em>z</em>) es la función de probabilidad correspondiente (logística o normal estándar). El estimador MV garantiza las propiedades de consistencia, eficiencia y normalidad asintótica. En consecuencia, las estimaciones serán fiables siempre y cuando el tamaño de la muestra sea suficientemente elevado.</p>
</section>
<section id="predicción" class="level5" data-number="3.10.1.5.2">
<h5 data-number="3.10.1.5.2" class="anchored" data-anchor-id="predicción"><span class="header-section-number">3.10.1.5.2</span> Predicción</h5>
<p>A partir de las estimaciones del modelo, <span class="math inline">\({\widehat{P}}_{i} = F({\widehat{\beta}}_{1} + {\widehat{\beta}}_{2}x_{Ki} + \ldots + {\widehat{\beta}}_{K}x_{Ki})\)</span>, los valores de predicción para la variable dependiente <em>y</em> vienen dados por</p>
<p><span class="math display">\[{\widehat{y}}_{i} = \left\{ \begin{matrix}
\ 0\ si\ \ {\widehat{P}}_{i} &lt; 0.5 \\
\ 1\ si\ \ {\widehat{P}}_{i} \geq 0.5 \\
\end{matrix} \right.\ \]</span></p>
</section>
<section id="grado-de-ajuste-del-modelo" class="level5" data-number="3.10.1.5.3">
<h5 data-number="3.10.1.5.3" class="anchored" data-anchor-id="grado-de-ajuste-del-modelo"><span class="header-section-number">3.10.1.5.3</span> Grado de ajuste del modelo</h5>
<p>Para todos los modelos de elección binaria, el estadístico <em>R</em><sup>2</sup> definido para el modelo lineal deja de ser una buena medida de la bondad del ajuste proporcionado por el modelo. Un estadístico muy utilizado es el conocido <em>R</em><sup>2</sup> de McFadden, que toma también valores entre 0 y 1 y que se calcula como</p>
<p><span class="math display">\[R_{McFadden}^{2} = 1 - \frac{l(\widehat{\mathbf{\beta}})}{l({\widehat{\mathbf{\beta}}}_{0})}\]</span></p>
<p>donde <span class="math inline">\(l\left( \widehat{\mathbf{\beta}} \right)\)</span> y <span class="math inline">\(l\left( {\widehat{\mathbf{\beta}}}_{0} \right)\)</span> son, respectivamente, los valores del logaritmo de la función de verosimilitud del modelo completo y del modelo sin variables explicativas (salvo la constante).</p>
<p>También suelen utilizarse como medida del grado de ajuste del modelo estimado las conocidas <em>tablas de clasificación</em>, en las cuales se representa el porcentaje de predicciones correctas del modelo estimado para los datos de la muestra.</p>
</section>
<section id="contraste-de-hipótesis" class="level5" data-number="3.10.1.5.4">
<h5 data-number="3.10.1.5.4" class="anchored" data-anchor-id="contraste-de-hipótesis"><span class="header-section-number">3.10.1.5.4</span> Contraste de hipótesis</h5>
<p>Para los modelos expuestos, <em>Logit</em> y <em>Probit</em>, se tiene que <span class="math inline">\(\widehat{\mathbf{\beta}}\overset{as}{\sim}N\left( \mathbf{\beta},Cov\left( \widehat{\mathbf{\beta}} \right) \right)\)</span>, donde <span class="math inline">\(Cov\left( \widehat{\mathbf{\beta}} \right) = \left( \mathbf{X}^{'}\mathbf{DX} \right)^{- 1}\)</span> y <span class="math inline">\(\mathbf{D}\)</span> es una matriz diagonal cuyos elementos tienen diferentes expresiones para cada modelo.</p>
<p>Por tanto, si el tamaño de muestra es grande, los contrastes de significación individual pueden realizarse mediante los estadísticos <em>z</em>, definidos como <span class="math inline">\(z_{j} = \frac{{\widehat{\beta}}_{j}}{se\left( {\widehat{\beta}}_{j} \right)}\)</span>, pero comparando tales <em>ratios</em> con los valores críticos de la distribución <em>N</em>(0,1) en lugar de los correspondientes a la distribución <em>t</em> de Student que se usan en las regresiones estándar.</p>
<p>Para contrastes generales del tipo <span class="math inline">\(H_{0}:\left\{ \mathbf{R\beta} = \mathbf{r} \right\}\)</span> se puede usar el test de Wald (<em>W</em>) o el test de razón de verosimilitudes (<em>LR</em>), que toma la forma</p>
<p><span class="math display">\[LR = 2\left\lbrack l({\widehat{\mathbf{\beta}}}_{NR}) - l({\widehat{\mathbf{\beta}}}_{R}) \right\rbrack\]</span></p>
<p>donde <span class="math inline">\(l\left( {\widehat{\mathbf{\beta}}}_{NR} \right)\)</span> y <span class="math inline">\(l\left( {\widehat{\mathbf{\beta}}}_{R} \right)\)</span> son, respectivamente, los valores del logaritmo de la función de verosimilitud del modelo no restringido y restringido. Ambos estadísticos siguen asintóticamente, bajo la hipótesis nula correspondiente, una distribución <span class="math inline">\(\ \chi_{q}^{2}\ \)</span>, siendo <em>q</em> el número de restricciones.</p>
</section>
<section id="efectos-marginales" class="level5" data-number="3.10.1.5.5">
<h5 data-number="3.10.1.5.5" class="anchored" data-anchor-id="efectos-marginales"><span class="header-section-number">3.10.1.5.5</span> Efectos marginales</h5>
<p>Como se ha dicho en la exposición teórica, los efectos marginales de los modelos <em>Logit</em> y <em>Probit</em> no son constantes, sino que dependen de los valores de las variables explicativas.</p>
<p>En ocasiones, el investigador puede estar interesado en el cálculo del <em>efecto marginal en la media</em> de las variables explicativas, <span class="math inline">\(\overline{\mathbf{x}} = \left( 1,{\overline{x}}_{2},\ldots,{\overline{x}}_{K} \right)\)</span>, en cuyo caso el valor que habría que utilizar será</p>
<p><span class="math display">\[\frac{\partial P}{\partial x_{j}}({\overline{x}}_{2},\ldots,{\overline{x}}_{K}) = \beta_{j}f(\beta_{1} + \beta_{2}{\overline{x}}_{2} + \ldots + \beta_{K}{\overline{x}}_{K})\]</span></p>
<p>donde <span class="math inline">\(f(z)\)</span> representa a la función de densidad <span class="math inline">\(\lambda(z)\)</span> o <span class="math inline">\(\phi(t)\)</span>, según corresponda.</p>
<p>Otras veces, en lugar de evaluar el efecto marginal en un valor específico, se considera más relevante el <em>efecto marginal promedio</em>, que se define como la media de los efectos marginales calculados en cada punto muestral</p>
<p><span class="math display">\[\overline{\frac{\partial P}{\partial x_{j}}} = \frac{\sum_{i = 1}^{n}{\beta_{j}f(\beta_{1} + \beta_{2}x_{2i} + \ldots + \beta_{K}x_{Ki})}}{n} = \beta_{j}\overline{f(\beta_{1} + \beta_{2}x_{2} + \ldots + \beta_{K}x_{K})}\]</span></p>
<p>Este valor representa la respuesta promedio de la probabilidad, para las observaciones de la muestra, ante un cambio en el valor de una determinada variable explicativa.</p>
</section>
</section>
</section>
<section id="variable-dependiente-limitada" class="level3" data-number="3.10.2">
<h3 data-number="3.10.2" class="anchored" data-anchor-id="variable-dependiente-limitada"><span class="header-section-number">3.10.2</span> Variable dependiente limitada</h3>
<section id="valores-censurados-el-modelo-tobit" class="level4" data-number="3.10.2.1">
<h4 data-number="3.10.2.1" class="anchored" data-anchor-id="valores-censurados-el-modelo-tobit"><span class="header-section-number">3.10.2.1</span> Valores censurados: el modelo Tobit</h4>
<p>En algunas ocasiones, la variable dependiente toma valores limitados porque todos los valores contenidos en un cierto rango se asocian a un único valor límite <em>c</em> que, en general, por conveniencia se supone que es cero. Se dice entonces que dicha variable está <em>censurada</em>.</p>
<p>Los ejemplos clásicos de este tipo de situaciones son la compra de vivienda o automóviles por parte de los individuos o las familias, donde se observa un gran número de individuos que no gastan cantidad alguna (<span class="math inline">\(y_{i} = 0\)</span>) en dicho bien. En términos microeconómicos, estas observaciones límite describen un comportamiento de <em>solución de esquina</em>, es decir, resultan de un comportamiento optimizador por parte de los agentes económicos, para quienes resulta óptimo gastar una cantidad cero en la compra de vivienda o automóvil.</p>
<p>Con el modelo de regresión lineal estándar no se puede explicar la diferencia cualitativa que existe entre las observaciones límite (0) y las observaciones normales (<span class="math inline">\(y_{i}\)</span>). De hecho, si se utilizan MCO para estimar modelos de regresión donde la variable dependiente está limitada, los resultados serán sesgados e inconsistentes, empeorando conforme aumenta el número de observaciones censuradas.</p>
<p>Para tratar estos casos de censura suele utilizarse el <em>modelo </em>Tobit*, propuesto por Tobin (1958), quien lo introdujo por primera vez para analizar la compra de automóviles en el mercado estadounidense.</p>
<p>En el modelo <em>Tobit</em> se define una variable latente asociada a “todas” las observaciones muestrales, <span class="math inline">\(y_{i}^{*}\)</span>, que cumple la propiedad</p>
<p><span class="math display">\[y_{i} = \left\{ \begin{matrix} \ 0\ \ si\ y_{i}^{*} &lt; 0 \\ \ y_{i}^{*}\ si\ y_{i}^{*} \geq 0 \\ \end{matrix} \right.\ \]</span></p>
<p>Si se supone que la variable latente sigue un modelo de regresión lineal estándar</p>
<p><span class="math display">\[y_{i}^{*} = \beta_{1} + \beta_{2}x_{2i} + \ldots + \beta_{K}x_{Ki} + e_{i}\]</span></p>
<p>entonces la distribución que sigue la variable <span class="math inline">\(y_{i}\)</span> es</p>
<p><span class="math display">\[\left\{ \begin{matrix} \ P\lbrack y_{i} = 0\rbrack = P\lbrack y_{i}^{*} &lt; 0\rbrack\ \ para\ las\ observaciones\ límite\ (y_{i} = 0) \\ \ \ \ \ \ f(y_{i}) = \varphi(y_{i}^{*})\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ para\ las\ observaciones\ continuas\ (y_{i} &gt; 0) \\ \end{matrix} \right.\ \]</span></p>
<p>con lo que se pone de manifiesto la diferencia entre los dos tipos de observaciones en el conjunto de datos observado.</p>
<p>Como se trata de un modelo no lineal, para estimar el modelo <em>Tobit</em> se utiliza habitualmente el método de máxima de verosimilitud, igual que ocurrió con los modelos <em>Logit</em> y <em>Probit</em>. En este caso, la función a maximizar viene dada por</p>
<p><span class="math display">\[\log L(\mathbf{\beta},\sigma) = \sum_{y_{i} = 0}^{}\left\lbrack log(1 - \Phi(\frac{\beta_{1} + \beta_{2}x_{2i} + \ldots + \beta_{K}x_{Ki}}{\sigma}) \right\rbrack + \]</span> <span class="math display">\[ +  \sum_{y_{i} &gt; 0}^{}\left\lbrack \log\varphi(\frac{\beta_{1} + \beta_{2}x_{2i} + \ldots + \beta_{K}x_{Ki}}{\sigma}) \right\rbrack\]</span> Si se cumplen las propiedades estándar en el modelo latente, se puede demostrar que el estimador MV tiene las propiedades de consistencia, eficiencia y normalidad asintótica. También pueden realizarse inferencias utilizando los estadísticos <em>z</em> y <em>LR</em>, en los mismos términos que se expusieron anteriormente para los modelos de elección binaria.</p>
<p>En lo referente a los efectos marginales, se tiene en este caso que</p>
<p><span class="math display">\[\frac{\partial E(y)}{\partial x_{j}} = \beta_{j}\Phi\left( \frac{\beta_{1} + \beta_{2}x_{2} + \ldots + \beta_{K}x_{K}}{\sigma} \right)\]</span></p>
<p>por lo que, de nuevo, estos efectos dependen de los valores de las variables explicativas, y ahora además de la desviación estándar residual, <em>σ</em>.</p>
<p>Como puede observarse, el factor de ajuste que multiplica a los parámetros <span class="math inline">\(\beta_{j}\)</span> está comprendido entre 0 y 1, ya que la función <span class="math inline">\(\Phi(z)\)</span> solo toma valores en ese rango, por lo que los efectos marginales serán siempre inferiores (en valor absoluto) a los parámetros del modelo.</p>
<p>Por otro lado, se pueden calcular los <em>efectos marginales en la media</em> de las variables explicativas</p>
<p><span class="math display">\[\frac{\partial E(y|{\overline{x}}_{2},\ldots,{\overline{x}}_{K})}{\partial x_{j}} = \beta_{j}\Phi\left( \frac{\beta_{1} + \beta_{2}{\overline{x}}_{2} + \ldots + \beta_{K}{\overline{x}}_{K}}{\sigma} \right)\]</span></p>
<p>o se puede computar el <em>efecto marginal promedio</em> sobre la muestra</p>
<p><span class="math display">\[\overline{\frac{\partial E(y)}{\partial x_{j}}} = \frac{\sum_{i = 1}^{n}{\beta_{j}\Phi\left( \frac{\beta_{1} + \beta_{2}x_{2i} + \ldots + \beta_{K}x_{Ki}}{\sigma} \right)}}{n} = \beta_{j}{\overline{\Phi(\frac{\beta_{1} + \beta_{2}x_{2} + \ldots + \beta_{K}x_{K}}{\sigma})}}^{}\]</span></p>
</section>
<section id="valores-con-truncamiento-selectivo-el-modelo-heckit" class="level4" data-number="3.10.2.2">
<h4 data-number="3.10.2.2" class="anchored" data-anchor-id="valores-con-truncamiento-selectivo-el-modelo-heckit"><span class="header-section-number">3.10.2.2</span> Valores con truncamiento selectivo: el modelo Heckit</h4>
<p>En algunas situaciones económicas, los datos observados de la variable dependiente se obtienen a partir de un mecanismo de selección muestral que no es aleatorio, por lo que dicha variable se ve sometida a un proceso de truncamiento selectivo.</p>
<p>Por ejemplo, si se quieren analizar los salarios de la población femenina, con el objeto de explicar los factores diferenciales del mercado laboral de las mujeres frente al de los hombres, hay que tener en cuenta que hace unas décadas una parte importante de la población femenina no participaba en el mercado de trabajo (eran amas de casa, es decir, con un trabajo no remunerado en el mercado), por lo que solo se observaban datos salariales para aquella parte de la población que formaba parte de la fuerza laboral (población activa). Existe, por consiguiente, un problema de truncamiento selectivo en los datos observados, que habrá que tener en cuenta al estimar el modelo econométrico correspondiente.</p>
<p>Igual que para el caso censurado, con el modelo de regresión lineal estándar no se puede explicar la diferencia cualitativa que existe entre las mujeres amas de casa, que no reciben salario alguno por su trabajo (<span class="math inline">\(y_{i} = 0\)</span>) y las mujeres que forman parte de la población ocupada, para las que se observan sus salarios de mercado <span class="math inline">\(y_{i}\)</span>. Si se utilizasen <em>MCO completos</em> para estimar la regresión<br>
<span class="math inline">\(y_{i} = \beta_{1} + \beta_{2}x_{2i} + \ldots + \beta_{K}x_{Ki} + e_{i}\)</span> que explicase los salarios de “toda la población femenina”, los resultados serían sesgados e inconsistentes. Por otra parte, si se aplicasen <em>MCO truncados</em> a dicho modelo, es decir, utilizando solo los datos de las mujeres con trabajo remunerado, los resultados también serían sesgados e inconsistentes, ya que la muestra truncada no sería aleatoria (pues el mecanismo de selección no es exógeno).</p>
<p>Para tratar este tipo de situaciones en las que existe un truncamiento selectivo de la muestra, la literatura econométrica ha propuesto el uso del denominado modelo <em>Heckit</em>, propueto por Heckman (1979) para resolver el problema de la estimación de una ecuación de salarios para las mujeres casadas.</p>
<p>Este modelo, también conocido en la literatura como <em>modelo Tobit tipo II</em>, es básicamente un modelo <em>Probit</em> más una regresión truncada. Así, el modelo <em>Heckit</em> se compone de dos ecuaciones. La primera de ellas, la <em>ecuación de selección</em>, que se corresponde con un modelo <em>Probit</em> que determina cuándo se observa la variable de interés, se expresa en términos de una variable de selección <span class="math inline">\(s_{i}\)</span>, asociada a “todas” las observaciones muestrales</p>
<p><span class="math display">\[s_{i} = \left\{ \begin{matrix} \ 0\ \ si\ s_{i}^{*} &lt; 0 \\ \ 1\ si\ s_{i}^{*} \geq 0 \\ \end{matrix} \right.\ \]</span></p>
<p>donde la variable latente <span class="math inline">\(s_{i}^{*}\)</span> de preferencias individuales sigue un modelo de regresión lineal estándar</p>
<p><span class="math display">\[s_{i}^{*} = \alpha_{1} + \alpha_{2}w_{2i} + \ldots + \alpha_{M}w_{Mi} + u_{i}\]</span></p>
<p>La segunda es la llamada <em>ecuación de intensidad</em>, que se corresponde con un modelo de regresión lineal para la variable de interés, <span class="math inline">\(y_{i}\)</span>, la cual es válida únicamente para la muestra truncada y, por tanto,</p>
<p><span class="math display">\[y_{i} = \left\{ \begin{matrix}
\ \beta_{1} + \beta_{2}x_{2i} + \ldots + \beta_{K}x_{Ki} + e_{i}\ \ si\ s_{i}^{} = 1 \\
\ no\ se\ observa\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ si\ s_{i}^{} = 0 \\
\end{matrix} \right.\ \]</span></p>
<p>Puede demostrarse que, teniendo en cuenta el proceso por el que se generan las observaciones de la variable dependiente <span class="math inline">\(y_{i}\)</span>, se cumple que</p>
<p><span class="math display">\[E(y_{i}|s_{i} = 1) = \beta_{1} + \beta_{2}x_{2i} + \ldots + \beta_{K}x_{Ki} + \beta_{\lambda}\lambda_{i}\]</span></p>
<p>donde la variable <span class="math inline">\(\lambda_{i}\)</span>, conocida como la <em>inversa de la razón de Mills</em>, viene definida por</p>
<p><span class="math display">\[\lambda_{i} = \frac{\varphi\left( \alpha_{1} + \alpha_{2}w_{2i} + \ldots + \alpha_{M}w_{Mi} \right)}{\Phi\left( \alpha_{1} + \alpha_{2}w_{2i} + \ldots + \alpha_{M}w_{Mi} \right)}\]</span></p>
<p>Para estimar el modelo <em>Heckit</em> es frecuente utilizar el método bietápico original propuesto por Heckman, que es un procedimiento condicional aproximado que da lugar a un estimador consistente de los parámetros del modelo. Este método procede como sigue:</p>
<ul>
<li>En la primera etapa se usa la muestra completa para estimar el modelo para la variable <span class="math inline">\(s_{i}\)</span> que rige la observación de la variable de interés:</li>
</ul>
<p><span class="math display">\[P\lbrack s_{i} = 1\rbrack = \Phi(\alpha_{1} + \alpha_{2}w_{2i} + \ldots + \alpha_{M}w_{Mi})\]</span></p>
<ul>
<li>En la segunda etapa se utiliza solo la muestra truncada, compuesta por las observaciones que cumplen la condición <span class="math inline">\(s_{i} = 1\)</span>:</li>
</ul>
<p><span class="math display">\[y_{i} = \beta_{1} + \beta_{2}x_{2i} + \ldots + \beta_{K}x_{Ki} + \beta_{\lambda}{\widehat{\lambda}}_{i} + u_{i}\]</span></p>
<p>Sin embargo, la forma más eficiente de estimar el modelo <em>Heckit</em> es utilizar el método de máxima de verosimilitud, que estima simultáneamente los parámetros de las dos ecuaciones del modelo.</p>
<p>En cuanto al cálculo de los efectos marginales, se tiene en este caso que</p>
<p><span class="math display">\[\frac{\partial E(y|s = 1)}{\partial x_{j}} = \beta_{j} - \alpha_{j}G(\lambda)\]</span></p>
<p>donde la función <em>G</em> viene dada por <span class="math inline">\(G\left( \lambda_{i} \right) = \beta_{\lambda}\lambda_{i}\left( \lambda_{i} + \mathbf{w}_{i}^{'}\mathbf{\alpha} \right)\)</span>, siendo <span class="math inline">\(\lambda_{i} = \frac{\phi\left( \mathbf{w}_{i}^{'}\mathbf{\alpha} \right)}{\Phi\left( \mathbf{w}_{i}^{'}\mathbf{\alpha} \right)}\)</span> la inversa de la razón de Mills y <span class="math inline">\(\mathbf{w}_{i}^{'}\mathbf{\alpha} = \alpha_{1} + \alpha_{2}w_{2i} + \ldots + \alpha_{M}w_{Mi}\)</span>.</p>
<p>Desde un punto de vista cualitativo puede apreciarse que, para una variable <em>x<sub>j</sub></em> que únicamente aparezca en la ecuación de intensidad (es decir, no se encuentra también entre los regresores <em>w</em>’s de la ecuación de selección), se tiene que <span class="math inline">\(\frac{\partial E\left( y_{i} \middle| s_{i} = 1 \right)}{\partial x_{ji}} = \beta_{j}\)</span>, puesto que <span class="math inline">\(\alpha_{j} = 0\)</span>. Por el contrario, si la variable <em>x<sub>j</sub></em> es común a las dos ecuaciones, selección e intensidad, entonces el efecto marginal tomará la forma general <span class="math inline">\(\frac{\partial E\left( y_{i} \middle| s_{i} = 1 \right)}{\partial x_{ji}} = \beta_{j} - \alpha_{j}G\left( \lambda_{i} \right)\)</span>, y no coincidirá ya con el efecto directo <span class="math inline">\(\beta_{j}\)</span>, puesto que el valor indirecto <span class="math inline">\(\alpha_{j}G\left( \lambda_{i} \right)\)</span> será distinto de cero en este caso.</p>
</section>
</section>
</section>
<section id="paneles-de-datos" class="level2" data-number="3.11">
<h2 data-number="3.11" class="anchored" data-anchor-id="paneles-de-datos"><span class="header-section-number">3.11</span> Paneles de datos</h2>
<section id="regresiones-para-datos-de-panel-y-efectos-individuales-inobservables" class="level3" data-number="3.11.1">
<h3 data-number="3.11.1" class="anchored" data-anchor-id="regresiones-para-datos-de-panel-y-efectos-individuales-inobservables"><span class="header-section-number">3.11.1</span> Regresiones para datos de panel y efectos individuales inobservables</h3>
<p>Un modelo de regresión básico para datos de corte transversal viene dado por la expresión <span class="math inline">\(y_{i} = \beta_{1} + \beta_{2}x_{2i} + \ldots + \beta_{K}x_{Ki} + e_{i}\)</span> donde <span class="math inline">\(i = 1,2,\ldots,n\)</span>. Cuando cada unidad transversal <em>i</em> se observa durante <em>T</em> períodos, el modelo resultante será</p>
<p><span class="math display">\[y_{it} = \beta_{1} + \beta_{2}x_{2it} + \cdots + \beta_{K}x_{Kit} + e_{it}\]</span></p>
<p>para <span class="math inline">\(i = 1,\ldots,n\)</span> y <span class="math inline">\(t = 1,\ldots,T\)</span>. Este será el prototipo de un modelo estático para datos de panel.</p>
<p>Las unidades transversales pueden ser individuos, familias, regiones, países, empresas, etc. En general, se hablará de <em>datos de panel</em> (también llamados <em>datos longitudinales</em>) cuando <em>n</em> es grande (varios cientos o incluso miles) relativo al número de períodos de observación <em>T</em> (normalmente de 2 a 10 períodos en la mayoría de los casos, y raras veces excediendo 20 observaciones temporales).</p>
<p>Para controlar la presencia de efectos individuales inobservables se supone que <span class="math inline">\(\ e_{it} = \mu_{i} + \nu_{it}\ \)</span>, donde <span class="math inline">\(\mu_{i}\)</span> recoge la heterogeneidad transversal no observada (diferencias entre unidades) y <span class="math inline">\(\nu_{it}\)</span> representa el término de perturbación con las propiedades estándar (media cero, varianza constante y no autocorrelacionado, ni entre unidades ni a lo largo del tiempo). Según que se asuma que el efecto <span class="math inline">\(\mu_{i}\)</span> es un parámetro fijo o una variable aleatoria se tendrá el <em>modelo de efectos fijos</em> o el <em>modelo de efectos aleatorios</em>.</p>
</section>
<section id="ventajas-y-limitaciones-derivadas-del-uso-de-paneles-de-datos" class="level3" data-number="3.11.2">
<h3 data-number="3.11.2" class="anchored" data-anchor-id="ventajas-y-limitaciones-derivadas-del-uso-de-paneles-de-datos"><span class="header-section-number">3.11.2</span> Ventajas y limitaciones derivadas del uso de paneles de datos</h3>
<p>Entre las ventajas de emplear datos de panel pueden incluirse las siguientes:</p>
<ol type="1">
<li><p>Control de la heterogeneidad individual: los datos transversales o temporales no son capaces, por sí solos, de controlar la heterogeneidad inherente en el comportamiento de los individuos, empresas, regiones o países, corriéndose el riesgo de obtener estimaciones sesgadas cuando se utilizan datos exclusivamente de un tipo o de otro. Sin embargo, a través del uso de datos de panel pueden controlarse estos efectos específicos, transversales o temporales, sean observables o no (generalmente no lo serán).</p></li>
<li><p>Proporciona datos con mayor cantidad de información, con mayor grado de variabilidad y con menor nivel de colinealidad entre los regresores; y también aumenta el número de grados de libertad y, por tanto, da lugar a una mayor eficiencia en las estimaciones.</p></li>
<li><p>Son un medio adecuado para estudiar procesos dinámicos de ajuste, ya que a partir de ellos se pueden analizar los cambios en el tiempo de las distribuciones transversales.</p></li>
<li><p>Ayudan a identificar y medir efectos que no son detectables con datos puros de corte transversal o de series temporales.</p></li>
<li><p>Permiten construir y contrastar modelos de comportamiento más complejos que con datos más simples.</p></li>
<li><p>Puesto que las unidades transversales de un panel de datos normalmente se refieren a individuos, familias o empresas (más que a países o regiones, para los cuales no se utiliza generalmente el término de panel de datos sino de <em>pool</em> temporal de secciones cruzadas), se evitan los sesgos que resultan cuando se trabaja con variables agregadas.</p></li>
</ol>
<p>Por otro lado, entre las limitaciones cabe destacar las siguientes:</p>
<ol type="1">
<li><p>Problemas de diseño muestral y de recogida de datos relacionados con inadecuadas tasas de cobertura, falta de respuesta, frecuencia y lapso temporal, período de referencia, etc.</p></li>
<li><p>Distorsiones provocadas por los errores de medida, que pueden aparecer por la falta de respuesta, errores de memoria, respuestas incorrectas deliberadas, etc.</p></li>
<li><p>Problemas de selección muestral tales como no aleatoriedad, autoselección, no respuesta inicial o abandono (sesgo por atrición).</p></li>
<li><p>En general, escasa dimensión temporal, lo que invalida algunos argumentos asintóticos y hace que la mayor parte de estos recaigan sobre el tamaño del corte transversal.</p></li>
<li><p>Dependencia espacial, es decir, correlación entre las unidades transversales.</p></li>
</ol>
</section>
<section id="modelo-de-efectos-fijos" class="level3" data-number="3.11.3">
<h3 data-number="3.11.3" class="anchored" data-anchor-id="modelo-de-efectos-fijos"><span class="header-section-number">3.11.3</span> Modelo de efectos fijos</h3>
<p>El modelo de efectos fijos, también conocido como <em>modelo de variables ficticias</em>, toma la forma</p>
<p><span class="math display">\[y_{it} = \alpha_{i} + \beta_{2}x_{2it} + \cdots + \beta_{K}x_{Kit} + \nu_{it}\]</span></p>
<p>para <span class="math inline">\(i = 1,\ldots,n\)</span> y <span class="math inline">\(t = 1,\ldots,T\)</span>, donde se ha denotado por <span class="math inline">\(\alpha_{i}\)</span> al término <span class="math inline">\(\beta_{1} + \mu_{i}\)</span>, habiéndose considerado los valores <span class="math inline">\(\mathbf{\mu}_{\mathbf{i}}\)</span> como parámetros fijos que deben estimarse.</p>
<p>Agrupando las <em>T</em> observaciones temporales para cada unidad transversal <em>i</em> se tiene el modelo <span class="math inline">\(\mathbf{y}_{i} = \mathbf{i}\alpha_{i} + \mathbf{X}_{i}\beta + \mathbf{v}_{i}\)</span>, y ordenando las unidades transversales se llega al modelo</p>
<p><span class="math display">\[\mathbf{y} = \begin{bmatrix} y_{1} \\ y_{2} \\ \vdots \\ y_{n} \\ \end{bmatrix} = \begin{bmatrix} i &amp; \mathbf{0} &amp; \ldots &amp; \mathbf{0} \\ \mathbf{0} &amp; i &amp; \ldots &amp; \mathbf{0} \\ \vdots &amp; \vdots &amp; \vdots &amp; \vdots \\ \mathbf{0} &amp; \mathbf{0} &amp; \ldots &amp; i \\ \end{bmatrix}\begin{bmatrix} \alpha_{1} \\ \alpha_{2} \\ \vdots \\ \alpha_{n} \\ \end{bmatrix} + \begin{bmatrix} X_{1} \\ X_{2} \\  \vdots \\ X_{n} \\ \end{bmatrix}\mathbf{\beta} + \begin{bmatrix} v_{1} \\ v_{2} \\  \vdots \\ v_{n} \\ \end{bmatrix}\]</span></p>
<p>que matricialmente se puede escribir como <span class="math inline">\(\mathbf{y} = \mathbf{I\alpha} + \mathbf{X\beta} + \mathbf{v}\)</span>.</p>
<p>El modelo de efectos fijos puede estimarse por MCO, regresando la variable endógena <span class="math inline">\(y_{it}\)</span> sobre las variables explicativas <span class="math inline">\(x_{kit}\)</span> y un conjunto de <em>n</em> variables ficticias individuales <em>D<sub>it</sub></em> (<em>D<sub>it</sub></em> = 1 para las <em>T</em> observaciones temporales de cada unidad <em>i</em>, y <em>D<sub>it</sub></em> = 0 para las <em>T</em> observaciones de cada unidad <em>j</em> ≠ <em>i</em>). De forma equivalente, para obtener las estimaciones MCO del vector <strong><em>β</em></strong> puede realizarse la regresión de<br>
<span class="math inline">\(y_{it} - {\overline{y}}_{i \cdot}\)</span> sobre las variables <span class="math inline">\(x_{kit} - {\overline{x}}_{ki \cdot}\)</span> (donde <span class="math inline">\({\overline{z}}_{i \cdot}\)</span> representa la media temporal de la variable <em>z</em> para cada unidad <em>i</em>) y, seguidamente, estimar los <span class="math inline">\(\alpha_{i}\)</span> a partir de las relaciones <span class="math inline">\({\widehat{\alpha}}_{i} = {\overline{y}}_{i \cdot} - {\overline{x}}_{i \cdot}^{'}\widehat{\beta}\)</span>.</p>
<p>El estimador que se obtiene se denomina <em>de efectos fijos (EF)</em>, o también <em>estimador intra-grupos</em> (<em>within</em>). Suponiendo que los términos de perturbación aleatorios <span class="math inline">\(\nu_{it}\)</span> cumplen las propiedades estándar y que las variables <span class="math inline">\(x_{kit}\)</span> no están correlacionadas con los mismos, se puede demostrar que <span class="math inline">\({\widehat{\mathbf{\beta}}}_{EF}\)</span> es un estimador insesgado y consistente si se verifica que <span class="math inline">\(n \times T \rightarrow \infty\)</span> (<em>T</em> puede ser fijo y cumplirse que <span class="math inline">\(n \rightarrow \infty\)</span>), mientras que <span class="math inline">\(\widehat{\mathbf{\alpha}}\)</span> es insesgado pero no será consistente salvo que se cumpla que <span class="math inline">\(T \rightarrow \infty\)</span>.</p>
<section id="modelo-con-efectos-individuales-y-efectos-temporales-fijos" class="level4" data-number="3.11.3.1">
<h4 data-number="3.11.3.1" class="anchored" data-anchor-id="modelo-con-efectos-individuales-y-efectos-temporales-fijos"><span class="header-section-number">3.11.3.1</span> Modelo con efectos individuales y efectos temporales fijos</h4>
<p>El modelo anterior puede extenderse a un modelo de efectos fijos de <em>doble vía</em> en el que aparecen también efectos inobservables temporales</p>
<p><span class="math display">\[y_{it} = \alpha_{i} + \delta_{t} + \beta_{2}x_{2it} + \cdots + \beta_{K}x_{Kit} + \nu_{it}\]</span></p>
<p>Desde el punto de vista práctico, para estimar este modelo habría que introducir dos conjuntos de variables ficticias, unas individuales y otras temporales, y el estimador MCO tendría las mismas propiedades anotadas en el epígrafe anterior, siendo distintas para el vector <strong><em>β</em></strong> que para los efectos individuales <span class="math inline">\(\alpha_{i}\)</span> o <span class="math inline">\(\delta_{t}\)</span>.</p>
</section>
<section id="anotaciones-respecto-al-modelo-de-efectos-fijos" class="level4" data-number="3.11.3.2">
<h4 data-number="3.11.3.2" class="anchored" data-anchor-id="anotaciones-respecto-al-modelo-de-efectos-fijos"><span class="header-section-number">3.11.3.2</span> Anotaciones respecto al modelo de efectos fijos</h4>
<p>En primer lugar, hay que tener en cuenta que el número de grados de libertad en el modelo de efectos fijos es <span class="math inline">\(nT - n - T - K\)</span> o <span class="math inline">\(nT - n - K\)</span>, según que se incluyan o no efectos temporales aparte de los individuales. Existe, por tanto, una importante pérdida de grados de libertad cuando <em>n</em> es elevado.</p>
<p>El estimador EF no permite estimar el efecto de variables invariantes en el tiempo (<span class="math inline">\(x_{it} = x_{i}\text{\ ,\ \ }\forall t\)</span>) tales como el sexo, raza, religión, etc. ya que dichas variables desaparecen con la transformación en desviaciones respecto a la media. Se han propuesto en la literatura varios métodos alternativos que combinan la estimación mediante variables instrumentales con la estimación EF, que evitan el problema anterior.</p>
<p>Para contrastar la significación estadística de los efectos individuales y/o temporales se realiza un test <em>F</em> conjunto del tipo <span class="math inline">\(H_{0}:\left\{ \alpha_{1} = \alpha_{2} = \ldots = \alpha_{n} \right\}\)</span> para el caso del modelo de “una vía” y <span class="math inline">\(H_{0}:\left\{ \alpha_{1} = \alpha_{2} = \ldots = \alpha_{n}\text{\ ,\ \ }\delta_{1} = \delta_{2} = \ldots = \delta_{T} \right\}\)</span> para el caso general de “doble vía”. Si no se rechaza la hipótesis nula, el modelo correcto sería el clásico, <span class="math inline">\(y_{it} = \beta_{1} + \beta_{2}x_{2it} + \ldots + \beta_{K}x_{Kit} + \nu_{it}\)</span>, conocido en el contexto de datos de panel como <em>modelo plano</em>.</p>
</section>
</section>
<section id="modelo-de-efectos-aleatorios" class="level3" data-number="3.11.4">
<h3 data-number="3.11.4" class="anchored" data-anchor-id="modelo-de-efectos-aleatorios"><span class="header-section-number">3.11.4</span> Modelo de efectos aleatorios</h3>
<p>En este modelo, en lugar de admitir que la heterogeneidad inobservable es de tipo determinista (los parámetros <span class="math inline">\(\mu_{i}\)</span> son fijos), se supone que tiene un carácter aleatorio. Así, se parte de la hipótesis de que <span class="math inline">\(\ \mu_{i} \approx IID\left( 0,\sigma_{\mu}^{2} \right)\ \)</span> y <span class="math inline">\(\nu_{it} \approx IID\left( 0,\sigma_{\nu}^{2} \right)\)</span>, siendo ambas variables aleatorias independientes entre sí, y se supone de nuevo que las variables explicativas <span class="math inline">\(x_{kit}\)</span> son independientes de los componentes aleatorios <span class="math inline">\(\mu_{i}\)</span> y <span class="math inline">\(\nu_{it}\)</span>.</p>
<p>Desde el punto de vista conceptual, el <em>modelo de efectos aleatorios</em>, también llamado <em>modelo de componentes del error</em> (debido a que el término de error se descompone como <span class="math inline">\(e_{it} = \mu_{i} + \nu_{it}\)</span>) **es apropiado cuando las <em>n</em> unidades transversales observadas son una muestra (aleatoria) de una población mayor (individuos, familias, empresas, etc.); en este caso cabe esperar que el efecto individual se caracterice mejor por una variable aleatoria, y las inferencias que se realicen serán respecto a la población y no respecto a la muestra aleatoria extraída. Por el contrario, el modelo de efectos fijos es más apropiado cuando el análisis se centra sobre un conjunto específico de <em>n</em> unidades, y la inferencia que se haga será condicional al comportamiento de dicho conjunto particular.</p>
<p>Bajo la hipótesis de efectos aleatorios, el término de error compuesto <span class="math inline">\(e_{it} = \mu_{i} + \nu_{it}\)</span> tendrá la estructura</p>
<p><span class="math display">\[E\left\lbrack e_{it}e_{js} \right\rbrack = \left\{ \begin{matrix}
\sigma_{\mu}^{2} + \sigma_{v}^{2}\ \ \ i = j,\ t = s \\
\sigma_{\mu}^{2}\ \ \ \ \ i = j,\ t \neq s \\
0\ \ i \neq j \\
\end{matrix} \right.\ \]</span></p>
<p>Entonces, el estimador MCO del modelo <span class="math inline">\(y_{it} = \beta_{1} + \beta_{2}x_{2it} + \ldots + \beta_{K}x_{Kit} + e_{it}\)</span> no será MELI y la matriz de covarianzas no tendrá la forma estándar <span class="math inline">\(\sigma^{2}\left( \mathbf{X}^{'}\mathbf{X} \right)^{- 1}\)</span>.</p>
<p>Agrupando las <em>T</em> observaciones para cada unidad <em>i</em>, <span class="math inline">\(\mathbf{y}_{i} = \mathbf{X}_{i}\beta + \mathbf{e}_{i}\)</span>, se tendrá que<br>
<span class="math inline">\(\Omega = E\left\lbrack \mathbf{e}_{i}\mathbf{e}_{i}^{'} \right\rbrack = \sigma_{\nu}^{2}\mathbf{I}_{T} + \sigma_{\mu}^{2}\mathbf{i}\mathbf{i}^{'}\)</span> y, por tanto, puesto que las unidades <em>i</em> y <em>j</em> son independientes, la matriz de covarianzas para el modelo global con las <span class="math inline">\(n \times T\)</span> observaciones tendrá la forma</p>
<p><span class="math display">\[\mathbf{V} = \begin{bmatrix}
\mathbf{\Omega} &amp; \mathbf{0} &amp; \ldots &amp; \mathbf{0} \\
\mathbf{0} &amp; \mathbf{\Omega} &amp; \ldots &amp; \ldots \\
\vdots &amp; \vdots &amp; \mathbf{\Omega} &amp; \mathbf{0} \\
\mathbf{0} &amp; \ldots &amp; \mathbf{0} &amp; \mathbf{\Omega} \\
\end{bmatrix}\]</span></p>
<p>En resumen, habrá que utilizar el estimador de mínimos cuadrados generalizados para tener en cuenta la violación de la hipótesis de esfericidad. En este caso concreto, el estimador MCG se obtiene mediante la regresión de la variable <span class="math inline">\(y_{it} - \lambda{\overline{y}}_{i \cdot}\)</span> sobre el vector de regresores <span class="math inline">\(x_{kit} - \lambda{\overline{x}}_{ki \cdot}\)</span>, donde <span class="math inline">\(\lambda = 1 - \sqrt{\frac{\sigma_{\nu}^{2}}{T\sigma_{\mu}^{2} + \sigma_{\nu}^{2}}}\ \)</span>. En la práctica, para obtener estimaciones consistentes del parámetro <span class="math inline">\(\lambda\)</span> generalmente se utiliza como estimación de la varianza <span class="math inline">\(\sigma_{\nu}^{2}\)</span> la varianza residual obtenida del estimador EF (<em>within</em>) multiplicada por el factor <span class="math inline">\(\frac{(nT - K)}{(nT - n - K)}\)</span>, y como estimación de la varianza <span class="math inline">\(\sigma_{\mu}^{2}\)</span> el valor <span class="math inline">\({\widehat{\sigma}}_{\nu}^{2} - \frac{{\widehat{\sigma}}_{b}^{2}}{T}\)</span>, donde <span class="math inline">\({\widehat{\sigma}}_{b}^{2}\)</span> es la varianza residual estimada del <em>estimador entre-grupos</em> (<em>between</em>), calculado mediante la regresión MCO de la variable <span class="math inline">\({\overline{y}}_{i \cdot}\)</span> sobre los regresores <span class="math inline">\({\overline{x}}_{ki \cdot}\ \)</span>. El estimador resultante, llamado <em>de efectos aleatorios (EA)</em>, será asintóticamente MELI (cuando se cumpla que <span class="math inline">\(n \times T \rightarrow \infty\)</span>), consistente y se distribuirá normalmente.</p>
<section id="anotaciones-respecto-al-modelo-de-efectos-aleatorios" class="level4" data-number="3.11.4.1">
<h4 data-number="3.11.4.1" class="anchored" data-anchor-id="anotaciones-respecto-al-modelo-de-efectos-aleatorios"><span class="header-section-number">3.11.4.1</span> Anotaciones respecto al modelo de efectos aleatorios</h4>
<ul>
<li><p>El modelo de doble vía <span class="math inline">\(e_{it} = \mu_{i} + \delta_{t} + \nu_{it}\)</span> puede analizarse de una forma similar al modelo básico, considerando también los efectos <span class="math inline">\(\delta_{t}\)</span> como aleatorios y aplicando MCG. Sin embargo, lo más usual es considerar los efectos individuales como aleatorios y los temporales como efectos fijos, estimándose estos últimos mediante la introducción de las correspondientes variables ficticias.</p></li>
<li><p>Para contrastar la validez del modelo de efectos aleatorios puede utilizarse el contraste** propuesto por Breusch y Pagan (1979), que es una prueba de multiplicadores de Lagrange diseñada para contrastar <span class="math inline">\(H_{0}:\left\{ \sigma_{\mu}^{2} = 0 \right\}\)</span> frente a la alternativa <span class="math inline">\(H_{1}:\left\{ \sigma_{\mu}^{2} \neq 0 \right\}\)</span>. El estadístico propuesto, bajo la hipótesis nula, se distribuye asintóticamente como <span class="math inline">\(\lambda_{BP}\overset{as}{\sim}\chi_{1}^{2}\)</span> .</p></li>
<li><p>Por otra parte, para elegir entre los dos estimadores alternativos propuestos, el modelo de efectos fijos o el modelo de efectos aleatorios, puede utilizarse el <em>test de Hausman</em>, que compara directamente los dos estimadores. Este contraste se basa en el hecho de que, bajo la hipótesis nula <span class="math inline">\(H_{0}:E\left\lbrack \mu_{i} \middle| x_{kit} \right\rbrack = 0\)</span>, el estimador EA es asintóticamente más eficiente que el estimador EF; sin embargo, si la hipótesis alternativa <span class="math inline">\(H_{1}:E\left\lbrack \mu_{i} \middle| x_{kit} \right\rbrack \neq 0\)</span> es cierta, el estimador EF mantendrá la consistencia, pero el estimador EA será sesgado e inconsistente. El estadístico, bajo la hipótesis nula, se distribuye asintóticamente como <span class="math inline">\(H\overset{as}{\sim}\chi_{K}^{2}\)</span>. El no rechazo de <em>H</em><sub>0</sub> implicaría decantarse por el modelo de EA, que sería más eficiente que el de EF, mientras que la “aceptación” de <em>H</em><sub>1</sub> supondría inclinarse hacia el modelo de EF, que sería el único estimador consistente en esta situación.</p></li>
</ul>


</section>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./p3-mrl-extendido.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">CAPÍTULO 3: DIAGNOSIS, CORRECCIONES Y EXTENSIONES DEL MODELO DE REGRESIÓN LINEAL</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./p3c2-app1.html" class="pagination-link">
        <span class="nav-page-text">Aplicación 3.1 (Evaluación, validación y especificación del MRL): El modelo APT de valoración de activos</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>