---
title: 'TEMA 3: DIAGNOSIS DEL MODELO DE REGRESIÓN LINEAL'
author:
  name: Julián Ramajo, ramajo@unex.es
  affiliation: GRADO EN ESTADÍSTICA | ECONOMETRIA (502243)
subtitle: 'Aplicación 3.1: El modelo APT (Arbitrage Pricing Theory)'
output:
  html_document:
    theme: journal
    highlight: haddock
    toc: yes
    toc_depth: 2
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '2'
---

# Introducción

En esta aplicación se estimará un modelo de regresión basado en la teoría de los precios de arbitraje (APT). Para más información sobre esta teoría se puede consultar la página: <https://es.wikipedia.org/wiki/Teor%C3%ADa_del_arbitraje>.

Definamos la rentabilidad de un activo (en porcentaje) como  R=100×log⁡((p_1+d)/p_0 ), donde p_1 y p_0 son, respectivamente, los precios de cotización del valor (título u obligación) al final y al principio de un período de tiempo y d es el dividendo cobrado (si lo hay) durante ese período. Denominemos por Rf al rendimiento de un activo libre de riesgo. Finalmente, denotemos por Rm a la rentabilidad que ofrece la cartera de mercado.

En este ejercicio se va a estimar el modelo APT con datos de series temporales de carácter mensual para el período que va desde marzo de 1986 hasta marzo de 2018.

Se utilizará como rentabilidad de referencia la correspondiente a las acciones de la empresa Microsoft, que denotaremos por R_MICROSOFT. Como rentabilidad sin riesgo, se usará el tipo de interés correspondiente a las letras del tesoro estadounidenses a tres meses, R_USTB3M, y como rentabilidad del mercado se tomará la asociada al índice S&P500, R_SP500. Las variables básicas que deben utilizarse en el modelo de regresión son, entonces, los ‘excesos de rentabilidad’, ER_MSOFT=(R_MICROSOFT-R_USTB3M) y ER_SP=(R_SP500-R_USTB3M).

La ecuación de valoración básica del modelo APT puede definirse del siguiente modo:

$$
ERMSOFT_{t} = \beta_1 + \beta_2  ERSP_{t} + \gamma_1  F_{1,t} + \gamma_2  F_{2,t} + ...+ \gamma_m  F_{m,t} + e_{t}
$$
donde las variables _F_ representan distintos factores que pueden afectar a la rentabilidad observada del título.

# Código R

```{r, message=FALSE}
library(tidyverse)
library(readxl)
#
macro <- read_excel("APT_MICROSOFT.xls")
dim(macro)
head(macro)
tail(macro)
summary(macro)
#
macro$dspread = c(NA,diff(macro$BMINUSA))
macro$dcredit = c(NA,diff(macro$CCREDIT))
macro$dprod = c(NA,diff(macro$INDPRO))
macro$dmoney = c(NA,diff(macro$M1SUPPLY))
macro$inflation = c(NA,100*diff(log(macro$CPI)))
macro$rterm = c(NA,diff(macro$USTB10Y-macro$USTB3M))
macro$dinflation = c(NA,100*diff(macro$inflation))
macro$r_msoft = c(NA,100*diff(log(macro$MICROSOFT)))
macro$r_sp = c(NA,100*diff(log(macro$SANDP)))
macro$er_sp = c(NA,100*diff(log(macro$SANDP)))-macro$USTB3M/12
macro$er_msoft = c(NA,100*diff(log(macro$MICROSOFT)))-macro$USTB3M/12
# Dar formato a la fechas
macro$Date = as.Date(macro$Date)
#
# ESTIMACIÓN MCO DEL MODELO
#
APT_msoft  <-  lm(er_msoft ~ er_sp + dprod + dcredit + dinflation + dmoney + dspread + rterm, data = macro)
summary(APT_msoft)
confint(APT_msoft, level=.95)
#
# ts_macro <- ts(macro, start=c(1986,3), end=c(2018,3), frequency = 12)
# APT_msoft_ts  <-  lm(er_msoft ~ er_sp + dprod + dcredit + dinflation + dmoney + dspread + rterm, data = ts_macro)
# summary(APT_msoft_ts)
#
# Contraste de hipótesis
# library(car)
# linearHypothesis(APT_msoft_0,c("dprod=0","dcredit=0","dmoney=0","dspread=0"))
#
# DIAGNOSIS DEL MODELO DE REGRESIÓN LIENAL
#
# VALIDACIÓN GLOBAL DE LAS HIPÓTESIS
#
library(gvlma)
gvmodel <- gvlma(APT_msoft)
summary(gvmodel)
#
# CHEQUEO GENERAL DEL MODELO
#
library(performance)
model_performance(APT_msoft)
check_model(APT_msoft)
# library(lindia)
# gg_diagnose(APT_msoft, plot.all=TRUE)
#
# CHEQUEO INDIVIDUAL DE CADA HIPÓTESIS
#
# ESPECIFICACIÓN DEL MODELO
#
# Forma funcional: test RESET de Ramsey
#
library(lmtest)
resettest(APT_msoft, power=2, type="fitted")
resettest(APT_msoft, power=2:3, type="fitted")
#
# Estabilidad de los parámetros estructurales
#
library(strucchange)
sbtest = Fstats(APT_msoft,data = macro) # Recorte del 15% en ambos lados de la muestra (15% de 383 obs. -> 57)
# Test de Chow con punto de ruptura conocido
JAN1996 = match(as.Date("1996-01-01"),macro$Date)
chow = sbtest$Fstats[JAN1996-2-57] # Punto ruptura (breakpoint): 2 NAs + 57 -> 59
chow
1-pchisq(chow,sbtest$nreg)
# Test de Chow recursivo (punto de ruptura desconocido)
sctest(sbtest)
bp = which.max(sbtest$Fstats)+59
macro$Date[bp]
# Estimación recursiva y test CUSUM
# Parámetros recursivos
beta = NULL # variable para introducir en cada paso el beta y su error estándar estimados
for (t in 20:nrow(macro)){
  lr = summary(lm(formula(APT_msoft), data = macro[3:t,]))
  beta = rbind(beta,lr$coefficients["er_sp",1:2])
  }
x_axis = macro$Date[20:nrow(macro)]
plot(x_axis,beta[,1],type = "l",ylim = c(0,3),xlab="",ylab="Beta")
lines(x_axis,beta[,1]+2*beta[,2],lty="dashed")
lines(x_axis,beta[,1]-2*beta[,2],lty="dashed")
# Test CUSUM
plot(efp(APT_msoft,data=macro))
# Cambio estructural y datos atípicos
# Sucesos especiales (uso de variables ficticias para modelizar la presencia de outliers aislados)
plot(macro$Date[-(1:2)],APT_msoft$residuals,type = "l", col="red",xlab="",ylab="")
lines(macro$Date[-(1:2)],APT_msoft$fitted.values)
legend("bottomright",c("Residuals","Fitted"), col = c("red","black"),lty=1)
#
macro$APR2000DUM = as.integer(macro$Date == as.Date("2000-04-01"))
macro$DEC2000DUM = as.integer(macro$Date == as.Date("2000-12-01"))
APT_msft_dummy = lm(er_msoft ~ er_sp + dprod + dcredit + dinflation + dmoney + dspread + rterm + APR2000DUM + DEC2000DUM, data = macro)
summary(APT_msft_dummy)
# Períodos especiales (conocidos a priori)
require(lubridate)
macro$JANDUM = as.integer(month(macro$Date) == 1)
summary(lm(er_msoft ~  er_sp + dprod + dcredit + dinflation + dmoney + dspread + rterm + APR2000DUM + DEC2000DUM + JANDUM, data = macro))
#
# TÉRMINO DE ERROR DEL MODELO
#
# Normalidad  de los errores
#
# Histograma de los residuos
hist(APT_msoft$residuals,main = "")
box()
# Estadísticos de normalidad
library(moments)
skewness(APT_msoft$residuals)
kurtosis(APT_msoft$residuals)
jarque.test(APT_msoft$residuals)
agostino.test(APT_msoft$residuals)
anscombe.test(APT_msoft$residuals)
#
# Heteroscedasticidad
#
# Gráfico de errores estimados (residuos)
plot(macro$Date[-(1:2)],APT_msoft$residuals,type = "l",xlab="",ylab="")
# Test de White
library(skedastic)
white_lm(APT_msoft)
white_lm(APT_msoft, interactions = TRUE)
# Test de Breusch-Pagan de heteroscedasticidad aditiva
library(skedastic)
glejser(APT_msoft)
breusch_pagan(APT_msoft)
library(lmtest)
bptest(formula(APT_msoft),data = macro,studentize = F)
bptest(formula(APT_msoft),data = macro,studentize = T)
#
# Heteroscedasticidad autorregresiva condicional (ARCH)
#
library(FinTS)
# Test de Engle
ArchTest(APT_msoft$residuals , lags = 1)
# Corrección de la matriz de covarianzas (MCO corregidos)
library(sandwich)
coeftest(APT_msoft,vcov. = vcovHC(APT_msoft,type="HC1")) # Corrección de White
#
# Autocorrelación
#
# Test de Durbin-Watson
dwtest(APT_msoft)
# Test de Breusch-Godfrey
bgtest(APT_msoft,order = 1)
bgtest(APT_msoft,order = 6)
# Corrección de la matriz de covarianzas
library(sandwich)
coeftest(APT_msoft,vcov. = NeweyWest(APT_msoft,lag = 6,adjust = T,prewhite = F)) # Corrección de Newey-West
#
# Multicolinealidad
#
library(car)
library(corrplot)
# Matriz de correlaciones
cor(macro[-(1:2),c("er_sp","dprod","dcredit","dinflation","dmoney","dspread","rterm")])
corrplot(cor(macro[-(1:2),c("er_sp","dprod","dcredit","dinflation","dmoney","dspread","rterm")]))
# FIVs
vif(APT_msoft)
```

# Código Python

```{python, message=FALSE}
import pandas as pd
import numpy as np
import statsmodels.api as sm
import statsmodels.formula.api as smf
import statsmodels.stats.api as sms
from statsmodels.stats.diagnostic import het_arch
from statsmodels.stats.outliers_influence import reset_ramsey
from statsmodels.compat import lzip
import scipy.stats as scs
import matplotlib.pyplot as plt
import seaborn as sns
#
macro = pd.read_excel('APT_MICROSOFT.xls', index_col=0)
macro.head()
macro.tail()
#
def LogDiff(x):
    x_diff = 100*np.log(x/x.shift(1))
    return x_diff
data = pd.DataFrame({'dspread' : macro['BMINUSA'] - macro['BMINUSA'].shift(1),
                    'dcredit' : macro['CCREDIT'] - macro['CCREDIT'].shift(1),
                    'dprod' : macro['INDPRO'] - macro['INDPRO'].shift(1),
                    'r_msoft' : LogDiff(macro['MICROSOFT']),
                    'r_sp' : LogDiff(macro['SANDP']),
                    'dmoney' : macro['M1SUPPLY'] - macro['M1SUPPLY'].shift(1),
                    'inflation' : LogDiff(macro['CPI']),
                    'term' : macro['USTB10Y'] - macro['USTB3M'],
                    'dinflation' : LogDiff(macro['CPI']) - LogDiff(macro['CPI']).shift(1),
                    'mustb3m' : macro['USTB3M']/12,
                    'rterm' : (macro['USTB10Y'] - macro['USTB3M']) - (macro['USTB10Y'] - macro['USTB3M']).shift(1),
                    'er_msoft' : LogDiff(macro['MICROSOFT']) - macro['USTB3M']/12,
                    'er_sp' : LogDiff(macro['SANDP']) - macro['USTB3M']/12})
data = data.dropna()
#
# ESTIMACIÓN MCO DEL MODELO
#
formula = 'er_msoft ~ er_sp + dprod + dcredit + dinflation + dmoney + dspread + rterm'
results = smf.ols(formula, data).fit()
print(results.summary())
# Contraste de hipótesis
# hypotheses = 'dprod = dcredit = dmoney = dspread = 0'
# F_test = results.f_test(hypotheses)
# print(F_test)
#
# DIGANOSIS DEL MODELO
#
# ESPECIFICACIÓN DEL MODELO
#
# Forma funcional: test RESET de Ramsey
reset_ramsey(results,degree=2)
reset_ramsey(results,degree=3)
#
# Estabilidad de los parámetros
#
# Test de Chow (punto de ruptura conocido)
# Definición del test de Chow
#    inputs:
#       data: a pandas DataFrame of independent and dependent variable
#    outputs:
#        rss: the sum of residuals
#        N: the observations of inputs
#        K: total number of parameters
#
def get_rss(data):
    formula = 'er_msoft ~ er_sp + dprod + dcredit + dinflation + dmoney + dspread + rterm'
    results = smf.ols(formula, data).fit()
    rss = (results.resid**2).sum() # obtain the residuals sum of square
    N = results.nobs
    K = results.df_model
    return rss, N, K
# División por submuestras (split samples)
data1 = data[:'1996-01-01']
data2 = data['1996-01-01':]
# Muestra completa
RSS_total, N_total, K_total = get_rss(data)
# Primera submuestra
RSS_1, N_1, K_1 = get_rss(data1)
# Segunda submuestra
RSS_2, N_2, K_2 = get_rss(data2)
# Estadístico de contraste
nominator = (RSS_total - (RSS_1 + RSS_2)) / K_total
denominator = (RSS_1 + RSS_2) / (N_1 + N_2 - 2*K_total)
result = nominator/denominator
# 
result
#
# Test de Chow recursivo (punto de ruptura desconocido)
name = ['test statistic', 'pval', 'crit']
test = sms.breaks_cusumolsresid(resid = results.resid, ddof = results.df_model)
lzip(name, test)
# Estimación recursiva y test CUSUM
data = data.dropna()
# Definición de los estadísticos recursivos
#
#    inputs:
#        variable: the string literals of a variable name in regression formula
#        i: the serial number of regression
#        interval: the number of consective data points in initial sample
#    outputs:
#        coeff: the coefficient estimation of the variable
#        se: the standard errors of the variable
# 
def recursive_reg(variable, i, interval):
    formula = 'er_msoft ~ er_sp + dprod + dcredit + dinflation + dmoney + dspread + rterm'
    results = smf.ols(formula, data.iloc[:i+interval]).fit()
    coeff = results.params[variable]
    se = results.bse[variable]
    return coeff, se
#
parameters = []
for i in range(373):
    coeff, se = recursive_reg('er_sp', i, 11)
    parameters.append((coeff,se))
parameters = pd.DataFrame(parameters, columns=['coeff','se'], index = data[:-10].index)
parameters['er_sp + 2*se'] = parameters['coeff'] + 2*parameters['se']
parameters['er_sp - 2*se'] = parameters['coeff'] - 2*parameters['se']
#
plt.figure(1)
plt.plot(parameters['coeff'], label=r'$\beta_{er_sp}$')
plt.plot(parameters['er_sp + 2*se'], label=r'$\beta_{er_sp} + 2*se$', linestyle='dashed')
plt.plot(parameters['er_sp - 2*se'], label=r'$\beta_{er_sp} - 2*se$', linestyle='dashed')
plt.xlabel('Date')
plt.grid(True)
plt.legend()
plt.show()
# Modelización de sucesos especiales (uso de variables ficticias para tratar la presencia de outliers aislados)
y_fitted = results.fittedvalues
residuals = results.resid
#
plt.figure(2)
plt.plot(residuals, label='Residuos')
plt.plot(y_fitted, label='Valores ajustados')
plt.xlabel('Date')
plt.ylabel('Desviaciones')
plt.grid(True)
plt.legend()
plt.show()
# Identificación de valores menores
residuals.nsmallest(2)
#
data['APR2000DUM'] = np.where(data.index == '2000-4-1', 1, 0)
data['DEC2000DUM'] = np.where(data.index == '2000-12-1', 1, 0)
#
formula = 'er_msoft ~ er_sp + dprod + dcredit + dinflation + dmoney + dspread + rterm + APR2000DUM + DEC2000DUM'
results = smf.ols(formula, data).fit()
print(results.summary())
#
# TÉRMINO DE ERROR DEL MODELO
#
# Normalidad de los errores
#
residuals = results.resid
# Histograma de los residuos
plt.figure(3)
plt.hist(residuals,20,edgecolor='black',linewidth=1.2)
plt.xlabel('Residuos')
plt.ylabel('Densidad estimada')
plt.show()
# Test de Jarque-Bera
name = ['Jarque-Bera', 'Chi^2 two-tail prob.', 'Skewness', 'Kurtosis']
JB_test = sms.jarque_bera(residuals)
lzip(name, JB_test)
# Contrastes de normalidad para una variable (var: ndarray)
def normality_tests(var):
  Skewness_test = scs.skew(var)
  Skewness_pvalue = scs.skewtest(var)[1]
  Kurtosis_test = scs.kurtosis(var)
  Kurtosis_pvalue = scs.kurtosistest(var)[1]
  Normality_test = scs.normaltest(var)
  return Skewness_test, Skewness_pvalue, Kurtosis_test, Kurtosis_pvalue, Normality_test
#
print(normality_tests(residuals))
#
name = ['Skewness_test', 'Skewness_pvalue', 'Kurtosis_test', 'Kurtosis_pvalue', 'Normality_test']
ntests = normality_tests(residuals)
lzip(name, ntests)
#
# Heteroscedasticidad
#
# Gráfico de errores estimados (residuos)
plt.figure(4)
plt.plot(results.resid)
plt.xlabel('Date')
plt.ylabel('Residuos')
plt.grid(True)
plt.show()
# Test de White
name = ['Lagrange multiplier statistic', 'p-value', 
        'f-value', 'f p-value']
W_test = sms.het_white(results.resid, results.model.exog)
lzip(name, W_test)
# Test de Breusch-Pagan (heter. aditiva)
name = ['Lagrange multiplier statistic', 'p-value', 
        'f-value', 'f p-value']
BP_test = sms.het_breuschpagan(results.resid, results.model.exog)
lzip(name, BP_test)
# Heteroscedasticidad autorregresiva condicional (ARCH): test de Engle
name = ['lm','lmpval','fval','fpval'] 
E_test = het_arch(results.resid, nlags=1)
lzip(name,E_test)
# Corrección de la matriz de covarianzas: estimador de White
results1 = smf.ols(formula, data).fit(cov_type='HC1')
print(results1.summary())
#
# Autocorrelación
#
# Test de Durbin-Watson
residuals = results.resid
sms.durbin_watson(residuals)
# Test de Breusch-Godfrey
name = ['Lagrange multiplier statistic', 'p-value', 
        'f-value', 'f p-value']
BG_test = sms.acorr_breusch_godfrey(results,1)
lzip(name, BG_test)
#
name = ['Lagrange multiplier statistic', 'p-value', 
        'f-value', 'f p-value']
BG_test = sms.acorr_breusch_godfrey(results,6)
lzip(name, BG_test)
# Corrección de la matriz de covarianzas: estimador de Newey-West (si maxlags=0 -> HAC=HC1)
results2 = smf.ols(formula, data).fit(cov_type='HAC', cov_kwds={'maxlags':6,'use_correction':True})
print(results2.summary())
#
# Multicolinealidad
#
# Matriz de correlaciones
data = data.dropna()
dataX = data[['er_sp','dprod','dcredit','dinflation','dmoney','dspread','rterm']]
dataX.corr()
plt.figure(5)
sns.heatmap(dataX.corr(), vmin=-0.25, vmax=0.25, center=0, annot=True, fmt='.2f', mask=~np.tri(dataX.corr().shape[1], k=-1, dtype=bool), cbar=False)
plt.show()
```

