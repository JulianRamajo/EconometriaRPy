---
title: 'TEMA 3: Diagnosis del modelo de regresión lineal'
author:
  name: Julián Ramajo, ramajo@unex.es
  affiliation: GRADO EN ESTADÍSTICA | ECONOMETRIA (502243)
subtitle: 'Aplicación 3.1: El modelo APT (Arbitrage Pricing Theory) de valoración de activos'
output:
  html_document:
    theme: journal
    highlight: haddock
    toc: yes
    toc_depth: 2
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '2'
---

# Introducción

En esta aplicación se estimará un modelo de regresión basado en la teoría de los precios de arbitraje (APT). Para más información sobre esta teoría se puede consultar la página: <https://es.wikipedia.org/wiki/Teor%C3%ADa_del_arbitraje>.

Definamos la rentabilidad de un activo financiero (en porcentaje) como  R=100×log⁡((p1+d)/p0), donde p1 y p0 son, respectivamente, los precios de cotización del valor (título u obligación) al final y al principio de un período de tiempo y d es el dividendo cobrado (si lo hay) durante ese período. Denominemos por Rf al rendimiento de un activo libre de riesgo. Finalmente, denotemos por Rm a la rentabilidad que ofrece la cartera de mercado.

En este ejercicio se va a estimar el modelo APT con datos de series temporales de carácter mensual para el período que va desde marzo de 1986 hasta marzo de 2018.

Se utilizará como rentabilidad de referencia la correspondiente a las acciones de la empresa Microsoft, que denotaremos por R_MICROSOFT. Como rentabilidad sin riesgo, se usará el tipo de interés correspondiente a las letras del tesoro estadounidenses a tres meses, R_USTB3M, y como rentabilidad del mercado se tomará la asociada al índice S&P500, R_SP500. Las variables básicas que deben utilizarse en el modelo de regresión son, entonces, los ‘excesos de rentabilidad’, erMSOFT=(R_MICROSOFT-R_USTB3M) y erSP=(R_SP500-R_USTB3M).

La ecuación de valoración básica del modelo APT puede definirse del siguiente modo:

$$
erMSOFT_{t} = \beta_1 + \beta_2  erSP_{t} + \gamma_1  F_{1,t} + \gamma_2  F_{2,t} + ...+ \gamma_m  F_{m,t} + e_{t}
$$
donde las variables _F_ representan distintos factores que pueden afectar a la rentabilidad observada del título.

# Código R

```{r, message=FALSE}
# Lectura de librerías
library(tidyverse)
library(readxl)
# Lectura de datos
apt <- read_excel("APT_MICROSOFT.xls")
dim(apt)
head(apt)
tail(apt)
summary(apt[,2:10])
# Transformación de variables
apt$dspread = c(NA,diff(apt$BMINUSA))
apt$dcredit = c(NA,diff(apt$CCREDIT))
apt$dprod = c(NA,diff(apt$INDPRO))
apt$dmoney = c(NA,diff(apt$M1SUPPLY))
apt$inflation = c(NA,100*diff(log(apt$CPI)))
apt$rterm = c(NA,diff(apt$USTB10Y-apt$USTB3M))
apt$dinflation = c(NA,100*diff(apt$inflation))
apt$r_msoft = c(NA,100*diff(log(apt$MICROSOFT)))
apt$r_sp = c(NA,100*diff(log(apt$SANDP)))
apt$er_sp = c(NA,100*diff(log(apt$SANDP)))-apt$USTB3M/12
apt$er_msoft = c(NA,100*diff(log(apt$MICROSOFT)))-apt$USTB3M/12
# Dar formato a la fechas
apt$Date = as.Date(apt$Date)
# Estimación MCO del modelo APT
APT_msft  <-  lm(er_msoft ~ er_sp + dprod + dcredit + dinflation + dmoney + dspread + rterm, data = apt)
summary(APT_msft)
confint(APT_msft, level=.95)
# apt_ts <- ts(apt, start=c(1986,3), end=c(2018,3), frequency = 12)
# APT_msft_ts  <-  lm(er_msoft ~ er_sp + dprod + dcredit + dinflation + dmoney + dspread + rterm, data = apt_ts)
# summary(APT_msft_ts)
# Contraste de significación de las variables dprod, dcredit, dmoney y dspread
# library(car)
# linearHypothesis(APT_msft_0,c("dprod=0","dcredit=0","dmoney=0","dspread=0"))
#
# Diagnosis del modelo estimado
# Validación global de las hipótesis básicas del MRL
library(gvlma)
gvmodel <- gvlma(APT_msft)
summary(gvmodel)
# Chequeo general del modelo estimado
library(performance)
model_performance(APT_msft)
check_model(APT_msft)
# library(lindia)
# gg_diagnose(APT_msft, plot.all=TRUE)
#
# Cheque individual de cada hipótesis
# Especificación de la forma funcional: el test RESET de Ramsey
library(lmtest)
resettest(APT_msft, power=2, type="fitted")
resettest(APT_msft, power=2:3, type="fitted")
# Estabilidad de los parámetros estructurales
library(strucchange)
sbtest = Fstats(APT_msft,data = apt) # Recorte bilateral del 15% de la muestra (15% de 383 obs. -> 57)
# Test de Chow con punto de ruptura conocido
JAN1996 = match(as.Date("1996-01-01"), apt$Date)
Chow_Chi2_test = sbtest$Fstats[JAN1996-2-57] # Punto ruptura (breakpoint): 2 NAs + 57 -> 59
Chow_Chi2_test
1-pchisq(Chow_Chi2_test,sbtest$nreg)
# Test de Chow recursivo (punto de ruptura desconocido)
sctest(sbtest)
bp = which.max(sbtest$Fstats)+59
apt$Date[bp]
# Estimación recursiva y test CUSUM
# Parámetros recursivos
beta = NULL # variable para introducir en cada paso el beta y su error estándar estimados
for (t in 20:nrow(apt)){
  lr = summary(lm(formula(APT_msft), data = apt[3:t,]))
  beta = rbind(beta,lr$coefficients["er_sp",1:2])
  }
x_axis = apt$Date[20:nrow(apt)]
plot(x_axis,beta[,1],type = "l",ylim = c(0,3),xlab="",ylab="Beta")
lines(x_axis,beta[,1]+2*beta[,2],lty="dashed")
lines(x_axis,beta[,1]-2*beta[,2],lty="dashed")
# Test CUSUM
plot(efp(APT_msft,data=apt))
# Cambio estructural y datos atípicos
# Sucesos especiales (uso de variables ficticias para modelizar la presencia de outliers aislados)
plot(apt$Date[-(1:2)],APT_msft$residuals,type = "l", col="red",xlab="",ylab="")
lines(apt$Date[-(1:2)],APT_msft$fitted.values)
legend("bottomright",c("Residuals","Fitted"), col = c("red","black"),lty=1)
# Creación de variables ficticias y estimación del modelo ampliado
apt$APR2000DUM = as.integer(apt$Date == as.Date("2000-04-01"))
apt$DEC2000DUM = as.integer(apt$Date == as.Date("2000-12-01"))
APT_msft_dummy = lm(er_msoft ~ er_sp + dprod + dcredit + dinflation + dmoney + dspread + rterm + APR2000DUM + DEC2000DUM, data = apt)
summary(APT_msft_dummy)
# Períodos especiales (conocidos a priori)
require(lubridate)
apt$JANDUM = as.integer(month(apt$Date) == 1)
summary(lm(er_msoft ~  er_sp + dprod + dcredit + dinflation + dmoney + dspread + rterm + APR2000DUM + DEC2000DUM + JANDUM, data = apt))

# Término de error del modelo
# Normalidad  de los errores
# Histograma de los residuos
hist(APT_msft_dummy$residuals,main = "")
box()
# Estadísticos de normalidad
library(moments)
skewness(APT_msft_dummy$residuals)
kurtosis(APT_msft_dummy$residuals)
jarque.test(APT_msft_dummy$residuals)
agostino.test(APT_msft_dummy$residuals)
anscombe.test(APT_msft_dummy$residuals)
# Heteroscedasticidad
# Gráfico de errores estimados (residuos)
plot(apt$Date[-(1:2)],APT_msft_dummy$residuals,type = "l",xlab="",ylab="")
# Test de White
library(skedastic)
white_lm(APT_msft_dummy)
white_lm(APT_msft_dummy, interactions = TRUE)
# Test de Breusch-Pagan de heteroscedasticidad aditiva
library(skedastic)
glejser(APT_msft_dummy)
breusch_pagan(APT_msft_dummy)
library(lmtest)
bptest(formula(APT_msft_dummy),data = apt)
bptest(formula(APT_msft_dummy),data = apt,studentize = F)
# Heteroscedasticidad autorregresiva condicional (ARCH)
library(FinTS)
# Test de Engle
ArchTest(APT_msft_dummy$residuals , lags = 1)
# Corrección de la matriz de covarianzas
library(sandwich)
coeftest(APT_msft_dummy,vcov. = vcovHC(APT_msft,type="HC1")) # Corrección de White
# Autocorrelación
# Test de Durbin-Watson
dwtest(APT_msft_dummy)
# Test de Breusch-Godfrey
bgtest(APT_msft_dummy,order = 1)
bgtest(APT_msft_dummy,order = 6)
# Corrección de la matriz de covarianzas
library(sandwich)
coeftest(APT_msft_dummy,vcov. = NeweyWest(APT_msft,lag = 6, adjust = T, prewhite = F)) # Corrección de Newey-West
# Multicolinealidad
library(car)
library(corrplot)
# Matriz de correlaciones
cor(apt[-(1:2),c("er_sp","dprod","dcredit","dinflation","dmoney","dspread","rterm")])
corrplot(cor(apt[-(1:2),c("er_sp","dprod","dcredit","dinflation","dmoney","dspread","rterm")]))
# FIVs
vif(APT_msft)
```

# Código Python

```{python, message=FALSE}
# Lectura de librerías
import pandas as pd
import numpy as np
import statsmodels.api as sm
import statsmodels.formula.api as smf
import statsmodels.stats.api as sms
import statsmodels.stats as smstats
import statsmodels.stats.diagnostic as smsdiag
from statsmodels.stats.outliers_influence import reset_ramsey
from statsmodels.compat import lzip
import scipy.stats as scs
import matplotlib.pyplot as plt
import seaborn as sns
# Lectura de datos
apt = pd.read_excel('APT_MICROSOFT.xls', index_col=0)
apt.head()
apt.tail()
apt.describe()
# Transformación de variables
def LogDiff(x):
    x_diff = 100*np.log(x/x.shift(1))
    return x_diff
#
apt['dspread'] = apt['BMINUSA'] - apt['BMINUSA'].shift(1)
apt['dcredit'] = apt['CCREDIT'] - apt['CCREDIT'].shift(1)
apt['dprod'] = apt['INDPRO'] - apt['INDPRO'].shift(1)
apt['r_msft'] = LogDiff(apt['MICROSOFT'])
apt['r_sp'] = LogDiff(apt['SANDP'])
apt['dmoney'] = apt['M1SUPPLY'] - apt['M1SUPPLY'].shift(1)
apt['inflation'] = LogDiff(apt['CPI'])
apt['term'] = apt['USTB10Y'] - apt['USTB3M']
apt['dinflation'] = LogDiff(apt['CPI']) - LogDiff(apt['CPI']).shift(1)
apt['mustb3m'] = apt['USTB3M']/12
apt['rterm'] = (apt['USTB10Y'] - apt['USTB3M']) - (apt['USTB10Y'] - apt['USTB3M']).shift(1)
apt['er_msoft'] = LogDiff(apt['MICROSOFT']) - apt['USTB3M']/12
apt['er_sp'] = LogDiff(apt['SANDP']) - apt['USTB3M']/12
# Eliminación de observaciones no disponibles (NA)
data = apt.dropna()
# Estimación MCO del modelo APT
formula = 'er_msoft ~ er_sp + dprod + dcredit + dinflation + dmoney + dspread + rterm'
results = smf.ols(formula, data).fit()
print(results.summary())
# Contraste de significación de variables
# hypotheses = 'dprod = dcredit = dmoney = dspread = 0'
# F_test = results.f_test(hypotheses)
# print(F_test)
#
# Diagnosis del modelo
#
# Especificación de la forma funcional: test RESET de Ramsey
reset_ramsey(results,degree=2)
reset_ramsey(results,degree=3)
# Estabilidad de los parámetros
# Test de Chow (punto de ruptura conocido)
# Definición del test de Chow
#    inputs:
#       data: objeto pandas DataFrame con las variables del modelo
#    outputs:
#        rss: suma de residuos al cuadrado
#        N: número de observaciones
#        K: número total de parámetros
#
def get_rss(data):
    formula = 'er_msoft ~ er_sp + dprod + dcredit + dinflation + dmoney + dspread + rterm'
    results = smf.ols(formula, data).fit()
    rss = (results.resid**2).sum() # suma de residuos al cuadrado
    N = results.nobs
    K = results.df_model
    return rss, N, K
# División por submuestras (split samples)
data1 = data[:'1996-01-01']
data2 = data['1996-01-01':]
# Muestra completa
RSS_total, N_total, K_total = get_rss(data)
# Primera submuestra
RSS_1, N_1, K_1 = get_rss(data1)
# Segunda submuestra
RSS_2, N_2, K_2 = get_rss(data2)
# Estadístico de contraste
numerador = (RSS_total - (RSS_1 + RSS_2)) / K_total
denominador = (RSS_1 + RSS_2) / (N_1 + N_2 - 2*K_total)
Chow_F_test = numerador/denominador
Chow_F_test
p_val = scs.f.sf(Chow_F_test, K_total, N_1 + N_2 - 2*K_total)
p_val
# Test de Chow recursivo (punto de ruptura desconocido)
name = ['Chow_rec_test', 'p-val', 'crit']
test = sms.breaks_cusumolsresid(resid = results.resid, ddof = results.df_model)
lzip(name, test)
# Estimación recursiva y test CUSUM
# Definición de los estadísticos recursivos
#
#    inputs:
#        variable: nombre literal de la variable en la fórmula de regresión
#        i: índice de serie de la regresión
#        interval: número de datos consecutivos en la muestra inicial
#    outputs:
#        coeff: coeficiente estimado de cada variable
#        se: error estándar de cada variable
# 
def recursive_reg(variable, i, interval):
    formula = 'er_msoft ~ er_sp + dprod + dcredit + dinflation + dmoney + dspread + rterm'
    results = smf.ols(formula, data.iloc[:i+interval]).fit()
    coeff = results.params[variable]
    se = results.bse[variable]
    return coeff, se
#
parameters = []
for i in range(373):
    coeff, se = recursive_reg('er_sp', i, 11)
    parameters.append((coeff,se))
parameters = pd.DataFrame(parameters, columns=['coeff','se'], index = data[:-10].index)
parameters['er_sp + 2*se'] = parameters['coeff'] + 2*parameters['se']
parameters['er_sp - 2*se'] = parameters['coeff'] - 2*parameters['se']
#
plt.figure(1)
plt.plot(parameters['coeff'], label=r'$\beta_{er_sp}$')
plt.plot(parameters['er_sp + 2*se'], label=r'$\beta_{er_sp} + 2*se$', linestyle='dashed')
plt.plot(parameters['er_sp - 2*se'], label=r'$\beta_{er_sp} - 2*se$', linestyle='dashed')
plt.xlabel('Date')
plt.grid(True)
plt.legend()
plt.show()
# Modelización de sucesos especiales (uso de variables ficticias para tratar la presencia de outliers aislados)
y_fitted = results.fittedvalues
residuals = results.resid
#
plt.figure(2)
plt.plot(residuals, label='Residuos')
plt.plot(y_fitted, label='Valores ajustados')
plt.xlabel('Date')
plt.ylabel('Desviaciones')
plt.grid(True)
plt.legend()
plt.show()
# Identificación de valores menores
residuals.nsmallest(2)
#
data['APR2000DUM'] = np.where(data.index == '2000-4-1', 1, 0)
data['DEC2000DUM'] = np.where(data.index == '2000-12-1', 1, 0)
#
formula = 'er_msoft ~ er_sp + dprod + dcredit + dinflation + dmoney + dspread + rterm + APR2000DUM + DEC2000DUM'
results = smf.ols(formula, data).fit()
print(results.summary())
#
# Término de error del modelo
# Normalidad de los errores
residuals = results.resid
# Histograma de los residuos
plt.figure(3)
plt.hist(residuals,20,edgecolor='black',linewidth=1.2)
plt.xlabel('Residuos')
plt.ylabel('Densidad estimada')
plt.show()
# Test de Jarque-Bera
name = ['Jarque-Bera', 'Chi^2 p-val', 'Skewness', 'Kurtosis']
JB_test = sms.jarque_bera(residuals)
lzip(name, JB_test)
print(pd.DataFrame([np.round(JB_test, 8)], columns=name))
# Contrastes de normalidad para una variable (var: ndarray)
def normality_tests(var):
  Skewness_test = scs.skew(var)
  Skewness_pval = scs.skewtest(var)[1]
  Kurtosis_test = scs.kurtosis(var)
  Kurtosis_pval = scs.kurtosistest(var)[1]
  Normality_test = scs.normaltest(var)
  return Skewness_test, Skewness_pval, Kurtosis_test, Kurtosis_pval, Normality_test
print(normality_tests(residuals))
#
name = ['Skewness_test', 'Skewness_p-val', 'Kurtosis_test', 'Kurtosis_p-val', 'Normality_test']
Norm_tests = normality_tests(residuals)
lzip(name, Norm_tests)
# Heteroscedasticidad
# Gráfico de errores estimados (residuos)
plt.figure(4)
plt.plot(results.resid)
plt.xlabel('Date')
plt.ylabel('Residuos')
plt.grid(True)
plt.show()
# Test de White
name = ['LM statistic', 'Chi^2 p-val', 'F statistic', 'F p-val']
W_test = smsdiag.het_white(results.resid, results.model.exog)
lzip(name, W_test)
print(pd.DataFrame([np.round(W_test, 8)], columns=name))
# Test de Breusch-Pagan (heter. aditiva)
name = ['LM statistic', 'Chi^2 p-val', 'F statistic', 'F p-val']
BP_test = smsdiag.het_breuschpagan(results.resid, results.model.exog)
lzip(name, BP_test)
print(pd.DataFrame([np.round(BP_test, 8)], columns=name))
# Heteroscedasticidad autorregresiva condicional (ARCH): test de Engle
name = ['LM statistic', 'Chi^2 p-val', 'F statistic', 'F p-val']
E_test = smsdiag.het_arch(results.resid, nlags=1)
lzip(name,E_test)
print(pd.DataFrame([np.round(E_test, 8)], columns=name))
# Corrección de la matriz de covarianzas: estimador de White
results1 = smf.ols(formula, data).fit(cov_type='HC1')
print(results1.summary())
# Autocorrelación
# Test de Durbin-Watson
residuals = results.resid
sms.durbin_watson(residuals)
# Test de Breusch-Godfrey
name = ['LM statistic', 'Chi^2 p-val', 'F statistic', 'F p-val']
BG_test = smsdiag.acorr_breusch_godfrey(results,1)
lzip(name, BG_test)
print(pd.DataFrame([np.round(BG_test, 8)], columns=name))
#
name = ['LM statistic', 'Chi^2 p-val', 'F statistic', 'F p-val']
BG_test = smsdiag.acorr_breusch_godfrey(results,6)
lzip(name, BG_test)
print(pd.DataFrame([np.round(BG_test, 8)], columns=name))
# Corrección de la matriz de covarianzas: estimador de Newey-West (si maxlags=0 -> HAC=HC1)
results2 = smf.ols(formula, data).fit(cov_type='HAC', cov_kwds={'maxlags':6,'use_correction':True})
print(results2.summary())
# Multicolinealidad
# Matriz de correlaciones
dataX = data[['er_sp','dprod','dcredit','dinflation','dmoney','dspread','rterm']]
dataX.corr()
plt.figure(5)
sns.heatmap(dataX.corr(), vmin=-0.25, vmax=0.25, center=0, annot=True, fmt='.2f', mask=~np.tri(dataX.corr().shape[1], k=-1, dtype=bool), cbar=False)
plt.show()
#
vif = pd.DataFrame()
vif["Variable"]   = list(dataX.columns)[:]
vif
#
formula = 'er_msoft ~ er_sp + dprod + dcredit + dinflation + dmoney + dspread + rterm'
results = smf.ols(formula, data).fit()
for i in range(1, results.model.exog.shape[1]):
     print(smstats.outliers_influence.variance_inflation_factor(results.model.exog, i))
```

