ggplot(aes(.fittedPC1, .fittedPC2)) + geom_point(size = 1.5) +
theme_half_open(12) + background_grid()
arrow_style <- arrow(angle = 20, ends = "first", type = "closed", length = grid::unit(8, "pt"))
PCA_X %>%
tidy(matrix = "rotation") %>%
pivot_wider(names_from = "PC", names_prefix = "PC", values_from = "value") %>%
ggplot(aes(PC1, PC2)) +
geom_segment(xend = 0, yend = 0, arrow = arrow_style) +
geom_text(aes(label = column),hjust = 1, nudge_x = -0.02, color = "#904C2F") +
xlim(-1.25, .5) + ylim(-.5, 1) +
coord_fixed() +
theme_minimal_grid(12)
lm_2 <- lm(RENTAB ~ PCA_X$x[,1:4])
S(lm_2)
# Significado de la variables explicativas
round(PCA_X$rotation[,1],2)
round(PCA_X$rotation[,2],2)
round(PCA_X$rotation[,3],2)
round(PCA_X$rotation[,4],2)
require(MASS)
lm_ridge <- lm.ridge(RENTAB ~ log(VT) + R1 + R2 + R3 + R4 + R5 + R6 + R7 + R8 + R9 + R10, lambda = seq(0, 0.05, len=100))
matplot(lm_ridge$lambda, coef(lm_ridge), type="l", xlab=expression(lambda), ylab=expression(hat(beta)),col=1)
which.min(lm_ridge$GCV)
lm_3 <- lm.ridge(RENTAB ~ log(VT) + R1 + R2 + R3 + R4 + R5 + R6 + R7 + R8 + R9 + R10, lambda = 0.05)
lm_3
lm_3 <- lm.ridge(RENTAB ~ log(VT) + R1 + R2 + R3 + R4 + R5 + R6 + R7 + R8 + R9 + R10, data=RENTAB_EMP, lambda = 0.05)
lm_3
require(pls)
lm_4 <- plsr(RENTAB ~ log(VT) + R1 + R3 + R4 + R5 + R6 + R7 + R8 + R9 + R10, data=RENTAB_EMP, ncomp=4, validation="CV") # invalid type (closure) for variable 'R2'
summary(lm_4)
plsCV <- RMSEP(lm_4, estimate="CV")
plot(plsCV,main="")
plot(lm_ridge$GCV)
plot(lm_ridge$CV)
lm_ridge$CV
plot(lm_ridge$GCV)
require(pls)
lm_4 <- plsr(RENTAB ~ log(VT) + R1 + R + R3 + R4 + R5 + R6 + R7 + R8 + R9 + R10, data=RENTAB_EMP, ncomp=4, validation="CV") # invalid type (closure) for variable 'R2'
summary(lm_4)
require(pls)
lm_4 <- plsr(RENTAB ~ log(VT) + R1 + R2 + R3 + R4 + R5 + R6 + R7 + R8 + R9 + R10, data=RENTAB_EMP, ncomp=4, validation="CV") # invalid type (closure) for variable 'R2'
summary(lm_4)
plsCV <- RMSEP(lm_4, estimate="CV")
plot(plsCV,main="")
require(pls)
lm_4 <- plsr(RENTAB ~ log(VT) + R1 + R2 + R3 + R4 + R5 + R6 + R7 + R8 + R9 + R10, data=RENTAB_EMP, ncomp=11, validation="CV")
summary(lm_4)
plsCV <- RMSEP(lm_4, estimate="CV")
plot(plsCV,main="")
coefplot(lm_4, ncomp=5)
lm_1 <- lm(RENTAB ~ log(VT) + R1 + R2 + R3 + R4 + R5 + R6 + R7 + R8 + R9 + R10, data=RENTAB_EMP)
S(lm_1)
require(pls)
lm_4 <- plsr(RENTAB ~ log(VT) + R1 + R2 + R3 + R4 + R5 + R6 + R7 + R8 + R9 + R10, data=RENTAB_EMP, ncomp=11, validation="CV")
summary(lm_4)
plsCV <- RMSEP(lm_4, estimate="CV")
plot(plsCV,main="")
coefplot(lm_4, ncomp=4)
?coefplot
coefplot(lm_4, ncomp=1:4)
coefplot(lm_4, ncomp=1:4, legendpos = "bottomright")
data <- data.frame(cbind(log(RENTAB_EMP$VT),RENTAB_EMP$R1,RENTAB_EMP$R2,RENTAB_EMP$R3,
RENTAB_EMP$R4, RENTAB_EMP$R5, RENTAB_EMP$R6, RENTAB_EMP$R7,
RENTAB_EMP$R8,RENTAB_EMP$R9,RENTAB_EMP$R10, RENTAB_EMP$RENTAB))
names(data)=c("l_VT","R1","R2","R3","R4","R5","R6","R7","R8","R9","R10","RENTAB")
View(data)
View(RENTAB_EMP)
lm_5 <- lars(as.matrix(data[,-12]),data$RENTAB)
plot(lm_5)
cv_lars_mod <- cv.lars(as.matrix(data[,-12]),data$RENTAB)
cv_lars_mod$index[which.min(cv_lars_mod$cv)]  # El valor mínimo se utiliza en el paso siguiente
require(lars)
data <- data.frame(cbind(log(RENTAB_EMP$VT),RENTAB_EMP$R1,RENTAB_EMP$R2,RENTAB_EMP$R3,
RENTAB_EMP$R4, RENTAB_EMP$R5, RENTAB_EMP$R6, RENTAB_EMP$R7,
RENTAB_EMP$R8,RENTAB_EMP$R9,RENTAB_EMP$R10, RENTAB_EMP$RENTAB))
names(data)=c("l_VT","R1","R2","R3","R4","R5","R6","R7","R8","R9","R10","RENTAB")
lm_5 <- lars(as.matrix(data[,-12]),data$RENTAB)
plot(lm_5)
cv_lars_mod <- cv.lars(as.matrix(data[,-12]),data$RENTAB)
cv_lars_mod$index[which.min(cv_lars_mod$cv)]  # El valor mínimo se utiliza en el paso siguiente
predict(lm_5,s=0.8384,type="coef",mode="fraction")$coef
coef(lm(RENTAB ~ log(VT) + R1 + R2 + R3 + R4 + R5 + R6 + R7 + R8 + R9 + R10, data=RENTAB_EMP))
help(preProcess, package=caret)
summary(data)
library(caret)
preProcValues <- preProcess(data, method = c("center", "scale"))
dataTransformed <- predict(preProcValues, data)
summary(data)
summary(dataTransformed)
View(dataTransformed)
View(data)
preProcValues <- preProcess(data[,-12], method = c("center", "scale"))
dataTransformed <- predict(preProcValues, data)
View(dataTransformed)
summary(dataTransformed)
summary(RENTAB_EMP)
summary(data)
summary(dataTransformed)
#
library(tidyverse)
library(car)
library(corrplot)
library(broom)
library(cowplot)
#
RENTAB_EMP <- read_delim("RENTAB_EMP.csv", ";", escape_double = FALSE, trim_ws = TRUE)
View(RENTAB_EMP)
#
lm_1 <- lm(RENTAB ~ log(VT) + R1 + R2 + R3 + R4 + R5 + R6 + R7 + R8 + R9 + R10, data=RENTAB_EMP)
S(lm_1)
install.packages("glmnet")
data <- data.frame(cbind(log(RENTAB_EMP$VT),RENTAB_EMP$R1,RENTAB_EMP$R2,RENTAB_EMP$R3,
RENTAB_EMP$R4, RENTAB_EMP$R5, RENTAB_EMP$R6, RENTAB_EMP$R7,
RENTAB_EMP$R8,RENTAB_EMP$R9,RENTAB_EMP$R10, RENTAB_EMP$RENTAB))
names(data)=c("l_VT","R1","R2","R3","R4","R5","R6","R7","R8","R9","R10","RENTAB")
View(data)
#
x_vars <- data.matrix(data[, 1:11])
# Getting the dependent variable
y_var <- data[, "RENTAB"]
#
View(x_vars)
# lambda
lambda_seq <- 10^seq(2, -2, by = -.1)
# lambda
lambda_seq
# Regresión ridge
library(glmnet)
fit <- glmnet(x_var, y_var, alpha = 0, lambda  = lambda_seq)
#
summary(fit)
#
# Regresión ridge
library(glmnet)
fit <- glmnet(x_vars, y_var, alpha = 0, lambda  = lambda_seq)
summary(fit)
#
Selección del lambda óptimo (criterio CV)
ridge_cv <- cv.glmnet(x_vars, y_var, alpha = 0, lambda = lambdas)
best_lambda <- ridge_cv$lambda.min
best_lambda
#
# Selección del lambda óptimo (criterio CV)
ridge_cv <- cv.glmnet(x_vars, y_var, alpha = 0, lambda = lambda_seq)
best_lambda <- ridge_cv$lambda.min
best_lambda
#
View(ridge_cv)
best_fit <- ridge_cv$glmnet.fit
head(best_fit)
#
best_ridge <- glmnet(x_vars, y_var, alpha = 0, lambda = 0.01)
coef(best_ridge)
summary(best_ridge)
#
coef(best_ridge)
coef(lm_1)
# Posible escalamiento de las variables explicativas
data <- data.frame(cbind(log(RENTAB_EMP$VT),RENTAB_EMP$R1,RENTAB_EMP$R2,RENTAB_EMP$R3,
RENTAB_EMP$R4, RENTAB_EMP$R5, RENTAB_EMP$R6, RENTAB_EMP$R7,
RENTAB_EMP$R8,RENTAB_EMP$R9,RENTAB_EMP$R10, RENTAB_EMP$RENTAB))
names(data)=c("l_VT","R1","R2","R3","R4","R5","R6","R7","R8","R9","R10","RENTAB")
#
library(caret)
preProcValues <- preProcess(data[,-12], method = c("center", "scale"))
dataTransformed <- predict(preProcValues, data)
#
View(dataTransformed)
View(data)
# Secuencia de lambdas
lambda_seq <- 10^seq(2, -2, by = -.1)
# Regresión ridge
library(glmnet)
fit <- glmnet(x_vars, y_var, alpha = 0, lambda  = lambda_seq)
summary(fit)
# Selección del lambda óptimo (criterio CV)
ridge_cv <- cv.glmnet(x_vars, y_var, alpha = 0, lambda = lambda_seq)
best_lambda <- ridge_cv$lambda.min
best_lambda
#
best_fit <- ridge_cv$glmnet.fit
head(best_fit)
#
best_ridge <- glmnet(x_vars, y_var, alpha = 0, lambda = 0.01)
coef(best_ridge)
summary(best_ridge)
#
#
library(tidyverse)
library(car)
library(corrplot)
library(broom)
library(cowplot)
#
RENTAB_EMP <- read_delim("RENTAB_EMP.csv", ";", escape_double = FALSE, trim_ws = TRUE)
View(RENTAB_EMP)
#
lm_1 <- lm(RENTAB ~ log(VT) + R1 + R2 + R3 + R4 + R5 + R6 + R7 + R8 + R9 + R10, data=RENTAB_EMP)
S(lm_1)
# DETECCIÃN DEL PROBLEMA
#
# MatrÃ­z de correlaciones de las variables explicativas (sin incluir la constante)
#
attach(RENTAB_EMP)
X <- data.frame(log(VT), R1, R2, R3, R4, R5, R6, R7, R8, R9, R10)
cor(X)
corrplot(cor(X))
# Factores de inflaciÃ³n de la varianza (VIF)
#
vif(lm_1)
vif(lm_1) > 10 # problema de colinealidad (se coresponde con un VIF=10, es decir, Rj^2=0.90)
sqrt(vif(lm_1))
sqrt(vif(lm_1)) > 2 # cota alternativa (se coresponde con un VIF=4, es decir, Rj^2=0.75)
# TRATAMIENTO DEL PROBLEMA
#
# MÃ©todo de componentes principales
# ClÃ¡sico
PCA_X <- prcomp(X, scale. = TRUE)
dim(PCA_X$rotation)
PCA_X$rotation
dim(PCA_X$x)
summary(PCA_X)
plot(PCA_X)
# Estilo tidyverse
PCA_X %>% tidy(matrix = "eigenvalues")
PCA_X %>% tidy(matrix = "rotation")
#
PCA_X %>%
tidy(matrix = "eigenvalues") %>%
ggplot(aes(PC, percent)) +
geom_col(fill = "#56B4E9", alpha = 0.8) +
scale_x_continuous(breaks = 1:11) +
scale_y_continuous(labels = scales::percent_format(), expand = expansion(mult = c(0, 0.01))) +
theme_minimal_hgrid(12)
#
PCA_X %>%
augment(RENTAB_EMP) %>%
ggplot(aes(.fittedPC1, .fittedPC2)) + geom_point(size = 1.5) +
theme_half_open(12) + background_grid()
#
arrow_style <- arrow(angle = 20, ends = "first", type = "closed", length = grid::unit(8, "pt"))
PCA_X %>%
tidy(matrix = "rotation") %>%
pivot_wider(names_from = "PC", names_prefix = "PC", values_from = "value") %>%
ggplot(aes(PC1, PC2)) +
geom_segment(xend = 0, yend = 0, arrow = arrow_style) +
geom_text(aes(label = column),hjust = 1, nudge_x = -0.02, color = "#904C2F") +
xlim(-1.25, .5) + ylim(-.5, 1) +
coord_fixed() +
theme_minimal_grid(12)
#
lm_2 <- lm(RENTAB ~ PCA_X$x[,1:4])
S(lm_2)
# Significado de la variables explicativas
round(PCA_X$rotation[,1],2)
round(PCA_X$rotation[,2],2)
round(PCA_X$rotation[,3],2)
round(PCA_X$rotation[,4],2)
#
lm_ridge <- lm.ridge(RENTAB ~ log(VT) + R1 + R2 + R3 + R4 + R5 + R6 + R7 + R8 + R9 + R10, data=RENTAB_EMP, lambda = seq(0, 0.1, len=10))
matplot(lm_ridge$lambda, coef(lm_ridge), type="l", xlab=expression(lambda), ylab=expression(hat(beta)),col=1)
which.min(lm_ridge$GCV)
plot(lm_ridge$GCV)
require(MASS)
lm_ridge <- lm.ridge(RENTAB ~ log(VT) + R1 + R2 + R3 + R4 + R5 + R6 + R7 + R8 + R9 + R10, data=RENTAB_EMP, lambda = seq(0, 0.1, len=10))
matplot(lm_ridge$lambda, coef(lm_ridge), type="l", xlab=expression(lambda), ylab=expression(hat(beta)),col=1)
which.min(lm_ridge$GCV)
plot(lm_ridge$GCV)
lm_3 <- lm.ridge(RENTAB ~ log(VT) + R1 + R2 + R3 + R4 + R5 + R6 + R7 + R8 + R9 + R10, data=RENTAB_EMP, lambda = 0.05)
lm_3
#
?lm.ridge
View(lm_3)
#
# MÃ©todo PLS
#
require(pls)
lm_4 <- plsr(RENTAB ~ log(VT) + R1 + R2 + R3 + R4 + R5 + R6 + R7 + R8 + R9 + R10, data=RENTAB_EMP, ncomp=11, validation="CV")
summary(lm_4)
plsCV <- RMSEP(lm_4, estimate="CV")
plot(plsCV,main="")
#
coefplot(lm_4, ncomp=1:4, legendpos = "bottomright")
# MÃ©todo LASSO
#
require(lars)
data <- data.frame(cbind(log(RENTAB_EMP$VT),RENTAB_EMP$R1,RENTAB_EMP$R2,RENTAB_EMP$R3,
RENTAB_EMP$R4, RENTAB_EMP$R5, RENTAB_EMP$R6, RENTAB_EMP$R7,
RENTAB_EMP$R8,RENTAB_EMP$R9,RENTAB_EMP$R10, RENTAB_EMP$RENTAB))
names(data)=c("l_VT","R1","R2","R3","R4","R5","R6","R7","R8","R9","R10","RENTAB")
lm_5 <- lars(as.matrix(data[,-12]),data$RENTAB)
plot(lm_5)
cv_lars_mod <- cv.lars(as.matrix(data[,-12]),data$RENTAB)
cv_lars_mod$index[which.min(cv_lars_mod$cv)]  # El valor mÃ­nimo se utiliza en el paso siguiente
predict(lm_5,s=0.8384,type="coef",mode="fraction")$coef  # estimaciones LASSO
coef(lm(RENTAB ~ log(VT) + R1 + R2 + R3 + R4 + R5 + R6 + R7 + R8 + R9 + R10, data=RENTAB_EMP)) #comparaciÃ³n con MCO
#
longley # not the same as the S-PLUS dataset
names(longley)[1] <- "y"
lm(y ~ ., longley)
lm.ridge(y ~ ., longley)
plot(lm.ridge(y ~ ., longley, lambda = seq(0,0.1,0.001)))
select(lm.ridge(y ~ ., longley,lambda = seq(0,0.1,0.0001)))
require(MASS)
lm_ridge <- lm.ridge(RENTAB ~ log(VT) + R1 + R2 + R3 + R4 + R5 + R6 + R7 + R8 + R9 + R10, data=RENTAB_EMP, lambda = seq(0,0.1,0.0001))
plot(lm.ridge(RENTAB ~ log(VT) + R1 + R2 + R3 + R4 + R5 + R6 + R7 + R8 + R9 + R10, data=RENTAB_EMP, lambda = seq(0,0.1,0.0001)))
select(lm.ridge(RENTAB ~ log(VT) + R1 + R2 + R3 + R4 + R5 + R6 + R7 + R8 + R9 + R10, data=RENTAB_EMP, lambda = seq(0,0.1,0.0001)))
matplot(lm_ridge$lambda, coef(lm_ridge), type="l", xlab=expression(lambda), ylab=expression(hat(beta)),col=1)
which.min(lm_ridge$GCV)
plot(lm_ridge$GCV)
lm.ridge(RENTAB ~ log(VT) + R1 + R2 + R3 + R4 + R5 + R6 + R7 + R8 + R9 + R10, data=RENTAB_EMP)
lm.ridge(RENTAB ~ log(VT) + R1 + R2 + R3 + R4 + R5 + R6 + R7 + R8 + R9 + R10, data=RENTAB_EMP, lambda = 0.05)
lm_3 <- lm.ridge(RENTAB ~ log(VT) + R1 + R2 + R3 + R4 + R5 + R6 + R7 + R8 + R9 + R10, data=RENTAB_EMP, lambda = 0.05)
lm_3
lm_3_1 <- lm.ridge(RENTAB ~ log(VT) + R1 + R2 + R3 + R4 + R5 + R6 + R7 + R8 + R9 + R10, data=RENTAB_EMP)
lm_3_2 <- lm.ridge(RENTAB ~ log(VT) + R1 + R2 + R3 + R4 + R5 + R6 + R7 + R8 + R9 + R10, data=RENTAB_EMP, lambda = 0.05)
#
View(lm_3_1)
View(lm_3_2)
View(lm_3_1)
View(lm_3_2)
x_vars <- data.matrix(data[, 1:11])
#
y_var <- data[, "RENTAB"]
#
# Secuencia de lambdas
lambda_seq <- 10^seq(2, -2, by = -.1)
# Regresión ridge
library(glmnet)
fit <- glmnet(x_vars, y_var, alpha = 0, lambda  = lambda_seq)
summary(fit)
# Selección del lambda óptimo (criterio CV)
ridge_cv <- cv.glmnet(x_vars, y_var, alpha = 0, lambda = lambda_seq)
best_lambda <- ridge_cv$lambda.min
best_lambda
#
best_fit <- ridge_cv$glmnet.fit
head(best_fit)
#
best_ridge <- glmnet(x_vars, y_var, alpha = 0, lambda = 0.05)
coef(best_ridge)
summary(best_ridge)
#
best_ridge <- glmnet(x_vars, y_var, alpha = 0, lambda = 0.05108)
coef(best_ridge)
summary(best_ridge)
#
best_ridge
best_ridge <- glmnet(x_vars, y_var, alpha = 0, lambda = 0.05)
best_ridge
coef(best_ridge)
#
lm_3_2 <- lm.ridge(RENTAB ~ log(VT) + R1 + R2 + R3 + R4 + R5 + R6 + R7 + R8 + R9 + R10, data=RENTAB_EMP, lambda = 0.05)
lm_3_2
#
# filter
#
gapminder_filtered <- filter(gapminder_selected, year>=1980)
library(tidyverse)
gapminder <- read_delim("GAPMINDER.csv", ";", escape_double = FALSE, trim_ws = TRUE)
# GM <- read_csv("https://raw.githubusercontent.com/CerebralMastication/r_for_the_student/master/01_data/gapminder.csv")
gapminder
#
library(skimr)
skim(gapminder)
#
# Operaciones básicas del Tidyverse
#
# dplyr verbs
#
# select
#
gapminder_selected <- select(gapminder, year, country, pop, gdpPercap)
#
# filter
#
gapminder_filtered <- filter(gapminder_selected, year>=1980)
#
# mutate
#
gapminder_mutated <- mutate(gapminder_filtered, GDP=gdpPercap*pop)
#
# group_by
#
gapminder_grouped <- group_by(gapminder_mutated, country)
#
# summarise
#
gapminder_summarised <- summarise(gapminder_grouped, AVG_GDP=mean(GDP))
#
# arrange
#
gapminder_arranged_ascending <- arrange(gapminder_summarised, AVG_GDP)
gapminder_arranged_descending <- arrange(gapminder_summarised, -AVG_GDP)
#
# The pipe operator (%>%)
#
AVG_GDP <-
gapminder %>%
select(year, country, pop, gdpPercap) %>%
filter(year>=1980) %>%
mutate(GDP=gdpPercap*pop) %>%
group_by(country) %>%
summarise(AVG_GDP=mean(GDP)) %>%
arrange(-AVG_GDP)
#
# Plotting and EDA
#
# esquisse plots
# Select the 'ggplot2' builder in the Addins menu of RStudio or ...
#
esquisse:::esquisser()
#
# GGally technique
#
gapminder %>% select(-country) %>% ggpairs()
#
# Regression
#
# Simple linear model
#
model_1 <- lm(lifeExp ~ gdpPercap, data=gapminder)
summary(model_1)
#
model_2 <- lm(lifeExp ~ gdpPercap + year, data=gapminder)
summary(model_2)
#
# LRM assumptions
#
library(lindia)
model_2 %>%
gg_diagnose(plot.all=TRUE,boxcox=TRUE)
#
model_3 <- lm(lifeExp ~ log(gdpPercap) + year, data=gapminder)
summary(model_3)
model_3 %>%
gg_diagnose(plot.all=TRUE,boxcox=TRUE)
#
library(tidyverse)
gapminder <- read_delim("GAPMINDER.csv", ";", escape_double = FALSE, trim_ws = TRUE)
# GM <- read_csv("https://raw.githubusercontent.com/CerebralMastication/r_for_the_student/master/01_data/gapminder.csv")
gapminder
#
library(skimr)
skim(gapminder)
#
# Operaciones básicas del Tidyverse
#
# dplyr verbs
#
# select
#
gapminder_selected <- select(gapminder, year, country, pop, gdpPercap)
#
# filter
#
gapminder_filtered <- filter(gapminder_selected, year>=1980)
#
# mutate
#
gapminder_mutated <- mutate(gapminder_filtered, GDP=gdpPercap*pop)
#
# group_by
#
gapminder_grouped <- group_by(gapminder_mutated, country)
#
# summarise
#
gapminder_summarised <- summarise(gapminder_grouped, AVG_GDP=mean(GDP))
#
# arrange
#
gapminder_arranged_ascending <- arrange(gapminder_summarised, AVG_GDP)
gapminder_arranged_descending <- arrange(gapminder_summarised, -AVG_GDP)
#
# The pipe operator (%>%)
#
AVG_GDP <-
gapminder %>%
select(year, country, pop, gdpPercap) %>%
filter(year>=1980) %>%
mutate(GDP=gdpPercap*pop) %>%
group_by(country) %>%
summarise(AVG_GDP=mean(GDP)) %>%
arrange(-AVG_GDP)
# GGally technique
#
gapminder %>% select(-country) %>% ggpairs()
#
# Simple linear model
#
model_1 <- lm(lifeExp ~ gdpPercap, data=gapminder)
summary(model_1)
#
model_2 <- lm(lifeExp ~ gdpPercap + year, data=gapminder)
summary(model_2)
#
# GGally technique
library(GGally)
#
gapminder %>% select(-country) %>% ggpairs()
# LRM assumptions
#
library(lindia)
model_2 %>%
gg_diagnose(plot.all=TRUE,boxcox=TRUE)
#
model_3 <- lm(lifeExp ~ log(gdpPercap) + year, data=gapminder)
summary(model_3)
model_3 %>%
gg_diagnose(plot.all=TRUE,boxcox=TRUE)
#
